{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fb7581-a09a-404f-a5ac-5a77996eafea",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157485d-f705-405f-9997-bdcadbc85218",
   "metadata": {},
   "source": [
    "### Using Hugging Face Pipelines on Intel® Gaudi® 2 - Text Generation\n",
    "\n",
    "This example shows how to use the Hugging Face Transformers pipeline API to run text generation task on Intel Gaudi.\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks. Text generation pipeline is one of them.\n",
    "\n",
    "#### Text Generation Pipeline Brief Introduction\n",
    "Text generation pipeline using any ModelWithLMHead. This pipeline predicts the words that will follow a specified text prompt.\n",
    "\n",
    "This language generation pipeline can currently be loaded from pipeline() using the following task identifier: \"text-generation\".\n",
    "\n",
    "The **models that this pipeline can use are models that have been trained with an autoregressive language modeling objective**, which includes the uni-directional models in the library (e.g. gpt2). See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f62c6-a93f-4d11-bff5-12d38b0f4b8b",
   "metadata": {},
   "source": [
    "#### Install the Hugging Face Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d1f096-17ee-451e-a356-1e5b2162a83d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum==1.11.1 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]==1.11.1) (1.11.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (1.12)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.11.1->optimum[habana]==1.11.1) (4.28.1)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (2.1.0a0+gitf8b6084)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (23.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (0.23.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum==1.11.1->optimum[habana]==1.11.1) (2.19.1)\n",
      "Requirement already satisfied: optimum-habana in /usr/local/lib/python3.8/dist-packages (from optimum[habana]==1.11.1) (1.6.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (4.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum==1.11.1->optimum[habana]==1.11.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum==1.11.1->optimum[habana]==1.11.1) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum==1.11.1->optimum[habana]==1.11.1) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum==1.11.1->optimum[habana]==1.11.1) (0.13.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.11.1->optimum[habana]==1.11.1) (0.2.0)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.11.1->optimum[habana]==1.11.1) (3.20.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum==1.11.1->optimum[habana]==1.11.1) (10.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum==1.11.1->optimum[habana]==1.11.1) (3.9.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]==1.11.1) (0.30.1)\n",
      "Requirement already satisfied: diffusers>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]==1.11.1) (0.28.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum==1.11.1->optimum[habana]==1.11.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.18.0->optimum-habana->optimum[habana]==1.11.1) (6.8.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.18.0->optimum-habana->optimum[habana]==1.11.1) (0.4.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.18.0->optimum-habana->optimum[habana]==1.11.1) (10.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum==1.11.1->optimum[habana]==1.11.1) (2023.11.17)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->optimum-habana->optimum[habana]==1.11.1) (5.9.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum==1.11.1->optimum[habana]==1.11.1) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.11.1->optimum[habana]==1.11.1) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.18.0->optimum-habana->optimum[habana]==1.11.1) (3.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Hugging_Face_pipelines\n",
    "!pip install optimum-habana==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c7a14-3d38-4258-a8de-339bde866b7e",
   "metadata": {},
   "source": [
    "#### Import all neccessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb90147-891d-4ad0-b288-40e1c6673946",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from habana_frameworks.torch.hpu import wrap_in_hpu_graph\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55e1ae-c373-4bed-a1a8-79ffe6bb0339",
   "metadata": {},
   "source": [
    "#### Prepare the input\n",
    "     We set prompt as a list of 3 prompts, it can do batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edc56bc-73a9-4ccf-bfce-19a962f64efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time, in a land far, far away,\",\n",
    "    \"In the beginning, there was darkness.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2e35f-7752-4187-9c36-7d903460d44f",
   "metadata": {},
   "source": [
    "#### Setup the pipeline\n",
    "To setup the Hugging Face pipeline we set the following:\n",
    "\n",
    "* Choose the Hugging Face task: \"text-generation\"\n",
    "   This Text generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
    "`\"text-generation\"`.\n",
    "* Set the device to \"hpu\" which allows the pipeline to run on Intel Gaudi\n",
    "* Choose model \"gpt2\" and data type to be bf16\n",
    "* Finally we'll use the \"wrap_in_hpu_graph\" to wrap the module forward function with HPU Graphs. This wrapper captures, caches and replays the graph. More info here.\n",
    "\n",
    "You will see that the Intel Gaudi will build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb88afb-f302-4349-a896-899a2ae0f466",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 10\n",
      "CPU RAM       : 102493984 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model = 'gpt2', trust_remote_code=True, torch_dtype=torch.bfloat16, device=\"hpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2289ef-2b6d-4703-9eb0-e2bfb1764d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model = wrap_in_hpu_graph(generator.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f9004-a059-4945-9bf9-0f15b19a623d",
   "metadata": {},
   "source": [
    "#### Execute the pipeline and output the results\n",
    "Here the input prompts are 3 prompts.\n",
    "It is batch inference of batch_size =3, and the outputs are list of 3 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3e8acc-57ef-45b5-8631-a6b3c565dfca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = generator(prompts, max_length = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ab4af-065f-4d22-88c6-c34267ba9dc6",
   "metadata": {},
   "source": [
    "Extract each output generated_text and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c8d308-a923-4a66-80eb-11fbdd54a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== output 0 ===: \n",
      "  Once upon a time, in a land far, far away, from the Earth, a very beautiful sun had spread forth.\n",
      "\n",
      "The sun that appeared before him had shone brighter than the sun that had eclipsed him. As the sun had risen from the earth, so had the moon risen from the earth, and the stars were risen from the earth. The earth shook, and the sun struck down upon the city of Sodom, and the stars fell upon Sodom, and the city of \n",
      "\n",
      "=== output 1 ===: \n",
      "  In the beginning, there was darkness. The night was full of snow, which made the man's mouth water with blood.\n",
      "\n",
      "Quran: There were many in the city. People were dancing from head to toe. They were in need of a place to hide. The police came to their rescue and helped them. They thought they could kill the man and get away with it. But when the police arrived, they did nothing.\n",
      "\n",
      "Quran: Allah and his messenger said: \" \n",
      "\n",
      "=== output 2 ===: \n",
      " The quick brown fox jumps over the lazy dog. 'Have a look,' he says. 'He loves to find new things.' It is a joke, but in fact he is in love.\n",
      "\n",
      "JAMES L. HILL / AFP - Getty Images\n",
      "\n",
      "A brown fox who is just starting to get settled in, gets along well with a rabbit's good looks, and talks nice. He is known to be a good hunter of rabbits.\n",
      "\n",
      "RENTON HARDIN /  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f\"=== output 0 ===: \\n  {output[0][0]['generated_text']} \\n\")\n",
    "print (f\"=== output 1 ===: \\n  {output[1][0]['generated_text']} \\n\")\n",
    "print (f\"=== output 2 ===: \\n {output[2][0]['generated_text']}  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6c00a-580f-4828-a2d5-dcd0db5348f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
