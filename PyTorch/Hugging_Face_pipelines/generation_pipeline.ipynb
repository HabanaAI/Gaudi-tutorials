{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fb7581-a09a-404f-a5ac-5a77996eafea",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157485d-f705-405f-9997-bdcadbc85218",
   "metadata": {},
   "source": [
    "### Using Hugging Face Pipelines on Intel® Gaudi® 2 - Text Generation\n",
    "\n",
    "This example shows how to use the Hugging Face Transformers pipeline API to run text generation task on Intel Gaudi.\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks. Text generation pipeline is one of them.\n",
    "\n",
    "#### Text Generation Pipeline Brief Introduction\n",
    "Text generation pipeline using any ModelWithLMHead. This pipeline predicts the words that will follow a specified text prompt.\n",
    "\n",
    "This language generation pipeline can currently be loaded from pipeline() using the following task identifier: \"text-generation\".\n",
    "\n",
    "The **models that this pipeline can use are models that have been trained with an autoregressive language modeling objective**, which includes the uni-directional models in the library (e.g. gpt2). See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f62c6-a93f-4d11-bff5-12d38b0f4b8b",
   "metadata": {},
   "source": [
    "#### Install the Hugging Face Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1f096-17ee-451e-a356-1e5b2162a83d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Hugging_Face_pipelines\n",
    "!pip install optimum-habana==1.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c7a14-3d38-4258-a8de-339bde866b7e",
   "metadata": {},
   "source": [
    "#### Import all neccessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb90147-891d-4ad0-b288-40e1c6673946",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Enable PT_HPU_LAZY_MODE=1\n",
    "import os\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '1'\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from habana_frameworks.torch.hpu import wrap_in_hpu_graph\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979cbc5",
   "metadata": {},
   "source": [
    "The command below may be needed to modify the existing Hugging Face model classes to use the Intel Gaudi specific version of the model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e47005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.habana.transformers.modeling_utils import adapt_transformers_to_gaudi\n",
    "adapt_transformers_to_gaudi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55e1ae-c373-4bed-a1a8-79ffe6bb0339",
   "metadata": {},
   "source": [
    "#### Prepare the input\n",
    "     We set prompt as a list of 3 prompts, it can do batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edc56bc-73a9-4ccf-bfce-19a962f64efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time, in a land far, far away,\",\n",
    "    \"In the beginning, there was darkness.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2e35f-7752-4187-9c36-7d903460d44f",
   "metadata": {},
   "source": [
    "#### Setup the pipeline\n",
    "To setup the Hugging Face pipeline we set the following:\n",
    "\n",
    "* Choose the Hugging Face task: \"text-generation\"\n",
    "   This Text generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
    "`\"text-generation\"`.\n",
    "* Set the device to \"hpu\" which allows the pipeline to run on Intel Gaudi\n",
    "* Choose model \"gpt2\" and data type to be bf16\n",
    "* Finally we'll use the \"wrap_in_hpu_graph\" to wrap the module forward function with HPU Graphs. This wrapper captures, caches and replays the graph. More info [here](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html).\n",
    "\n",
    "You will see that the Intel Gaudi will build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb88afb-f302-4349-a896-899a2ae0f466",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model = 'gpt2', trust_remote_code=True, torch_dtype=torch.bfloat16, device=\"hpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2289ef-2b6d-4703-9eb0-e2bfb1764d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model = wrap_in_hpu_graph(generator.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f9004-a059-4945-9bf9-0f15b19a623d",
   "metadata": {},
   "source": [
    "#### Execute the pipeline and output the results\n",
    "Here the input prompts are 3 prompts.\n",
    "It is batch inference of batch_size =3, and the outputs are list of 3 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e8acc-57ef-45b5-8631-a6b3c565dfca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = generator(prompts, max_length = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ab4af-065f-4d22-88c6-c34267ba9dc6",
   "metadata": {},
   "source": [
    "Extract each output generated_text and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8d308-a923-4a66-80eb-11fbdd54a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"=== output 0 ===: \\n  {output[0][0]['generated_text']} \\n\")\n",
    "print (f\"=== output 1 ===: \\n  {output[1][0]['generated_text']} \\n\")\n",
    "print (f\"=== output 2 ===: \\n {output[2][0]['generated_text']}  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6c00a-580f-4828-a2d5-dcd0db5348f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
