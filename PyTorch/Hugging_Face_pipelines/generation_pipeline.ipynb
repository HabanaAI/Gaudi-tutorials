{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fb7581-a09a-404f-a5ac-5a77996eafea",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
	"SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157485d-f705-405f-9997-bdcadbc85218",
   "metadata": {},
   "source": [
    "### Using Hugging Face Pipelines on Intel® Gaudi® 2 - Text Generation\n",
    "\n",
    "This example shows how to use the Hugging Face Transformers pipeline API to run text generation task on Intel Gaudi.\n",
    "\n",
    "The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks. Text generation pipeline is one of them.\n",
    "\n",
    "#### Text Generation Pipeline Brief Introduction\n",
    "Text generation pipeline using any ModelWithLMHead. This pipeline predicts the words that will follow a specified text prompt.\n",
    "\n",
    "This language generation pipeline can currently be loaded from pipeline() using the following task identifier: \"text-generation\".\n",
    "\n",
    "The **models that this pipeline can use are models that have been trained with an autoregressive language modeling objective**, which includes the uni-directional models in the library (e.g. gpt2). See the list of available models on [huggingface.co/models](https://huggingface.co/models?filter=text-generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f62c6-a93f-4d11-bff5-12d38b0f4b8b",
   "metadata": {},
   "source": [
    "#### Install the Hugging Face Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1f096-17ee-451e-a356-1e5b2162a83d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Hugging_Face_pipelines\n",
    "!pip install optimum-habana==1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c7a14-3d38-4258-a8de-339bde866b7e",
   "metadata": {},
   "source": [
    "#### Import all neccessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb90147-891d-4ad0-b288-40e1c6673946",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from habana_frameworks.torch.hpu import wrap_in_hpu_graph\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979cbc5",
   "metadata": {},
   "source": [
    "The command below may be needed to modify the existing Hugging Face model classes to use the Intel Gaudi specific version of the model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e47005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.habana.transformers.modeling_utils import adapt_transformers_to_gaudi\n",
    "adapt_transformers_to_gaudi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55e1ae-c373-4bed-a1a8-79ffe6bb0339",
   "metadata": {},
   "source": [
    "#### Prepare the input\n",
    "     We set prompt as a list of 3 prompts, it can do batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edc56bc-73a9-4ccf-bfce-19a962f64efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time, in a land far, far away,\",\n",
    "    \"In the beginning, there was darkness.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2e35f-7752-4187-9c36-7d903460d44f",
   "metadata": {},
   "source": [
    "#### Setup the pipeline\n",
    "To setup the Hugging Face pipeline we set the following:\n",
    "\n",
    "* Choose the Hugging Face task: \"text-generation\"\n",
    "   This Text generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
    "`\"text-generation\"`.\n",
    "* Set the device to \"hpu\" which allows the pipeline to run on Intel Gaudi\n",
    "* Choose model \"gpt2\" and data type to be bf16\n",
    "* Finally we'll use the \"wrap_in_hpu_graph\" to wrap the module forward function with HPU Graphs. This wrapper captures, caches and replays the graph. More info [here](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html).\n",
    "\n",
    "You will see that the Intel Gaudi will build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb88afb-f302-4349-a896-899a2ae0f466",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 10\n",
      "CPU RAM       : 102493984 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model = 'gpt2', trust_remote_code=True, torch_dtype=torch.bfloat16, device=\"hpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a2289ef-2b6d-4703-9eb0-e2bfb1764d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.model = wrap_in_hpu_graph(generator.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f9004-a059-4945-9bf9-0f15b19a623d",
   "metadata": {},
   "source": [
    "#### Execute the pipeline and output the results\n",
    "Here the input prompts are 3 prompts.\n",
    "It is batch inference of batch_size =3, and the outputs are list of 3 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3e8acc-57ef-45b5-8631-a6b3c565dfca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = generator(prompts, max_length = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ab4af-065f-4d22-88c6-c34267ba9dc6",
   "metadata": {},
   "source": [
    "Extract each output generated_text and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c8d308-a923-4a66-80eb-11fbdd54a54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== output 0 ===: \n",
      "  Once upon a time, in a land far, far away, from the Earth, a very beautiful sun had spread forth.\n",
      "\n",
      "The sun that appeared before him had shone brighter than the sun that had eclipsed him. As the sun had risen from the earth, so had the moon risen from the earth, and the stars were risen from the earth. The earth shook, and the sun struck down upon the city of Sodom, and the stars fell upon Sodom, and the city of \n",
      "\n",
      "=== output 1 ===: \n",
      "  In the beginning, there was darkness. The night was full of snow, which made the man's mouth water with blood.\n",
      "\n",
      "Quran: There were many in the city. People were dancing from head to toe. They were in need of a place to hide. The police came to their rescue and helped them. They thought they could kill the man and get away with it. But when the police arrived, they did nothing.\n",
      "\n",
      "Quran: Allah and his messenger said: \" \n",
      "\n",
      "=== output 2 ===: \n",
      " The quick brown fox jumps over the lazy dog. 'Have a look,' he says. 'He loves to find new things.' It is a joke, but in fact he is in love.\n",
      "\n",
      "JAMES L. HILL / AFP - Getty Images\n",
      "\n",
      "A brown fox who is just starting to get settled in, gets along well with a rabbit's good looks, and talks nice. He is known to be a good hunter of rabbits.\n",
      "\n",
      "RENTON HARDIN /  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (f\"=== output 0 ===: \\n  {output[0][0]['generated_text']} \\n\")\n",
    "print (f\"=== output 1 ===: \\n  {output[1][0]['generated_text']} \\n\")\n",
    "print (f\"=== output 2 ===: \\n {output[2][0]['generated_text']}  \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6c00a-580f-4828-a2d5-dcd0db5348f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
