{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2956a8d-b3d3-4e23-b32d-b1f0021d65c9",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c961b3-85f8-4047-bb33-255f3e98b604",
   "metadata": {},
   "source": [
    "## Translation using Hugging Face Pipelines on the Intel&reg; Gaudi&reg; 2 AI Acclerator\n",
    "This tutorial will show how to run translation tasks using Hugging Face pipelines.  We'll show a simple example to a more complex example where we fine tune the t5 model with a specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acee050-cc39-4303-b64a-513fba5be7f0",
   "metadata": {},
   "source": [
    "#### Installation and Setup\n",
    "Install the Hugging Face Optimum for Intel® Gaudi® Accelerators library and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c46ed-7504-4fbe-89f6-681c0fcf8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Hugging_Face_pipelines\n",
    "!pip install --quiet optimum-habana==1.13.2\n",
    "!git clone -b v1.13.2 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424732a-418e-4a3c-b6f5-5b9f915d6842",
   "metadata": {},
   "source": [
    "Install DeepSpeed for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8516b917-8028-4978-bdb0-bb4da370cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet git+https://github.com/HabanaAI/DeepSpeed.git@1.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5ca36-1a11-4694-8861-47a3fa6b4780",
   "metadata": {},
   "source": [
    "In this case, we'll be using the \"Translation\" Task example from the Hugging Face Examples directory, so we'll go to this directory and install the specific requiremetns and create the directory to hold the fine-tuned model.  For this example, the fine tuning has already been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474f616-b49d-4862-9492-50b5531b6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd optimum-habana/examples/translation\n",
    "!pip install -r requirements.txt\n",
    "!pip install pickleshare\n",
    "!mkdir finetune_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff00219-1707-427c-b593-26644ee9f965",
   "metadata": {},
   "source": [
    "#### Simple Example using the Hugging Face Pipeline on Intel Gaudi\n",
    "In this case, the example below just shows the simple setup of the Hugging Face pipeline for the translation task and runs inference only.  Note that the pipelne sets the `device=\"hpu\"` to ensure that the infernece is running on the Intel Gaudi AI Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845d5fc5-27c8-4e4f-aee1-988a04f65c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Bonjour, je voudrais aller au magasin pour obtenir du lait.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "text = \"translate English to French: Good Morning, I'd like to run to the store to get some milk.\"\n",
    "translator_pipe = pipeline(\"translation_xx_to_yy\", model=\"t5-small\", device=\"hpu\", torch_dtype=torch.bfloat16, max_length=200)\n",
    "translator_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e210b-c3b7-4aa4-891e-9f8b3692208a",
   "metadata": {},
   "source": [
    "#### Fine Tuning with DeepSpeed\n",
    "We now run the Fine Tuning of the t5 model with the English-German Dataset wmt14-en-de-pre-processed, we'll take the output of the model for inference.  To accelerate the fine tuning, we'll use DeepSpeed and eight Intel Gaudi Accelerators.   Note the Intel Gaudi speific commands used.  \n",
    "    --use_habana  \n",
    "    --use_lazy_mode  \n",
    "    --use_hpu_graphs_for_training  \n",
    "    \n",
    "For more information you can refer to the Hugging Face Translation example [here](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc95a6-1005-4a44-aca1-ff5f6f389f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../gaudi_spawn.py \\\n",
    "    --world_size 4 --use_deepspeed run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --source_lang en \\\n",
    "    --target_lang de \\\n",
    "    --source_prefix \"translate English to German: \" \\\n",
    "    --dataset_name stas/wmt14-en-de-pre-processed \\\n",
    "    --output_dir ./finetune_model_output \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps=20000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --predict_with_generate \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --report_to none \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/t5 \\\n",
    "    --ignore_pad_token_for_loss False \\\n",
    "    --pad_to_max_length \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1a437-100c-4fcb-a026-c42dd99e2f12",
   "metadata": {},
   "source": [
    "Now that the model is fine Tuned, you can see the updated model in the `./finetune_model_output` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3cee3d-8f70-4525-b667-3e555305b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/translation/finetune_model_output\n",
      "total 436416\n",
      "drwxr-xr-x 3 root root     12288 May 24 05:38 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 6 root root      4096 May 23 15:59 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root       313 May 24 01:46 all_results.json\n",
      "-rw-r--r-- 1 root root      1503 May 24 01:46 config.json\n",
      "-rw-r--r-- 1 root root       247 May 24 01:46 gaudi_config.json\n",
      "-rw-r--r-- 1 root root       501 May 24 01:46 generation_config.json\n",
      "-rw-r--r-- 1 root root 219726224 May 24 01:46 model.safetensors\n",
      "drwxr-xr-x 4 root root      4096 May 23 16:03 \u001b[01;34mruns\u001b[0m/\n",
      "-rw-r--r-- 1 root root      2543 May 24 01:46 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root    791656 May 24 01:46 spiece.model\n",
      "-rw-r--r-- 1 root root     20746 May 24 01:46 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root   2422191 May 24 01:46 tokenizer.json\n",
      "-rw-r--r-- 1 root root    243191 May 24 01:46 trainer_state.json\n",
      "-rw-r--r-- 1 root root      5752 May 24 01:46 training_args.bin\n",
      "-rw-r--r-- 1 root root       313 May 24 01:46 train_results.json\n",
      "-rw-r--r-- 1 root root 223610880 May 24 05:38 \u001b[01;31mtranslation_finetuned.tar\u001b[0m\n",
      "/root/optimum-habana/examples/translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%cd finetune_model_output\n",
    "%ls -al\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a9369-a2f4-4f76-b406-4a3a9b7db1ff",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Now we'll enter a prompt for the simple setup of Hugging Face Translation pipeline using the new Fine Tuned tuned model.  If you want to skip the Fine Tuning, you can just change the `path_to_local_model=\"t5-small\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abbcd12-c8eb-4043-95ff-9cb49dddf619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence for translation from English to German:  I want to eat pizza and play soccer all night long.\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a sentence for translation from English to German: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0feb12b6-a24b-4773-a5c1-58ee6ef06ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I want to eat pizza and play soccer all night long.\n",
      "German: Ich möchte Pizza essen und Fußball die ganze Nacht spielen.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Point to the location of the fine-tuned model, If you want to skip the fine tuning step and just run the T5 model direclty, comment out the first line and uncomment the second line:  \n",
    "path_to_local_model = \"./finetune_model_output\"\n",
    "#path_to_local_model =\"t5-small\"\n",
    "\n",
    "# Load the tokenizer and model from the specified local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_local_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_local_model)\n",
    "\n",
    "# Create the Hugging Face pipeline with the input prompt\n",
    "text = f\"translate English to German: {prompt}\"\n",
    "translator_pipeline = pipeline(\"translation_xx_to_yy\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16, max_length=150)\n",
    "output = translator_pipeline(text)\n",
    "\n",
    "# Print the results:\n",
    "print(f\"English: {prompt}\")\n",
    "print(f\"German: {output[0]['translation_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfc047-6a29-4a88-a467-002be42cc729",
   "metadata": {},
   "source": [
    "#### Simple Gradio Front End for Translation\n",
    "In this final example, we'll move the Hugging Face pipeline into a Gradio user interface to make it easier to have ongoing translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9076230e-d864-4175-b5ac-44895afcc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio>=4.31.5\n",
    "%load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727a3142-a273-438c-8614-ef4a79732252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 152\n",
      "CPU RAM       : 1056439948 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Point to the location of the fine-tuned model, If you want to skip the fine tuning step and just run the T5 model direclty, comment out the first line and uncomment the second line:  \n",
    "path_to_local_model = \"./finetune_model_output\"\n",
    "#path_to_local_model =\"t5-small\"\n",
    "\n",
    "# Load the tokenizer and model from the specified local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_local_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_local_model)\n",
    "\n",
    "# Create the translation pipeline\n",
    "translator_pipeline = pipeline(\"translation_xx_to_yy\", model=path_to_local_model, device=\"hpu\", tokenizer=tokenizer, torch_dtype=torch.bfloat16, max_length=500)\n",
    "\n",
    "def text_gen(inputs):\n",
    "    # Format the input text for translation\n",
    "    text = f\"translate English to German: {inputs}\"\n",
    "    outputs = translator_pipeline(text)\n",
    "\n",
    "    # Extract and return the translation result\n",
    "    return outputs[0]['translation_text']\n",
    "\n",
    "inputs = gr.Textbox(label=\"Prompt\", value=\"I'd like to order a hamburger and a cold glass of beer\")\n",
    "outputs = gr.Markdown(label=\"Response\")\n",
    "\n",
    "demo = gr.Interface(\n",
    "        fn=text_gen,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        title=\"Translation on Intel&reg; Gaudi&reg; 2\", \n",
    "        description=\"Have a chat with Intel Gaudi\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808eb2dd-68ab-4845-b87c-20ac709cae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
