{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2956a8d-b3d3-4e23-b32d-b1f0021d65c9",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c961b3-85f8-4047-bb33-255f3e98b604",
   "metadata": {},
   "source": [
    "## Translation using Hugging Face Pipelines on the Intel&reg; Gaudi&reg; 2 AI Acclerator\n",
    "This tutorial will show how to run translation tasks using Hugging Face pipelines.  We'll show a simple example to a more complex example where we fine tune the t5 model with a specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the `exit()` command to re-start the Python kernel to ensure that there are no other proceses holding the Intel Gaudi Accelerator as you start to run this notebook.  \n",
    "# You will see a warning that the kernel has died, this is expected.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acee050-cc39-4303-b64a-513fba5be7f0",
   "metadata": {},
   "source": [
    "#### Installation and Setup\n",
    "Install the Hugging Face Optimum for Intel® Gaudi® Accelerators library and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c46ed-7504-4fbe-89f6-681c0fcf8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/\n",
    "!git clone -b v1.14.0 https://github.com/huggingface/optimum-habana.git\n",
    "!pip install --quiet optimum-habana==1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424732a-418e-4a3c-b6f5-5b9f915d6842",
   "metadata": {},
   "source": [
    "Install DeepSpeed for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516b917-8028-4978-bdb0-bb4da370cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet git+https://github.com/HabanaAI/DeepSpeed.git@1.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5ca36-1a11-4694-8861-47a3fa6b4780",
   "metadata": {},
   "source": [
    "In this case, we'll be using the \"Translation\" Task example from the Hugging Face Examples directory, so we'll go to this directory and install the specific requiremetns and create the directory to hold the fine-tuned model.  For this example, the fine tuning has already been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474f616-b49d-4862-9492-50b5531b6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/translation\n",
    "!pip install -r requirements.txt\n",
    "!pip install pickleshare\n",
    "!mkdir finetune_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff00219-1707-427c-b593-26644ee9f965",
   "metadata": {},
   "source": [
    "#### Simple Example using the Hugging Face Pipeline on Intel Gaudi\n",
    "In this case, the example below just shows the simple setup of the Hugging Face pipeline for the translation task and runs inference only.  Note that the pipelne sets the `device=\"hpu\"` to ensure that the inference is running on the Intel Gaudi AI Accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845d5fc5-27c8-4e4f-aee1-988a04f65c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056412772 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Bonjour, je voudrais aller au magasin pour obtenir du lait.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "text = \"translate English to French: Good Morning, I'd like to run to the store to get some milk.\"\n",
    "translator_pipe = pipeline(\"translation_xx_to_yy\", model=\"t5-small\", device=\"hpu\", torch_dtype=torch.bfloat16, max_length=200)\n",
    "translator_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e210b-c3b7-4aa4-891e-9f8b3692208a",
   "metadata": {},
   "source": [
    "#### Fine Tuning with DeepSpeed\n",
    "We now run the Fine Tuning of the t5 model with the English-German Dataset wmt14-en-de-pre-processed, we'll take the output of the model for inference.  To accelerate the fine tuning, we'll use DeepSpeed and eight Intel Gaudi Accelerators.   Note the Intel Gaudi speific commands used.  \n",
    "    --use_habana  \n",
    "    --use_lazy_mode  \n",
    "    --use_hpu_graphs_for_training  \n",
    "    \n",
    "For more information you can refer to the Hugging Face Translation example [here](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc95a6-1005-4a44-aca1-ff5f6f389f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 4 --no_local_rank --master_port 29500 run_translation.py --model_name_or_path t5-small --do_train --do_eval --source_lang en --target_lang de --source_prefix \"translate English to German: \" --dataset_name stas/wmt14-en-de-pre-processed --output_dir ./finetune_model_output --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --overwrite_output_dir --save_steps=20000 --save_total_limit=3 --predict_with_generate --use_habana --use_lazy_mode --report_to none --use_hpu_graphs_for_training --gaudi_config_name Habana/t5 --ignore_pad_token_for_loss False --pad_to_max_length --throughput_warmup_steps 3 --bf16\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:159: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-10-19 22:52:25,126] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-10-19 22:52:26,163] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-10-19 22:52:26,239] [INFO] [runner.py:568:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None run_translation.py --model_name_or_path t5-small --do_train --do_eval --source_lang en --target_lang de --source_prefix translate English to German:  --dataset_name stas/wmt14-en-de-pre-processed --output_dir ./finetune_model_output --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --overwrite_output_dir --save_steps=20000 --save_total_limit=3 --predict_with_generate --use_habana --use_lazy_mode --report_to none --use_hpu_graphs_for_training --gaudi_config_name Habana/t5 --ignore_pad_token_for_loss False --pad_to_max_length --throughput_warmup_steps 3 --bf16\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:159: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-10-19 22:52:28,039] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-10-19 22:52:29,068] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-10-19 22:52:29,068] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-10-19 22:52:29,068] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-10-19 22:52:29,068] [INFO] [launch.py:164:main] dist_world_size=4\n",
      "[2024-10-19 22:52:29,068] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2024-10-19 22:52:29,069] [INFO] [launch.py:256:main] process 1802 spawned with command: ['/usr/bin/python3', '-u', 'run_translation.py', '--model_name_or_path', 't5-small', '--do_train', '--do_eval', '--source_lang', 'en', '--target_lang', 'de', '--source_prefix', 'translate English to German: ', '--dataset_name', 'stas/wmt14-en-de-pre-processed', '--output_dir', './finetune_model_output', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--save_steps=20000', '--save_total_limit=3', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--report_to', 'none', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--throughput_warmup_steps', '3', '--bf16']\n",
      "[2024-10-19 22:52:29,069] [INFO] [launch.py:256:main] process 1803 spawned with command: ['/usr/bin/python3', '-u', 'run_translation.py', '--model_name_or_path', 't5-small', '--do_train', '--do_eval', '--source_lang', 'en', '--target_lang', 'de', '--source_prefix', 'translate English to German: ', '--dataset_name', 'stas/wmt14-en-de-pre-processed', '--output_dir', './finetune_model_output', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--save_steps=20000', '--save_total_limit=3', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--report_to', 'none', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--throughput_warmup_steps', '3', '--bf16']\n",
      "[2024-10-19 22:52:29,069] [INFO] [launch.py:256:main] process 1804 spawned with command: ['/usr/bin/python3', '-u', 'run_translation.py', '--model_name_or_path', 't5-small', '--do_train', '--do_eval', '--source_lang', 'en', '--target_lang', 'de', '--source_prefix', 'translate English to German: ', '--dataset_name', 'stas/wmt14-en-de-pre-processed', '--output_dir', './finetune_model_output', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--save_steps=20000', '--save_total_limit=3', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--report_to', 'none', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--throughput_warmup_steps', '3', '--bf16']\n",
      "[2024-10-19 22:52:29,070] [INFO] [launch.py:256:main] process 1805 spawned with command: ['/usr/bin/python3', '-u', 'run_translation.py', '--model_name_or_path', 't5-small', '--do_train', '--do_eval', '--source_lang', 'en', '--target_lang', 'de', '--source_prefix', 'translate English to German: ', '--dataset_name', 'stas/wmt14-en-de-pre-processed', '--output_dir', './finetune_model_output', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--save_steps=20000', '--save_total_limit=3', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--report_to', 'none', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--throughput_warmup_steps', '3', '--bf16']\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "gaudi_config.json: 100%|█████████████████████| 90.0/90.0 [00:00<00:00, 1.02MB/s]\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "10/19/2024 22:52:36 - WARNING - __main__ - Process rank: 1, device: hpu, distributed training: True, mixed-precision training: True\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "10/19/2024 22:52:37 - WARNING - __main__ - Process rank: 2, device: hpu, distributed training: True, mixed-precision training: True\n",
      "10/19/2024 22:52:37 - WARNING - __main__ - Process rank: 3, device: hpu, distributed training: True, mixed-precision training: True\n",
      "10/19/2024 22:52:37 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: True, mixed-precision training: True\n",
      "10/19/2024 22:52:37 - INFO - __main__ - Training/evaluation parameters GaudiSeq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "compile_dynamic=None,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp8=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/t5,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./finetune_model_output/runs/Oct19_22-52-34_sc09wynn05-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./finetune_model_output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "pipelining_fwd_bwd=False,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./finetune_model_output,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=20000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "use_compiled_autograd=False,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading data: 100%|██████████████████████| 283M/283M [00:09<00:00, 28.4MB/s]\n",
      "Downloading data: 100%|██████████████████████| 305M/305M [00:10<00:00, 27.9MB/s]\n",
      "Downloading data: 100%|██████████████████████| 238M/238M [00:08<00:00, 27.7MB/s]\n",
      "Downloading data: 100%|██████████████████████| 343k/343k [00:00<00:00, 1.39MB/s]\n",
      "Downloading data: 100%|██████████████████████| 475k/475k [00:00<00:00, 2.09MB/s]\n",
      "Generating train split: 100%|█| 4548885/4548885 [00:04<00:00, 1059847.21 example\n",
      "Generating validation split: 100%|█| 2169/2169 [00:00<00:00, 694302.48 examples/\n",
      "Generating test split: 100%|█████| 2999/2999 [00:00<00:00, 913487.12 examples/s]\n",
      "Found cached dataset wmt14-en-de-pre-processed (/root/.cache/huggingface/datasets/stas___wmt14-en-de-pre-processed/ende/1.1.0/7617090058992d01345cbead219b800acd77d3ac)\n",
      "10/19/2024 22:53:14 - INFO - datasets.builder - Found cached dataset wmt14-en-de-pre-processed (/root/.cache/huggingface/datasets/stas___wmt14-en-de-pre-processed/ende/1.1.0/7617090058992d01345cbead219b800acd77d3ac)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/stas___wmt14-en-de-pre-processed/ende/1.1.0/7617090058992d01345cbead219b800acd77d3ac\n",
      "10/19/2024 22:53:14 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/stas___wmt14-en-de-pre-processed/ende/1.1.0/7617090058992d01345cbead219b800acd77d3ac\n",
      "[INFO|configuration_utils.py:675] 2024-10-19 22:53:15,074 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:742] 2024-10-19 22:53:15,075 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-19 22:53:15,170 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-19 22:53:15,170 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-19 22:53:15,170 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-19 22:53:15,171 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2206] 2024-10-19 22:53:15,171 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:3732] 2024-10-19 22:53:15,240 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "[INFO|configuration_utils.py:1099] 2024-10-19 22:53:15,244 >> Generate config GaudiGenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4574] 2024-10-19 22:53:15,385 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4582] 2024-10-19 22:53:15,385 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1054] 2024-10-19 22:53:15,483 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "[INFO|configuration_utils.py:1099] 2024-10-19 22:53:15,483 >> Generate config GaudiGenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0%|    | 0/4548885 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/stas___wmt14-en-de-pre-processed/ende/1.1.0/7617090058992d01345cbead219b800acd77d3ac/cache-0789c65a38d8fe26.arrow\n",
      "10/19/2024 22:53:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/stas___wmt14-en-de-pre-processed/ende/1.1.0/7617090058992d01345cbead219b800acd77d3ac/cache-0789c65a38d8fe26.arrow\n",
      "Running tokenizer on train dataset:  96%|▉| 4355000/4548885 [28:14<01:24, 2287.2"
     ]
    }
   ],
   "source": [
    "!python3 ../gaudi_spawn.py \\\n",
    "    --world_size 4 --use_deepspeed run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --source_lang en \\\n",
    "    --target_lang de \\\n",
    "    --source_prefix \"translate English to German: \" \\\n",
    "    --dataset_name stas/wmt14-en-de-pre-processed \\\n",
    "    --output_dir ./finetune_model_output \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps=20000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --predict_with_generate \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --report_to none \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/t5 \\\n",
    "    --ignore_pad_token_for_loss False \\\n",
    "    --pad_to_max_length \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1a437-100c-4fcb-a026-c42dd99e2f12",
   "metadata": {},
   "source": [
    "Now that the model is fine Tuned, you can see the updated model in the `./finetune_model_output` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3cee3d-8f70-4525-b667-3e555305b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/translation/finetune_model_output\n",
      "total 436416\n",
      "drwxr-xr-x 3 root root     12288 May 24 05:38 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 6 root root      4096 May 23 15:59 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root       313 May 24 01:46 all_results.json\n",
      "-rw-r--r-- 1 root root      1503 May 24 01:46 config.json\n",
      "-rw-r--r-- 1 root root       247 May 24 01:46 gaudi_config.json\n",
      "-rw-r--r-- 1 root root       501 May 24 01:46 generation_config.json\n",
      "-rw-r--r-- 1 root root 219726224 May 24 01:46 model.safetensors\n",
      "drwxr-xr-x 4 root root      4096 May 23 16:03 \u001b[01;34mruns\u001b[0m/\n",
      "-rw-r--r-- 1 root root      2543 May 24 01:46 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root    791656 May 24 01:46 spiece.model\n",
      "-rw-r--r-- 1 root root     20746 May 24 01:46 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root   2422191 May 24 01:46 tokenizer.json\n",
      "-rw-r--r-- 1 root root    243191 May 24 01:46 trainer_state.json\n",
      "-rw-r--r-- 1 root root      5752 May 24 01:46 training_args.bin\n",
      "-rw-r--r-- 1 root root       313 May 24 01:46 train_results.json\n",
      "-rw-r--r-- 1 root root 223610880 May 24 05:38 \u001b[01;31mtranslation_finetuned.tar\u001b[0m\n",
      "/root/optimum-habana/examples/translation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%cd finetune_model_output\n",
    "%ls -al\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a9369-a2f4-4f76-b406-4a3a9b7db1ff",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Now we'll enter a prompt for the simple setup of Hugging Face Translation pipeline using the new Fine Tuned tuned model.  If you want to skip the Fine Tuning, you can just change the `path_to_local_model=\"t5-small\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abbcd12-c8eb-4043-95ff-9cb49dddf619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence for translation from English to German:  I want to eat pizza and play soccer all night long.\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a sentence for translation from English to German: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0feb12b6-a24b-4773-a5c1-58ee6ef06ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: I want to eat pizza and play soccer all night long.\n",
      "German: Ich möchte Pizza essen und Fußball die ganze Nacht spielen.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Point to the location of the fine-tuned model, If you want to skip the fine tuning step and just run the T5 model direclty, comment out the first line and uncomment the second line:  \n",
    "path_to_local_model = \"./finetune_model_output\"\n",
    "#path_to_local_model =\"t5-small\"\n",
    "\n",
    "# Load the tokenizer and model from the specified local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_local_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_local_model)\n",
    "\n",
    "# Create the Hugging Face pipeline with the input prompt\n",
    "text = f\"translate English to German: {prompt}\"\n",
    "translator_pipeline = pipeline(\"translation_xx_to_yy\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16, max_length=150)\n",
    "output = translator_pipeline(text)\n",
    "\n",
    "# Print the results:\n",
    "print(f\"English: {prompt}\")\n",
    "print(f\"German: {output[0]['translation_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfc047-6a29-4a88-a467-002be42cc729",
   "metadata": {},
   "source": [
    "#### Simple Gradio Front End for Translation\n",
    "In this final example, we'll move the Hugging Face pipeline into a Gradio user interface to make it easier to have ongoing translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9076230e-d864-4175-b5ac-44895afcc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio>=4.31.5\n",
    "%load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727a3142-a273-438c-8614-ef4a79732252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 152\n",
      "CPU RAM       : 1056439948 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Point to the location of the fine-tuned model, If you want to skip the fine tuning step and just run the T5 model direclty, comment out the first line and uncomment the second line:  \n",
    "path_to_local_model = \"./finetune_model_output\"\n",
    "#path_to_local_model =\"t5-small\"\n",
    "\n",
    "# Load the tokenizer and model from the specified local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_local_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_local_model)\n",
    "\n",
    "# Create the translation pipeline\n",
    "translator_pipeline = pipeline(\"translation_xx_to_yy\", model=path_to_local_model, device=\"hpu\", tokenizer=tokenizer, torch_dtype=torch.bfloat16, max_length=500)\n",
    "\n",
    "def text_gen(inputs):\n",
    "    # Format the input text for translation\n",
    "    text = f\"translate English to German: {inputs}\"\n",
    "    outputs = translator_pipeline(text)\n",
    "\n",
    "    # Extract and return the translation result\n",
    "    return outputs[0]['translation_text']\n",
    "\n",
    "inputs = gr.Textbox(label=\"Prompt\", value=\"I'd like to order a hamburger and a cold glass of beer\")\n",
    "outputs = gr.Markdown(label=\"Response\")\n",
    "\n",
    "demo = gr.Interface(\n",
    "        fn=text_gen,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        title=\"Translation on Intel&reg; Gaudi&reg; 2\", \n",
    "        description=\"Have a chat with Intel Gaudi\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808eb2dd-68ab-4845-b87c-20ac709cae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
