{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2956a8d-b3d3-4e23-b32d-b1f0021d65c9",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c961b3-85f8-4047-bb33-255f3e98b604",
   "metadata": {},
   "source": [
    "## Translation using Hugging Face Pipelines on the Intel&reg; Gaudi&reg; 2 AI Acclerator\n",
    "This tutorial will show how to run translation tasks using Hugging Face pipelines.  We'll show a simple example to a more complex example where we fine tune the t5 model with a specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc0bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the `exit()` command to re-start the Python kernel to ensure that there are no other proceses holding the Intel Gaudi Accelerator as you start to run this notebook.  \n",
    "# You will see a warning that the kernel has died, this is expected.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acee050-cc39-4303-b64a-513fba5be7f0",
   "metadata": {},
   "source": [
    "#### Installation and Setup\n",
    "Install the Hugging Face Optimum for Intel® Gaudi® Accelerators library and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c46ed-7504-4fbe-89f6-681c0fcf8ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.16.0 https://github.com/huggingface/optimum-habana.git\n",
    "!pip install --quiet optimum-habana==1.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424732a-418e-4a3c-b6f5-5b9f915d6842",
   "metadata": {},
   "source": [
    "Install DeepSpeed for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8516b917-8028-4978-bdb0-bb4da370cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet git+https://github.com/HabanaAI/DeepSpeed.git@1.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5ca36-1a11-4694-8861-47a3fa6b4780",
   "metadata": {},
   "source": [
    "In this case, we'll be using the \"Translation\" Task example from the Hugging Face Examples directory, so we'll go to this directory and install the specific requiremetns and create the directory to hold the fine-tuned model.  For this example, the fine tuning has already been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474f616-b49d-4862-9492-50b5531b6a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/translation\n",
    "!pip install -r requirements.txt\n",
    "!pip install pickleshare\n",
    "!mkdir finetune_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff00219-1707-427c-b593-26644ee9f965",
   "metadata": {},
   "source": [
    "#### Simple Example using the Hugging Face Pipeline on Intel Gaudi\n",
    "In this case, the example below just shows the simple setup of the Hugging Face pipeline for the translation task and runs inference only.  Note that the pipelne sets the `device=\"hpu\"` to ensure that the inference is running on the Intel Gaudi AI Accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d5fc5-27c8-4e4f-aee1-988a04f65c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable PT_HPU_LAZY_MODE=1\n",
    "import os\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '1'\n",
    "\n",
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "text = \"translate English to French: Good Morning, I'd like to run to the store to get some milk.\"\n",
    "translator_pipe = pipeline(\"translation_xx_to_yy\", model=\"t5-small\", device=\"hpu\", torch_dtype=torch.bfloat16, max_length=200)\n",
    "translator_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e210b-c3b7-4aa4-891e-9f8b3692208a",
   "metadata": {},
   "source": [
    "#### Fine Tuning with DeepSpeed\n",
    "We now run the Fine Tuning of the t5 model with the English-German Dataset wmt14-en-de-pre-processed, we'll take the output of the model for inference.  To accelerate the fine tuning, we'll use DeepSpeed and eight Intel Gaudi Accelerators.   Note the Intel Gaudi speific commands used.  \n",
    "    --use_habana  \n",
    "    --use_lazy_mode  \n",
    "    --use_hpu_graphs_for_training  \n",
    "    \n",
    "For more information you can refer to the Hugging Face Translation example [here](https://github.com/huggingface/optimum-habana/tree/main/examples/translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc95a6-1005-4a44-aca1-ff5f6f389f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PT_HPU_LAZY_MODE=1 python3 ../gaudi_spawn.py \\\n",
    "    --world_size 4 --use_deepspeed run_translation.py \\\n",
    "    --model_name_or_path t5-small \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --source_lang en \\\n",
    "    --target_lang de \\\n",
    "    --source_prefix \"translate English to German: \" \\\n",
    "    --dataset_name stas/wmt14-en-de-pre-processed \\\n",
    "    --output_dir ./finetune_model_output \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --save_steps=20000 \\\n",
    "    --save_total_limit=3 \\\n",
    "    --predict_with_generate \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --report_to none \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/t5 \\\n",
    "    --ignore_pad_token_for_loss False \\\n",
    "    --pad_to_max_length \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1a437-100c-4fcb-a026-c42dd99e2f12",
   "metadata": {},
   "source": [
    "Now that the model is fine Tuned, you can see the updated model in the `./finetune_model_output` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cee3d-8f70-4525-b667-3e555305b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd finetune_model_output\n",
    "%ls -al\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a9369-a2f4-4f76-b406-4a3a9b7db1ff",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "Now we'll enter a prompt for the simple setup of Hugging Face Translation pipeline using the new Fine Tuned tuned model.  If you want to skip the Fine Tuning, you can just change the `path_to_local_model=\"t5-small\"`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbcd12-c8eb-4043-95ff-9cb49dddf619",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = input(\"Enter a sentence for translation from English to German: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb12b6-a24b-4773-a5c1-58ee6ef06ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Point to the location of the fine-tuned model, If you want to skip the fine tuning step and just run the T5 model direclty, comment out the first line and uncomment the second line:  \n",
    "path_to_local_model = \"./finetune_model_output\"\n",
    "#path_to_local_model =\"t5-small\"\n",
    "\n",
    "# Load the tokenizer and model from the specified local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_local_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_local_model)\n",
    "\n",
    "# Create the Hugging Face pipeline with the input prompt\n",
    "text = f\"translate English to German: {prompt}\"\n",
    "translator_pipeline = pipeline(\"translation_xx_to_yy\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16, max_length=150)\n",
    "output = translator_pipeline(text)\n",
    "\n",
    "# Print the results:\n",
    "print(f\"English: {prompt}\")\n",
    "print(f\"German: {output[0]['translation_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfc047-6a29-4a88-a467-002be42cc729",
   "metadata": {},
   "source": [
    "#### Simple Gradio Front End for Translation\n",
    "In this final example, we'll move the Hugging Face pipeline into a Gradio user interface to make it easier to have ongoing translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9076230e-d864-4175-b5ac-44895afcc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio>=4.31.5\n",
    "%load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a3142-a273-438c-8614-ef4a79732252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Point to the location of the fine-tuned model, If you want to skip the fine tuning step and just run the T5 model direclty, comment out the first line and uncomment the second line:  \n",
    "path_to_local_model = \"./finetune_model_output\"\n",
    "#path_to_local_model =\"t5-small\"\n",
    "\n",
    "# Load the tokenizer and model from the specified local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_local_model)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(path_to_local_model)\n",
    "\n",
    "# Create the translation pipeline\n",
    "translator_pipeline = pipeline(\"translation_xx_to_yy\", model=path_to_local_model, device=\"hpu\", tokenizer=tokenizer, torch_dtype=torch.bfloat16, max_length=500)\n",
    "\n",
    "def text_gen(inputs):\n",
    "    # Format the input text for translation\n",
    "    text = f\"translate English to German: {inputs}\"\n",
    "    outputs = translator_pipeline(text)\n",
    "\n",
    "    # Extract and return the translation result\n",
    "    return outputs[0]['translation_text']\n",
    "\n",
    "inputs = gr.Textbox(label=\"Prompt\", value=\"I'd like to order a hamburger and a cold glass of beer\")\n",
    "outputs = gr.Markdown(label=\"Response\")\n",
    "\n",
    "demo = gr.Interface(\n",
    "        fn=text_gen,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        title=\"Translation on Intel&reg; Gaudi&reg; 2\", \n",
    "        description=\"Have a chat with Intel Gaudi\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "808eb2dd-68ab-4845-b87c-20ac709cae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
