{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fb7581-a09a-404f-a5ac-5a77996eafea",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157485d-f705-405f-9997-bdcadbc85218",
   "metadata": {},
   "source": [
    "### Running Hugging Face with FP8 on Intel® Gaudi®  - Text Generation\n",
    "\n",
    "This example shows how to quantize a Hugging Face models from fp32 to fp8 with Intel Gaudi and the Optimum for Intel Gaudi (aka Optimum Habana) library.\n",
    "\n",
    "Llama2-70b, Llama2-7b, Llama3-70b, Llama3-8b, Mixtral-8x7B, Falcon-7B, Falcon-40B, Falcon-180B, phi-2 and Llama3-405B in FP8 are enabled using the [Intel Neural Compressor (INC)](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_FP8.html), which provides model measurement and quantization capabilities in PyTorch. From synapse 1.17 / optimum-habana 1.13 release, INC is used by default for measuring and quantization. Habana Quantization Toolkit (HQT), which was used earlier, will be removed in future releases. To use HQT, disable INC by setting the following environment variable: `USE_INC=0`.\n",
    "\n",
    "More information on enabling fp8 in SynapseAI is available here:\n",
    "https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_FP8.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f62c6-a93f-4d11-bff5-12d38b0f4b8b",
   "metadata": {},
   "source": [
    "#### Install the Hugging Face Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1f096-17ee-451e-a356-1e5b2162a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd ~/Gaudi-tutorials/PyTorch/Hugging_Face_pipelines/Benchmarking_on_Optimum-habana_with_fp8\n",
    "%pip install optimum-habana==1.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257d27b-c742-4290-a477-a0d48f5797bb",
   "metadata": {},
   "source": [
    "#### Download the Hugging Face Optimum Habana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f0405-70ee-4225-acf4-91340678c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b v1.16.0 https://github.com/huggingface/optimum-habana.git;cd optimum-habana/examples/text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e181d2-fa9e-4193-90ca-1ca5d6668101",
   "metadata": {},
   "source": [
    "#### Install Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f25b4d-47a0-42d7-af30-4216f16d2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt;pip install -r requirements_lm_eval.txt;pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.20.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c7a14-3d38-4258-a8de-339bde866b7e",
   "metadata": {},
   "source": [
    "#### Measure the tensor quantization statistics \n",
    "Here is an example to measure the tensor quantization statistics on Llama3-8B with 1 card:  \n",
    "By changing model_name_or_path, a different llama model could be applied.  \n",
    "By changing world_size, multiple gaudi cards could be used for measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb90147-891d-4ad0-b288-40e1c6673946",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!HF_DATASETS_TRUST_REMOTE_CODE=true QUANT_CONFIG=./quantization_config/maxabs_measure.json python ../gaudi_spawn.py \\\n",
    "--use_deepspeed --world_size 1 run_lm_eval.py \\\n",
    "-o acc_llama3_8b_bs1_quant.txt \\\n",
    "--model_name_or_path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "--warmup 0 \\\n",
    "--use_hpu_graphs \\\n",
    "--use_kv_cache \\\n",
    "--trim_logits \\\n",
    "--batch_size 1 \\\n",
    "--bucket_size=128 \\\n",
    "--bucket_internal \\\n",
    "--trust_remote_code \\\n",
    "--tasks hellaswag lambada_openai piqa winogrande \\\n",
    "--bf16 \\\n",
    "--attn_softmax_bf16 \\\n",
    "--use_flash_attention \\\n",
    "--flash_attention_recompute \\\n",
    "--flash_attention_causal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55e1ae-c373-4bed-a1a8-79ffe6bb0339",
   "metadata": {},
   "source": [
    "#### Quantize and run the fp8 model\n",
    "Here is an example to quantize the model based on previous measurements for LLama3.1 8B model:  \n",
    "By changing model_name_or_path, a different llama model could be applied.  \n",
    "By changing world_size, multiple gaudi cards could be used for measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edc56bc-73a9-4ccf-bfce-19a962f64efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HF_DATASETS_TRUST_REMOTE_CODE=true QUANT_CONFIG=./quantization_config/maxabs_quant.json python ../gaudi_spawn.py \\\n",
    "--use_deepspeed --world_size 1 run_generation.py \\\n",
    "--model_name_or_path meta-llama/Llama-3.1-8B-Instruct \\\n",
    "--attn_softmax_bf16 \\\n",
    "--use_hpu_graphs \\\n",
    "--use_kv_cache \\\n",
    "--limit_hpu_graphs \\\n",
    "--max_input_tokens 128 \\\n",
    "--max_new_tokens 128 \\\n",
    "--batch_size 1536 \\\n",
    "--bucket_size=128 \\\n",
    "--bucket_internal \\\n",
    "--attn_batch_split 2 \\\n",
    "--bf16 \\\n",
    "--reuse_cache \\\n",
    "--trim_logits \\\n",
    "--use_flash_attention \\\n",
    "--flash_attention_recompute \\\n",
    "--flash_attention_causal_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
