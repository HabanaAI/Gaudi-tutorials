{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f52f444-9a84-449c-b0c7-d26f3ff5961d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366e601-bdd4-46cd-b972-54ffacbe0f33",
   "metadata": {},
   "source": [
    "## Using Hugging Face Pipelines on Intel&reg; Gaudi&reg; 2 - Visual Question Answering\n",
    "This section showcases how to use the Hugging Face Transformers pipeline API to run visual question answering task on Intel Gaudi.\n",
    "\n",
    "Hugging Face pipeline is an easy way to use models for inference. It is objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks. We choose \"visual-question-answering\" task here.    \n",
    "Pipeline workflow is defined as a sequence of the following operations:\n",
    "\n",
    "        Input -> Tokenization -> Model Inference -> Post-Processing (Task dependent) -> Output\n",
    "Pipeline supports running on CPU or GPU through the device argument. Users can specify device argument, for example, we set device=\"hpu\" in this case. \"adapt_transformers_to_gaudi\" will replace some Transformers' methods for equivalent methods optimized for Intel Gaudi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48300c4-b6d1-4298-a278-49ff0876b11a",
   "metadata": {},
   "source": [
    "### The First step is Install the Hugging Face Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37b8e4-d840-4a54-9b46-5d3f90173e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Hugging_Face_pipelines\n",
    "!pip install optimum-habana==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99abce9-8491-4677-b9cb-dd38b1f2b8b1",
   "metadata": {},
   "source": [
    "### Import all neccessary dependencies and call adapt_transformers_to_gaudi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82003c0-8fc0-44a3-a41f-e32c010171b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import requests\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from optimum.habana.transformers.modeling_utils import adapt_transformers_to_gaudi\n",
    "from habana_frameworks.torch.hpu import wrap_in_hpu_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613b5a4",
   "metadata": {},
   "source": [
    "The command below may be needed to modify the existing Hugging Face model classes to use the Intel Gaudi specific version of the model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d694813-8657-468d-967e-e39cc4a057ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_transformers_to_gaudi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c819f6-e24e-46f7-b267-072094b7e130",
   "metadata": {},
   "source": [
    "### Download and initialize the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ca9db-d248-4ffc-a7f9-53685b5fd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"https://free-images.com/md/3478/woman_dog_pacsi_paw.jpg\"\n",
    "image = PIL.Image.open(requests.get(image_path, stream=True, timeout=3000).raw).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a5c875-7d49-42e3-8063-4848fc6f52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the image, we'll be able to ask questions about this picture and get an answer\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b71930-13b0-4e3b-8dc7-295d98e2f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Enter a question about the image above: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38953f0-fb3a-4a0a-8159-98327f0be2cd",
   "metadata": {},
   "source": [
    "Enter a question about the image above:  What is happening in this picture?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "140d4c3b-a142-4146-9a4a-7fee8437e018",
   "metadata": {},
   "source": [
    "### Setup the pipeline\n",
    "To setup the Hugging Face pipeline we set the following  \n",
    "1. choose the Hugging Face task: for this, we use visual-question-answering.\n",
    "2. Set the device to \"hpu\" which allows the pipeline to run on Intel Gaudi\n",
    "3. Choose model \"Salesforce/blip-vqa-capfilt-large\" and data type to be bf16  \n",
    "\n",
    "Finally we'll use the \"wrap_in_hpu_graph\" to wrap the module forward function with HPU Graphs. This wrapper captures, caches and replays the graph. More info [here](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html).  \n",
    "\n",
    "You will see that the Intel Gaudi will build the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f37f8e-ec5e-4a0d-92fe-9c6e66c66e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"visual-question-answering\", model=\"Salesforce/blip-vqa-capfilt-large\", torch_dtype=torch.bfloat16, device=\"hpu\")\n",
    "generator.model = wrap_in_hpu_graph(generator.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e058ed1-105b-437c-b9ab-311c2a5c7078",
   "metadata": {},
   "source": [
    "### Execute the Pipeline and Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ac0c0-35ed-4302-a0d6-47aaaf4c0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autocast(device_type=\"hpu\", dtype=torch.bfloat16, enabled=torch.bfloat16):\n",
    "    result = generator(image, question, batch_size=1, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4dfc9d-b190-4b47-8356-de2e1519a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question)\n",
    "print(\"result = \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f4bc6c-02ac-41f4-8b81-b27f7e234344",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
