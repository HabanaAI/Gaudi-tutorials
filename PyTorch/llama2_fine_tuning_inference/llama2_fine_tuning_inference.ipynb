{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9599e2f3-6b9c-4578-9501-7d5c65df408a",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d33aa-8f5b-4de3-bdaa-b2bfac88e1e8",
   "metadata": {},
   "source": [
    "# Using Paramater Efficient Fine Tuning on Llama2\n",
    "This example will Fine Tune the Llama2-7B model using Parameter Efficient Fine Tuining (PEFT) and then run inference on a text prompt.  This will be using the Llama2 model with two task examples from the Optimum Habana library on the Hugging Face model repository.   The Optimum Habana library is optimized for Deep Learning training and inference on First-gen Gaudi and Gaudi2 and offers tasks such as text generation, language modeling, question answering and more. For all the examples and models, please refer to the [Optimum Habana GitHub](https://github.com/huggingface/optimum-habana#validated-models).\n",
    "\n",
    "This example will Fine Tune the Llama2-7B model using Parameter Efficient Fine Tuining (PEFT) on the timdettmers/openassistant-guanaco dataset using the Language-Modeling Task in Optimum Habana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337ed16-dcfa-424f-a46d-933880d12d04",
   "metadata": {},
   "source": [
    "### Parameter Efficient Fine Tuning\n",
    "Parameter Efficient Fine Tuning is a strategy for adapting large pre-trained language models to specific tasks while minimizing computational and memory demands.   It aims to reduce the computational cost and memory requirements associated with fine-tuning large models while maintaining or even improving their performance.  It does so by adding a smaller task-specific layer, leveraging knowledge distillation, and often relying on few-shot learning, resulting in efficient yet effective models for various natural language understanding tasks.   PEFT starts with a pre-trained language model that has already learned a wide range of language understanding tasks from a large corpus of text data. These models are usually large and computationally expensive.   Instead of fine-tuning the entire pre-trained model, PEFT adds a task-specific layer or a few task-specific layers on top of the pre-trained model. These additional layers are relatively smaller and have fewer parameters compared to the base model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404692f3-1266-4dcb-8885-ec6016aa5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad8bfa-90ae-4e32-963f-821b92ddab0e",
   "metadata": {},
   "source": [
    "### Model Setup: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db7e60-ff31-4a76-9dd5-cb4eb76412b9",
   "metadata": {},
   "source": [
    "##### Install the Habana Deepspeed Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaab9f1b-f081-4156-b86f-b1e9b23f9a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.11.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.11.0) to /tmp/pip-req-build-13fj4hk5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-13fj4hk5\n",
      "  Running command git checkout -b 1.11.0 --track origin/1.11.0\n",
      "  Switched to a new branch '1.11.0'\n",
      "  Branch '1.11.0' set up to track remote branch '1.11.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit a24dac1fa60f4e229da854b494ef40a086792521\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hjson in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (3.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (1.11.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (1.10.13)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (2.0.1a0+gitdefeb45)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.9.4+hpu.synapse.v1.11.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2.0.0->deepspeed==0.9.4+hpu.synapse.v1.11.0) (4.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.9.4+hpu.synapse.v1.11.0) (3.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.9.4+hpu.synapse.v1.11.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.9.4+hpu.synapse.v1.11.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.9.4+hpu.synapse.v1.11.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->deepspeed==0.9.4+hpu.synapse.v1.11.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->deepspeed==0.9.4+hpu.synapse.v1.11.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240274a-dd9a-4ca9-a73d-b4ff956c343d",
   "metadata": {},
   "source": [
    "##### Install the Parameter Efficient Fine Tuning Library methods\n",
    "This is taking the PEFT method from the Hugging Face repository and will be used to help create the PEFT Fine Tuning with the Llama2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d21efb3-978e-4585-915a-4c8a9ba9b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'peft'...\n",
      "remote: Enumerating objects: 4582, done.\u001b[K\n",
      "remote: Counting objects: 100% (1519/1519), done.\u001b[K\n",
      "remote: Compressing objects: 100% (402/402), done.\u001b[K\n",
      "remote: Total 4582 (delta 1299), reused 1202 (delta 1089), pack-reused 3063\u001b[K\n",
      "Receiving objects: 100% (4582/4582), 8.57 MiB | 24.72 MiB/s, done.\n",
      "Resolving deltas: 100% (2977/2977), done.\n",
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/peft\n",
      "Processing /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/peft\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (0.23.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (2.0.1a0+gitdefeb45)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (6.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from peft==0.6.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.21.0->peft==0.6.0.dev0) (0.17.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers->peft==0.6.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers->peft==0.6.0.dev0) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers->peft==0.6.0.dev0) (0.13.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->accelerate>=0.21.0->peft==0.6.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.13.0->peft==0.6.0.dev0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft==0.6.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft==0.6.0.dev0) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft==0.6.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers->peft==0.6.0.dev0) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.13.0->peft==0.6.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: peft\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peft: filename=peft-0.6.0.dev0-py3-none-any.whl size=121547 sha256=498919930b7a2530501d7e54b217252e91cc77cd7d2dcbdf78d858d381dbb8f3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ccrb2zeq/wheels/cf/83/d1/97be770a2ee8031c2160cf57e9c1634a7b3c9224c008f0e1b0\n",
      "Successfully built peft\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.0.dev0\n",
      "    Uninstalling peft-0.6.0.dev0:\n",
      "      Successfully uninstalled peft-0.6.0.dev0\n",
      "Successfully installed peft-0.6.0.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/peft.git\n",
    "%cd peft\n",
    "!pip install .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f0f3e-8bf8-4c1c-adbb-3926b292e5ed",
   "metadata": {},
   "source": [
    "##### Install the Optimum-Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8408759-937d-472a-bd00-e67142a90fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[habana] in /usr/local/lib/python3.8/dist-packages (1.13.2)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.0.1a0+gitdefeb45)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.14.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.17.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (4.33.3)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Requirement already satisfied: optimum-habana in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.7.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.12.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (6.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.66.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (2023.5.5)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (3.20.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum[habana]) (10.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.70.15)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (2.0.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.8.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.3.0)\n",
      "Requirement already satisfied: accelerate>=0.22.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]) (0.23.0)\n",
      "Requirement already satisfied: diffusers>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]) (0.21.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.22.0->optimum-habana->optimum[habana]) (5.9.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.18.0->optimum-habana->optimum[habana]) (10.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.18.0->optimum-habana->optimum[habana]) (6.8.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (3.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.18.0->optimum-habana->optimum[habana]) (3.16.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade-strategy eager optimum[habana]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b06c57-0d8c-4d40-85ae-1eea945a3ef9",
   "metadata": {},
   "source": [
    "##### Pull the Hugging Face Examples from GitHub\n",
    "These contain the working Hugging Face Task Examples that have been optimized for Gaudi.  For Fine Tuning, we'll use the language-modeling task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50af3c79-6641-47d7-a440-09eba2bd5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 5793, done.\u001b[K\n",
      "remote: Counting objects: 100% (2832/2832), done.\u001b[K\n",
      "remote: Compressing objects: 100% (978/978), done.\u001b[K\n",
      "remote: Total 5793 (delta 2293), reused 2046 (delta 1820), pack-reused 2961\u001b[K\n",
      "Receiving objects: 100% (5793/5793), 2.96 MiB | 14.87 MiB/s, done.\n",
      "Resolving deltas: 100% (3734/3734), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e369a6-ce17-40a2-8a52-d735b7140e09",
   "metadata": {},
   "source": [
    "##### Go to the Language Modeling Task and install the model specific requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "441de6d9-2b6f-4cf2-8bfd-664cec1c4998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/language-modeling/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed7d45",
   "metadata": {},
   "source": [
    "##### How to access and Use the Llama2 model\n",
    "\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/.\n",
    "Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-7b-hf, you need the following: \n",
    "- Have a HuggingFace account\n",
    "- Agree to the terms of use of the model in its model card on the HF Hub\n",
    "- set a read token\n",
    "- Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa53882-c834-4ff3-8fc4-742579ee8cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token <your_token_here> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4c81e-c48d-45e0-93de-579e53995602",
   "metadata": {},
   "source": [
    "## Fine Tuning the model with PEFT and LoRA\n",
    "\n",
    "We'll now run the fine tuning with the PEFT method. Remember that the PEFT methods only fine-tune a small number of extra model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning.\n",
    "\n",
    "##### Here's a summary of the command required to run the Fine Tuning, you'll run this in the next cell below. \n",
    "Note in this case the following: \n",
    "1. Using the language modeling with LoRA; `run_lora_clm.py`\n",
    "2. It's very efficient: only 0.06% of the total paramters are being fine tuned of the total 7B parameters.\n",
    "3. The maximum memory used was 33.03 GB out of a total memory available 94.61 GB\n",
    "4. Only 2 epochs are needed for fine tuning, it takes less than 6 minutes to run. \n",
    "\n",
    "```\n",
    "python ../gaudi_spawn.py \\\n",
    "       --world_size 8    --use_mpi run_lora_clm.py \\\n",
    "       --model_name_or_path meta-llama/Llama-2-7b-hf  \\\n",
    "       --dataset_name timdettmers/openassistant-guanaco \\\n",
    "       --bf16 True \\\n",
    "       --output_dir ./model_lora_llama \\\n",
    "       --num_train_epochs 2 \\\n",
    "       --per_device_train_batch_size 2 \\\n",
    "       --per_device_eval_batch_size 2 \\\n",
    "       --gradient_accumulation_steps 4 \\\n",
    "       --evaluation_strategy \"no\"\\\n",
    "       --save_strategy \"steps\"\\\n",
    "       --save_steps 2000 \\\n",
    "       --save_total_limit 1 \\\n",
    "       --learning_rate 1e-4 \\\n",
    "       --logging_steps 1 \\\n",
    "       --dataset_concatenation \\\n",
    "       --do_train \\\n",
    "       --use_habana \\\n",
    "       --use_lazy_mode \\\n",
    "       --throughput_warmup_steps 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a6d717b-388b-410a-8983-f3e0fbf3d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-02 22:07:50,564] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "Running with the following model specific env vars: \n",
      "MASTER_ADDR=localhost\n",
      "MASTER_PORT=12345\n",
      "DistributedRunner run(): command = mpirun -n 8 --bind-to core --map-by socket:PE=10 --rank-by core --report-bindings --allow-run-as-root /usr/bin/python run_lora_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name timdettmers/openassistant-guanaco --bf16 True --output_dir ./model_lora_llamaOA --num_train_epochs 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy steps --save_steps 2000 --save_total_limit 1 --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3\n",
      "[sc09super17-klb2:59466] MCW rank 4 bound to socket 1[core 40[hwt 0-1]], socket 1[core 41[hwt 0-1]], socket 1[core 42[hwt 0-1]], socket 1[core 43[hwt 0-1]], socket 1[core 44[hwt 0-1]], socket 1[core 45[hwt 0-1]], socket 1[core 46[hwt 0-1]], socket 1[core 47[hwt 0-1]], socket 1[core 48[hwt 0-1]], socket 1[core 49[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09super17-klb2:59466] MCW rank 5 bound to socket 1[core 50[hwt 0-1]], socket 1[core 51[hwt 0-1]], socket 1[core 52[hwt 0-1]], socket 1[core 53[hwt 0-1]], socket 1[core 54[hwt 0-1]], socket 1[core 55[hwt 0-1]], socket 1[core 56[hwt 0-1]], socket 1[core 57[hwt 0-1]], socket 1[core 58[hwt 0-1]], socket 1[core 59[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09super17-klb2:59466] MCW rank 6 bound to socket 1[core 60[hwt 0-1]], socket 1[core 61[hwt 0-1]], socket 1[core 62[hwt 0-1]], socket 1[core 63[hwt 0-1]], socket 1[core 64[hwt 0-1]], socket 1[core 65[hwt 0-1]], socket 1[core 66[hwt 0-1]], socket 1[core 67[hwt 0-1]], socket 1[core 68[hwt 0-1]], socket 1[core 69[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../..]\n",
      "[sc09super17-klb2:59466] MCW rank 7 bound to socket 1[core 70[hwt 0-1]], socket 1[core 71[hwt 0-1]], socket 1[core 72[hwt 0-1]], socket 1[core 73[hwt 0-1]], socket 1[core 74[hwt 0-1]], socket 1[core 75[hwt 0-1]], socket 1[core 76[hwt 0-1]], socket 1[core 77[hwt 0-1]], socket 1[core 78[hwt 0-1]], socket 1[core 79[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]\n",
      "[sc09super17-klb2:59466] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09super17-klb2:59466] MCW rank 1 bound to socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]], socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]]: [../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09super17-klb2:59466] MCW rank 2 bound to socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]], socket 0[core 24[hwt 0-1]], socket 0[core 25[hwt 0-1]], socket 0[core 26[hwt 0-1]], socket 0[core 27[hwt 0-1]], socket 0[core 28[hwt 0-1]], socket 0[core 29[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09super17-klb2:59466] MCW rank 3 bound to socket 0[core 30[hwt 0-1]], socket 0[core 31[hwt 0-1]], socket 0[core 32[hwt 0-1]], socket 0[core 33[hwt 0-1]], socket 0[core 34[hwt 0-1]], socket 0[core 35[hwt 0-1]], socket 0[core 36[hwt 0-1]], socket 0[core 37[hwt 0-1]], socket 0[core 38[hwt 0-1]], socket 0[core 39[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[2023-10-02 22:07:54,743] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,743] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,749] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,763] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,764] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,764] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,765] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2023-10-02 22:07:54,765] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 0, device: hpu:0\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - INFO - __main__ -   Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./model_lora_llamaOA/runs/Oct02_22-07-57_sc09super17-klb2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=1.0,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./model_lora_llamaOA,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=2,\n",
      "per_device_train_batch_size=2,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./model_lora_llamaOA,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=2000,\n",
      "save_strategy=steps,\n",
      "save_total_limit=1,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 5, device: hpu:5\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 4, device: hpu:4\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 2, device: hpu:2\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 3, device: hpu:3\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 7, device: hpu:7\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 1, device: hpu:1\n",
      "distributed training: True, 16-bits training: True\n",
      "10/02/2023 22:07:58 - WARNING - __main__ -   Process rank: 6, device: hpu:6\n",
      "distributed training: True, 16-bits training: True\n",
      "[INFO|configuration_utils.py:715] 2023-10-02 22:07:58,385 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json\n",
      "[INFO|configuration_utils.py:775] 2023-10-02 22:07:58,386 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1852] 2023-10-02 22:07:58,478 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1852] 2023-10-02 22:07:58,478 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2023-10-02 22:07:58,478 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1852] 2023-10-02 22:07:58,478 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1852] 2023-10-02 22:07:58,478 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/tokenizer_config.json\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Downloading readme: 100%|██████████| 395/395 [00:00<00:00, 5.28MB/s]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "10/02/2023 22:07:59 - WARNING - huggingface_hub.repocard -   Repo card metadata block was not found. Setting CardData to empty.\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/20.9M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|██        | 4.19M/20.9M [00:00<00:02, 6.82MB/s]\u001b[A\n",
      "Downloading data:  60%|██████    | 12.6M/20.9M [00:00<00:00, 15.3MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 20.9M/20.9M [00:01<00:00, 18.6MB/s]\u001b[A\n",
      "Downloading data files:  50%|█████     | 1/2 [00:01<00:01,  1.13s/it]\n",
      "Downloading data:   0%|          | 0.00/1.11M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 1.11M/1.11M [00:00<00:00, 2.75MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 2961.03it/s]\n",
      "Generating train split: 9846 examples [00:00, 320711.96 examples/s]\n",
      "Generating test split: 518 examples [00:00, 141873.41 examples/s]\n",
      "Map: 100%|██████████| 9846/9846 [00:00<00:00, 12076.63 examples/s]\n",
      "Map: 100%|██████████| 9846/9846 [00:00<00:00, 11201.42 examples/s]\n",
      "[INFO|modeling_utils.py:2862] 2023-10-02 22:08:02,985 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1200] 2023-10-02 22:08:02,986 >> Instantiating GaudiLlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:768] 2023-10-02 22:08:02,986 >> Generate config GaudiGenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ignore_eos\": null,\n",
      "  \"static_shapes\": null,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]] \n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\n",
      "[INFO|modeling_utils.py:3648] 2023-10-02 22:08:57,430 >> All model checkpoint weights were used when initializing GaudiLlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3656] 2023-10-02 22:08:57,430 >> All the weights of GaudiLlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GaudiLlamaForCausalLM for predictions without further training.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.36it/s][INFO|configuration_utils.py:730] 2023-10-02 22:08:57,535 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/generation_config.json\n",
      "[INFO|configuration_utils.py:768] 2023-10-02 22:08:57,536 >> Generate config GaudiGenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ignore_eos\": null,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"static_shapes\": null,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.33.3\"\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 9846/9846 [00:03<00:00, 2700.05 examples/s]34it/s]\n",
      "Map: 100%|██████████| 518/518 [00:00<00:00, 2754.72 examples/s]\n",
      "Map:   0%|          | 0/518 [00:00<?, ? examples/s]\n",
      "\n",
      "Map:   0%|          | 0/518 [00:00<?, ? examples/s]\n",
      "\n",
      "Map:   0%|          | 0/518 [00:00<?, ? examples/s]\n",
      "Map:   0%|          | 0/518 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 518/518 [00:00<00:00, 2163.08 examples/s]10/02/2023 22:09:06 - INFO - __main__ -   Using data collator of type DataCollatorForLanguageModeling\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446944 KB\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "[INFO|trainer.py:680] 2023-10-02 22:09:15,914 >> ***** Running training *****\n",
      "[INFO|trainer.py:681] 2023-10-02 22:09:15,914 >>   Num examples = 6,580\n",
      "[INFO|trainer.py:682] 2023-10-02 22:09:15,914 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:683] 2023-10-02 22:09:15,914 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:686] 2023-10-02 22:09:15,914 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:687] 2023-10-02 22:09:15,914 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:688] 2023-10-02 22:09:15,914 >>   Total optimization steps = 206\n",
      "[INFO|trainer.py:689] 2023-10-02 22:09:15,917 >>   Number of trainable parameters = 4,194,304\n",
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 0/206 [00:00<?, ?it/s]\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[WARNING|logging.py:290] 2023-10-02 22:09:15,925 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 1/206 [00:52<2:58:26, 52.22s/it]{'loss': 1.6013, 'learning_rate': 9.951456310679612e-05, 'epoch': 0.01, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 23.04, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4856, 'learning_rate': 9.902912621359223e-05, 'epoch': 0.02, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 31.44, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5767, 'learning_rate': 9.854368932038835e-05, 'epoch': 0.03, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5491, 'learning_rate': 9.805825242718448e-05, 'epoch': 0.04, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5253, 'learning_rate': 9.757281553398059e-05, 'epoch': 0.05, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5639, 'learning_rate': 9.70873786407767e-05, 'epoch': 0.06, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5571, 'learning_rate': 9.660194174757282e-05, 'epoch': 0.07, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5054, 'learning_rate': 9.611650485436893e-05, 'epoch': 0.08, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.6034, 'learning_rate': 9.563106796116505e-05, 'epoch': 0.09, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5834, 'learning_rate': 9.514563106796118e-05, 'epoch': 0.1, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.556, 'learning_rate': 9.466019417475729e-05, 'epoch': 0.11, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5545, 'learning_rate': 9.417475728155341e-05, 'epoch': 0.12, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.6205, 'learning_rate': 9.368932038834952e-05, 'epoch': 0.13, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4947, 'learning_rate': 9.320388349514564e-05, 'epoch': 0.14, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5266, 'learning_rate': 9.271844660194175e-05, 'epoch': 0.15, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5306, 'learning_rate': 9.223300970873788e-05, 'epoch': 0.16, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5721, 'learning_rate': 9.174757281553399e-05, 'epoch': 0.17, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5576, 'learning_rate': 9.126213592233011e-05, 'epoch': 0.17, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.473, 'learning_rate': 9.077669902912622e-05, 'epoch': 0.18, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4935, 'learning_rate': 9.029126213592234e-05, 'epoch': 0.19, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.6305, 'learning_rate': 8.980582524271845e-05, 'epoch': 0.2, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5058, 'learning_rate': 8.932038834951457e-05, 'epoch': 0.21, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5179, 'learning_rate': 8.88349514563107e-05, 'epoch': 0.22, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4459, 'learning_rate': 8.834951456310681e-05, 'epoch': 0.23, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4845, 'learning_rate': 8.786407766990292e-05, 'epoch': 0.24, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.48, 'learning_rate': 8.737864077669902e-05, 'epoch': 0.25, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4698, 'learning_rate': 8.689320388349514e-05, 'epoch': 0.26, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5068, 'learning_rate': 8.640776699029127e-05, 'epoch': 0.27, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4809, 'learning_rate': 8.592233009708738e-05, 'epoch': 0.28, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4876, 'learning_rate': 8.54368932038835e-05, 'epoch': 0.29, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4629, 'learning_rate': 8.495145631067961e-05, 'epoch': 0.3, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4051, 'learning_rate': 8.446601941747573e-05, 'epoch': 0.31, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4304, 'learning_rate': 8.398058252427184e-05, 'epoch': 0.32, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4827, 'learning_rate': 8.349514563106797e-05, 'epoch': 0.33, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4103, 'learning_rate': 8.300970873786408e-05, 'epoch': 0.34, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3655, 'learning_rate': 8.25242718446602e-05, 'epoch': 0.35, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4781, 'learning_rate': 8.203883495145631e-05, 'epoch': 0.36, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.5197, 'learning_rate': 8.155339805825243e-05, 'epoch': 0.37, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4851, 'learning_rate': 8.106796116504854e-05, 'epoch': 0.38, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4073, 'learning_rate': 8.058252427184466e-05, 'epoch': 0.39, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4253, 'learning_rate': 8.009708737864078e-05, 'epoch': 0.4, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3586, 'learning_rate': 7.96116504854369e-05, 'epoch': 0.41, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3959, 'learning_rate': 7.912621359223301e-05, 'epoch': 0.42, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4444, 'learning_rate': 7.864077669902913e-05, 'epoch': 0.43, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3732, 'learning_rate': 7.815533980582524e-05, 'epoch': 0.44, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3857, 'learning_rate': 7.766990291262136e-05, 'epoch': 0.45, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3701, 'learning_rate': 7.718446601941748e-05, 'epoch': 0.46, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3978, 'learning_rate': 7.66990291262136e-05, 'epoch': 0.47, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3932, 'learning_rate': 7.621359223300971e-05, 'epoch': 0.48, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4141, 'learning_rate': 7.572815533980583e-05, 'epoch': 0.49, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4167, 'learning_rate': 7.524271844660194e-05, 'epoch': 0.5, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4165, 'learning_rate': 7.475728155339806e-05, 'epoch': 0.5, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3278, 'learning_rate': 7.427184466019417e-05, 'epoch': 0.51, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3984, 'learning_rate': 7.37864077669903e-05, 'epoch': 0.52, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4031, 'learning_rate': 7.330097087378641e-05, 'epoch': 0.53, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3388, 'learning_rate': 7.281553398058253e-05, 'epoch': 0.54, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3929, 'learning_rate': 7.233009708737864e-05, 'epoch': 0.55, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3702, 'learning_rate': 7.184466019417476e-05, 'epoch': 0.56, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3618, 'learning_rate': 7.135922330097087e-05, 'epoch': 0.57, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3887, 'learning_rate': 7.0873786407767e-05, 'epoch': 0.58, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3005, 'learning_rate': 7.038834951456312e-05, 'epoch': 0.59, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3627, 'learning_rate': 6.990291262135923e-05, 'epoch': 0.6, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.415, 'learning_rate': 6.941747572815534e-05, 'epoch': 0.61, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4201, 'learning_rate': 6.893203883495146e-05, 'epoch': 0.62, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3728, 'learning_rate': 6.844660194174757e-05, 'epoch': 0.63, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3317, 'learning_rate': 6.79611650485437e-05, 'epoch': 0.64, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4173, 'learning_rate': 6.747572815533982e-05, 'epoch': 0.65, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3823, 'learning_rate': 6.699029126213593e-05, 'epoch': 0.66, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3612, 'learning_rate': 6.650485436893205e-05, 'epoch': 0.67, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3673, 'learning_rate': 6.601941747572816e-05, 'epoch': 0.68, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4796, 'learning_rate': 6.553398058252428e-05, 'epoch': 0.69, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3729, 'learning_rate': 6.504854368932039e-05, 'epoch': 0.7, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3633, 'learning_rate': 6.456310679611652e-05, 'epoch': 0.71, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3484, 'learning_rate': 6.407766990291263e-05, 'epoch': 0.72, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3835, 'learning_rate': 6.359223300970875e-05, 'epoch': 0.73, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.351, 'learning_rate': 6.310679611650486e-05, 'epoch': 0.74, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.379, 'learning_rate': 6.262135922330098e-05, 'epoch': 0.75, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4657, 'learning_rate': 6.213592233009709e-05, 'epoch': 0.76, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3319, 'learning_rate': 6.16504854368932e-05, 'epoch': 0.77, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4445, 'learning_rate': 6.116504854368932e-05, 'epoch': 0.78, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.407, 'learning_rate': 6.0679611650485434e-05, 'epoch': 0.79, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3527, 'learning_rate': 6.019417475728155e-05, 'epoch': 0.8, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4286, 'learning_rate': 5.970873786407767e-05, 'epoch': 0.81, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3512, 'learning_rate': 5.9223300970873785e-05, 'epoch': 0.82, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3425, 'learning_rate': 5.87378640776699e-05, 'epoch': 0.83, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.289, 'learning_rate': 5.825242718446602e-05, 'epoch': 0.83, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2798, 'learning_rate': 5.7766990291262135e-05, 'epoch': 0.84, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3529, 'learning_rate': 5.728155339805825e-05, 'epoch': 0.85, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3783, 'learning_rate': 5.679611650485437e-05, 'epoch': 0.86, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4329, 'learning_rate': 5.6310679611650486e-05, 'epoch': 0.87, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3319, 'learning_rate': 5.58252427184466e-05, 'epoch': 0.88, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3108, 'learning_rate': 5.533980582524272e-05, 'epoch': 0.89, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3963, 'learning_rate': 5.4854368932038836e-05, 'epoch': 0.9, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3006, 'learning_rate': 5.436893203883495e-05, 'epoch': 0.91, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3674, 'learning_rate': 5.3883495145631065e-05, 'epoch': 0.92, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3362, 'learning_rate': 5.339805825242719e-05, 'epoch': 0.93, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3284, 'learning_rate': 5.29126213592233e-05, 'epoch': 0.94, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 48%|████▊     | 98/206 [02:14<01:14,  1.45it/s]{'loss': 1.3582, 'learning_rate': 5.2427184466019416e-05, 'epoch': 0.95, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3047, 'learning_rate': 5.194174757281554e-05, 'epoch': 0.96, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3675, 'learning_rate': 5.145631067961165e-05, 'epoch': 0.97, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 49%|████▉     | 101/206 [02:16<01:11,  1.46it/s]{'loss': 1.4369, 'learning_rate': 5.0970873786407766e-05, 'epoch': 0.98, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3054, 'learning_rate': 5.048543689320389e-05, 'epoch': 0.99, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4231, 'learning_rate': 5e-05, 'epoch': 1.0, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3667, 'learning_rate': 4.951456310679612e-05, 'epoch': 1.01, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4002, 'learning_rate': 4.902912621359224e-05, 'epoch': 1.02, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3027, 'learning_rate': 4.854368932038835e-05, 'epoch': 1.03, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3303, 'learning_rate': 4.805825242718447e-05, 'epoch': 1.04, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 52%|█████▏    | 108/206 [02:21<01:07,  1.45it/s]{'loss': 1.371, 'learning_rate': 4.757281553398059e-05, 'epoch': 1.05, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4267, 'learning_rate': 4.7087378640776703e-05, 'epoch': 1.06, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.386, 'learning_rate': 4.660194174757282e-05, 'epoch': 1.07, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3575, 'learning_rate': 4.611650485436894e-05, 'epoch': 1.08, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3328, 'learning_rate': 4.5631067961165054e-05, 'epoch': 1.09, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3851, 'learning_rate': 4.514563106796117e-05, 'epoch': 1.1, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3393, 'learning_rate': 4.466019417475728e-05, 'epoch': 1.11, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3276, 'learning_rate': 4.4174757281553404e-05, 'epoch': 1.12, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3094, 'learning_rate': 4.368932038834951e-05, 'epoch': 1.13, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.35, 'learning_rate': 4.3203883495145634e-05, 'epoch': 1.14, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3627, 'learning_rate': 4.271844660194175e-05, 'epoch': 1.15, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3916, 'learning_rate': 4.223300970873786e-05, 'epoch': 1.16, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3437, 'learning_rate': 4.1747572815533984e-05, 'epoch': 1.17, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3292, 'learning_rate': 4.12621359223301e-05, 'epoch': 1.17, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.392, 'learning_rate': 4.077669902912621e-05, 'epoch': 1.18, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3639, 'learning_rate': 4.029126213592233e-05, 'epoch': 1.19, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3797, 'learning_rate': 3.980582524271845e-05, 'epoch': 1.2, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3471, 'learning_rate': 3.9320388349514564e-05, 'epoch': 1.21, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3528, 'learning_rate': 3.883495145631068e-05, 'epoch': 1.22, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4095, 'learning_rate': 3.83495145631068e-05, 'epoch': 1.23, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3108, 'learning_rate': 3.7864077669902914e-05, 'epoch': 1.24, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3271, 'learning_rate': 3.737864077669903e-05, 'epoch': 1.25, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3098, 'learning_rate': 3.689320388349515e-05, 'epoch': 1.26, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3404, 'learning_rate': 3.6407766990291265e-05, 'epoch': 1.27, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3963, 'learning_rate': 3.592233009708738e-05, 'epoch': 1.28, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.262, 'learning_rate': 3.54368932038835e-05, 'epoch': 1.29, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3476, 'learning_rate': 3.4951456310679615e-05, 'epoch': 1.3, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3507, 'learning_rate': 3.446601941747573e-05, 'epoch': 1.31, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2999, 'learning_rate': 3.398058252427185e-05, 'epoch': 1.32, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3673, 'learning_rate': 3.3495145631067966e-05, 'epoch': 1.33, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3487, 'learning_rate': 3.300970873786408e-05, 'epoch': 1.34, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3518, 'learning_rate': 3.2524271844660195e-05, 'epoch': 1.35, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3797, 'learning_rate': 3.2038834951456316e-05, 'epoch': 1.36, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2773, 'learning_rate': 3.155339805825243e-05, 'epoch': 1.37, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3345, 'learning_rate': 3.1067961165048545e-05, 'epoch': 1.38, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4181, 'learning_rate': 3.058252427184466e-05, 'epoch': 1.39, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3225, 'learning_rate': 3.0097087378640774e-05, 'epoch': 1.4, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3469, 'learning_rate': 2.9611650485436892e-05, 'epoch': 1.41, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2843, 'learning_rate': 2.912621359223301e-05, 'epoch': 1.42, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.337, 'learning_rate': 2.8640776699029125e-05, 'epoch': 1.43, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3923, 'learning_rate': 2.8155339805825243e-05, 'epoch': 1.44, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.337, 'learning_rate': 2.766990291262136e-05, 'epoch': 1.45, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3649, 'learning_rate': 2.7184466019417475e-05, 'epoch': 1.46, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3425, 'learning_rate': 2.6699029126213593e-05, 'epoch': 1.47, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.4082, 'learning_rate': 2.6213592233009708e-05, 'epoch': 1.48, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2974, 'learning_rate': 2.5728155339805826e-05, 'epoch': 1.49, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3515, 'learning_rate': 2.5242718446601944e-05, 'epoch': 1.5, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3826, 'learning_rate': 2.475728155339806e-05, 'epoch': 1.5, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3243, 'learning_rate': 2.4271844660194176e-05, 'epoch': 1.51, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3557, 'learning_rate': 2.3786407766990294e-05, 'epoch': 1.52, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3041, 'learning_rate': 2.330097087378641e-05, 'epoch': 1.53, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3226, 'learning_rate': 2.2815533980582527e-05, 'epoch': 1.54, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3081, 'learning_rate': 2.233009708737864e-05, 'epoch': 1.55, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3889, 'learning_rate': 2.1844660194174756e-05, 'epoch': 1.56, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3434, 'learning_rate': 2.1359223300970874e-05, 'epoch': 1.57, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3838, 'learning_rate': 2.0873786407766992e-05, 'epoch': 1.58, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3774, 'learning_rate': 2.0388349514563107e-05, 'epoch': 1.59, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3752, 'learning_rate': 1.9902912621359225e-05, 'epoch': 1.6, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3172, 'learning_rate': 1.941747572815534e-05, 'epoch': 1.61, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3444, 'learning_rate': 1.8932038834951457e-05, 'epoch': 1.62, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3397, 'learning_rate': 1.8446601941747575e-05, 'epoch': 1.63, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2981, 'learning_rate': 1.796116504854369e-05, 'epoch': 1.64, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3427, 'learning_rate': 1.7475728155339808e-05, 'epoch': 1.65, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3219, 'learning_rate': 1.6990291262135926e-05, 'epoch': 1.66, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3086, 'learning_rate': 1.650485436893204e-05, 'epoch': 1.67, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3828, 'learning_rate': 1.6019417475728158e-05, 'epoch': 1.68, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3081, 'learning_rate': 1.5533980582524273e-05, 'epoch': 1.69, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3013, 'learning_rate': 1.5048543689320387e-05, 'epoch': 1.7, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3389, 'learning_rate': 1.4563106796116505e-05, 'epoch': 1.71, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3338, 'learning_rate': 1.4077669902912621e-05, 'epoch': 1.72, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 86%|████████▋ | 178/206 [03:10<00:19,  1.41it/s]{'loss': 1.3758, 'learning_rate': 1.3592233009708738e-05, 'epoch': 1.73, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 87%|████████▋ | 179/206 [03:10<00:18,  1.43it/s]{'loss': 1.2993, 'learning_rate': 1.3106796116504854e-05, 'epoch': 1.74, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3016, 'learning_rate': 1.2621359223300972e-05, 'epoch': 1.75, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3115, 'learning_rate': 1.2135922330097088e-05, 'epoch': 1.76, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.2652, 'learning_rate': 1.1650485436893204e-05, 'epoch': 1.77, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3783, 'learning_rate': 1.116504854368932e-05, 'epoch': 1.78, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3489, 'learning_rate': 1.0679611650485437e-05, 'epoch': 1.79, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3178, 'learning_rate': 1.0194174757281553e-05, 'epoch': 1.8, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3706, 'learning_rate': 9.70873786407767e-06, 'epoch': 1.81, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3192, 'learning_rate': 9.223300970873788e-06, 'epoch': 1.82, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3756, 'learning_rate': 8.737864077669904e-06, 'epoch': 1.83, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3303, 'learning_rate': 8.25242718446602e-06, 'epoch': 1.83, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3627, 'learning_rate': 7.766990291262136e-06, 'epoch': 1.84, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3714, 'learning_rate': 7.281553398058253e-06, 'epoch': 1.85, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3026, 'learning_rate': 6.796116504854369e-06, 'epoch': 1.86, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3755, 'learning_rate': 6.310679611650486e-06, 'epoch': 1.87, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 94%|█████████▍| 194/206 [03:21<00:08,  1.46it/s]{'loss': 1.314, 'learning_rate': 5.825242718446602e-06, 'epoch': 1.88, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 95%|█████████▍| 195/206 [03:21<00:07,  1.46it/s]{'loss': 1.3437, 'learning_rate': 5.3398058252427185e-06, 'epoch': 1.89, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3702, 'learning_rate': 4.854368932038835e-06, 'epoch': 1.9, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3124, 'learning_rate': 4.368932038834952e-06, 'epoch': 1.91, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.368, 'learning_rate': 3.883495145631068e-06, 'epoch': 1.92, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3073, 'learning_rate': 3.3980582524271844e-06, 'epoch': 1.93, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3003, 'learning_rate': 2.912621359223301e-06, 'epoch': 1.94, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3569, 'learning_rate': 2.4271844660194174e-06, 'epoch': 1.95, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      " 98%|█████████▊| 202/206 [03:26<00:02,  1.43it/s]{'loss': 1.2877, 'learning_rate': 1.941747572815534e-06, 'epoch': 1.96, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3892, 'learning_rate': 1.4563106796116506e-06, 'epoch': 1.97, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3568, 'learning_rate': 9.70873786407767e-07, 'epoch': 1.98, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3251, 'learning_rate': 4.854368932038835e-07, 'epoch': 1.99, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "{'loss': 1.3166, 'learning_rate': 0.0, 'epoch': 2.0, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "100%|██████████| 206/206 [03:29<00:00,  1.46it/s][INFO|trainer.py:950] 2023-10-02 22:12:45,381 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 206/206 [03:29<00:00,  1.46it/s]{'train_runtime': 209.4643, 'train_samples_per_second': 92.111, 'train_steps_per_second': 1.442, 'train_loss': 1.3855091194504674, 'epoch': 2.0, 'memory_allocated (GB)': 21.41, 'max_memory_allocated (GB)': 33.03, 'total_memory_available (GB)': 94.61}\n",
      "100%|██████████| 206/206 [03:29<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "!python ../gaudi_spawn.py --world_size 8 --use_mpi run_lora_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf  --dataset_name timdettmers/openassistant-guanaco --bf16 True --output_dir ./model_lora_llama --num_train_epochs 2 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 4 --evaluation_strategy \"no\" --save_strategy \"steps\" --save_steps 2000 --save_total_limit 1 --learning_rate 1e-4 --logging_steps 1 --dataset_concatenation --do_train --use_habana --use_lazy_mode --throughput_warmup_steps 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f4aa5-5dc0-4662-bbca-a7a156be37f2",
   "metadata": {},
   "source": [
    "#### LoRA Fine Tuning Completed\n",
    "You will now see a \"model_lora_llama\" folder created which contains the PEFT model `adapter_model.bin` which will be used in the inference example below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5b5cf-e4a4-4d67-888b-e3ddcdff1a5d",
   "metadata": {},
   "source": [
    "## Inference with Llama2\n",
    "\n",
    "We'll now use the Hugging Face `text-generation` task to run inference on the Llama2-7b model; we'll generate text based on an included prompt.  Notice that we've included a path to the PEFT model that we just created.\n",
    "\n",
    "First, well move to the text-generation examples folder and install the requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14841b58-2697-459d-ace5-763d721468f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/text-generation\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/text-generation\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03479984-e872-48df-b106-cb9b1dd445d3",
   "metadata": {},
   "source": [
    "You will see that we are now running inference with the `run_generation.py` task and we are including the PEFT model that we Fine Tuned in the steps above. \n",
    "\n",
    "```\n",
    "python run_generation.py \\\n",
    "   --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "   --batch_size 1 \\\n",
    "   --max_new_tokens 500 \\\n",
    "   --n_iterations 4 \\\n",
    "   --use_kv_cache \\\n",
    "   --use_hpu_graphs \\\n",
    "   --bf16 \\\n",
    "   --prompt \"Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\" \\\n",
    "   --peft_model /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llama/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9df37900-665f-48cf-982f-f7c2eae7c549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 22:19:40 - INFO - __main__ - Single-device run.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-02 22:19:41,267] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "Fetching 2 files: 100%|████████████████████████| 2/2 [00:00<00:00, 37786.52it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.48it/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446944 KB\n",
      "------------------------------------------------------------------------------\n",
      "10/02/2023 22:20:42 - INFO - __main__ - Args: Namespace(attn_softmax_bf16=False, bad_words=None, batch_size=1, bf16=True, bucket_size=-1, column_name=None, dataset_max_samples=-1, dataset_name=None, device='hpu', do_sample=False, force_words=None, limit_hpu_graphs=False, local_rank=-1, max_input_tokens=0, max_new_tokens=500, model_name_or_path='meta-llama/Llama-2-7b-hf', model_revision='main', n_iterations=4, num_beams=1, num_return_sequences=1, output_dir=None, peft_model='/root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llamaOA/', profiling_steps=0, profiling_warmup_steps=0, prompt='Can you write a short introduction about the relevance of the term monopsony in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.', reuse_cache=False, seed=27, token=None, trim_logits=False, use_hpu_graphs=True, use_kv_cache=True, warmup=3)\n",
      "10/02/2023 22:20:42 - INFO - __main__ - device: hpu, n_hpu: 1, bf16: True\n",
      "10/02/2023 22:20:43 - INFO - __main__ - Graph compilation...\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "10/02/2023 22:20:58 - INFO - __main__ - Running generate...\n",
      "\n",
      "Input/outputs:\n",
      "input 1: ('Can you write a short introduction about the relevance of the term monopsony in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.',)\n",
      "output 1: ('Can you write a short introduction about the relevance of the term monopsony in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\\nMonopsony is a term used in economics to describe a market situation where there is only one buyer of a good or service. In a monopsony, the buyer has significant market power and can set the price of the good or service at a level that is lower than the market equilibrium price. This can lead to lower wages for workers and lower profits for sellers.\\nOne example of a potential monopsony in the labour market is a large company that employs a large percentage of the workers in a particular industry. In this situation, the company has significant market power and can set the wages for its workers at a level that is lower than the market equilibrium wage. This can lead to lower wages for workers and lower profits for sellers.\\nAnother example of a potential monopsony in the labour market is a government that controls a large percentage of the labour market. In this situation, the government has significant market power and can set the wages for its workers at a level that is lower than the market equilibrium wage. This can lead to lower wages for workers and lower profits for sellers.\\nThere is also potential for monopsony in the market for goods and services. For example, a large company that controls a large percentage of the market for a particular good or service can set the price of the good or service at a level that is lower than the market equilibrium price. This can lead to lower profits for sellers and lower wages for workers.\\nIn conclusion, monopsony is a term used in economics to describe a market situation where there is only one buyer of a good or service. In a monopsony, the buyer has significant market power and can set the price of the good or service at a level that is lower than the market equilibrium price. This can lead to lower wages for workers and lower profits for sellers. Tags: 2020 Elections | Donald Trump | Joe Biden | Kamala Harris | polls | presidential election | 2020\\nPoll: Biden, Harris Lead Trump, Pence in 2020 Presidential Race\\nFormer Vice President Joe Biden and Sen. Kamala Harris, D-Calif., lead President Donald Trump and Vice President Mike Pence in a hypothetical 20',)\n",
      "\n",
      "\n",
      "Stats:\n",
      "----------------------------------------------------------------------\n",
      "Throughput (including tokenization) = 122.64711014361208 tokens/second\n",
      "Memory allocated                    = 13.24 GB\n",
      "Max memory allocated                = 13.58 GB\n",
      "Total memory available              = 94.61 GB\n",
      "Graph compilation duration          = 15.793254804098979 seconds\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_generation.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --max_new_tokens 500 --n_iterations 4 --use_kv_cache --use_hpu_graphs --bf16 --prompt \"Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\" --peft_model /root/Gaudi-tutorials/PyTorch/llama2_fine_tuning_inference/optimum-habana/examples/language-modeling/model_lora_llama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a38c-eeab-49d8-b12a-73599636e445",
   "metadata": {},
   "source": [
    "##### Comparison without PEFT and LoRA\n",
    "In this example, we're simply running the Llama2 7B model **without** including the PEFT fine tuned model, so the you are losing the additional detail that is brought to the model, and the results have signficantly less information and fidelity compared to the last model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1a408c-00af-47ae-a7f3-d80c625d6fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/02/2023 22:21:58 - INFO - __main__ - Single-device run.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[2023-10-02 22:21:59,140] [INFO] [real_accelerator.py:123:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "Fetching 2 files: 100%|████████████████████████| 2/2 [00:00<00:00, 35246.25it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  2.01it/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446944 KB\n",
      "------------------------------------------------------------------------------\n",
      "10/02/2023 22:23:00 - INFO - __main__ - Args: Namespace(attn_softmax_bf16=False, bad_words=None, batch_size=1, bf16=True, bucket_size=-1, column_name=None, dataset_max_samples=-1, dataset_name=None, device='hpu', do_sample=False, force_words=None, limit_hpu_graphs=False, local_rank=-1, max_input_tokens=0, max_new_tokens=500, model_name_or_path='meta-llama/Llama-2-7b-hf', model_revision='main', n_iterations=4, num_beams=1, num_return_sequences=1, output_dir=None, peft_model=None, profiling_steps=0, profiling_warmup_steps=0, prompt='Can you write a short introduction about the relevance of the term monopsony in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.', reuse_cache=False, seed=27, token=None, trim_logits=False, use_hpu_graphs=True, use_kv_cache=True, warmup=3)\n",
      "10/02/2023 22:23:00 - INFO - __main__ - device: hpu, n_hpu: 1, bf16: True\n",
      "10/02/2023 22:23:00 - INFO - __main__ - Graph compilation...\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "10/02/2023 22:23:14 - INFO - __main__ - Running generate...\n",
      "\n",
      "Input/outputs:\n",
      "input 1: ('Can you write a short introduction about the relevance of the term monopsony in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.',)\n",
      "output 1: ('Can you write a short introduction about the relevance of the term monopsony in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\\nThe term monopsony is used to describe a market situation where there is only one buyer. In the labour market, this means that there is only one employer. The employer is the monopsonist.\\nThe monopsonist can set the wage rate. The monopsonist can also set the number of workers employed.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the working conditions.\\nThe monopsonist can also set the working hours.\\nThe monopsonist can also set the working location.\\nThe monopsonist can also set the working time.\\nThe monopsonist can also set the',)\n",
      "\n",
      "\n",
      "Stats:\n",
      "---------------------------------------------------------------------\n",
      "Throughput (including tokenization) = 131.5897886699392 tokens/second\n",
      "Memory allocated                    = 13.23 GB\n",
      "Max memory allocated                = 13.56 GB\n",
      "Total memory available              = 94.61 GB\n",
      "Graph compilation duration          = 14.368307742057368 seconds\n",
      "---------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_generation.py --model_name_or_path meta-llama/Llama-2-7b-hf --batch_size 1 --max_new_tokens 500 --n_iterations 4 --use_kv_cache --use_hpu_graphs --bf16 --prompt \"Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ff00f-95df-4d1b-9cd6-2891dba9251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
