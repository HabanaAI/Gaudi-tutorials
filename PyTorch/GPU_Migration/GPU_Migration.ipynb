{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "## BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# Model Migration from GPU to the Intel&reg; Gaudi&reg; 2 AI Processor \n",
    "\n",
    "The GPU Migration toolkit simplifies migrating PyTorch models that run on GPU-based architecture to run on the Intel® Gaudi® AI accelerator. Rather than manually replacing Python API calls that have dependencies on GPU libraries with Gaudi-specific API calls, the toolkit automates this process so you can run your model with fewer modifications.  \n",
    "\n",
    "In this notebook we will demonstrate how to use the GPU Migration toolset on a ResNet50 model which is based on open source implementation of ResNet50.  \n",
    "\n",
    "Refer to the [GPU Migration Toolkit](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html) for more information.  \n",
    "\n",
    "In addtion to this ResNet50 migration, there are addtional GPU Migration example on the Intel Gaudi GitHub page [here](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/examples/gpu_migration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Clone the Model-References repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/GPU_Migration\n",
      "Cloning into 'Model-References'...\n",
      "remote: Enumerating objects: 18994, done.\u001b[K\n",
      "remote: Counting objects: 100% (4783/4783), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2089/2089), done.\u001b[K\n",
      "remote: Total 18994 (delta 2579), reused 4522 (delta 2380), pack-reused 14211 (from 1)\u001b[K\n",
      "Receiving objects: 100% (18994/18994), 119.29 MiB | 40.87 MiB/s, done.\n",
      "Resolving deltas: 100% (10265/10265), done.\n",
      "Updating files: 100% (1622/1622), done.\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/GPU_Migration\n",
    "!git clone -b 1.16.2 https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset (Optional)\n",
    "To fully run this example you can download [Tiny ImageNet dataset](https://www.kaggle.com/datasets/saumandas/tiny-imagenet) from Kaggle. It needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch).   \n",
    "Run the cells in this section to continue downloading the dataset to your local folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d9c58-f1e8-473c-8928-30e34a974b9c",
   "metadata": {},
   "source": [
    "> **Note: You do not need to have the dataset loaded to see the Migration steps and logging.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282bdd7-b5af-4d6b-a050-a0776800f147",
   "metadata": {},
   "source": [
    "Configure your Kaggle API credentials. **You'll need to download your Kaggle API token (kaggle.json) from your Kaggle account settings to the local folder first.** \n",
    "Running this cell will place this file in the ~/.kaggle/ directory on this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbdde0f-7630-45e4-b2f8-155c535c3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet kaggle\n",
    "import os\n",
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "!cp ./kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d saumandas/tiny-imagenet\n",
    "!chmod 600 ./tiny-imagenet.zip\n",
    "os.makedirs(\"/root/datasets/\", exist_ok=True)\n",
    "!unzip ./tiny-imagenet.zip  -x \"tiny-imagenet-200/test*\" -d /root/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Navigate to the model example to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2212748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee70dc",
   "metadata": {},
   "source": [
    "#### Import GPU Migration Toolkit package and Habana Torch Library\n",
    "Look into train.py, you will see in the first line that we will load the `gpu.migration` libary which is already inclucded in the Intel Gaudi Software: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ff2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
        "     4\timport habana_frameworks.torch.gpu_migration\n",
        "     5\timport datetime\n",
        "     6\timport os\n",
        "     7\timport time\n",
        "     8\timport warnings\n",
        "     9\t\n",
        "    10\timport presets\n",
        "    11\timport torch\n",
        "    12\timport torch.utils.data\n",
        "    13\timport torchvision\n",
        "    14\timport transforms\n",
        "    15\timport utils\n",
        "    16\timport habana_frameworks.torch.utils.experimental as htexp\n",
        "    17\tfrom sampler import RASampler\n",
        "    18\tfrom torch import nn\n",
        "    19\tfrom torch.utils.data.dataloader import default_collate\n",
        "    20\tfrom torchvision.transforms.functional import InterpolationMode\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 20 | tail -n 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed748205",
   "metadata": {},
   "source": [
    "#### Placing mark_step()\n",
    "You will have to place the mark_step() function after the optimizer and loss.backward calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4e1aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    38\t        optimizer.zero_grad(set_to_none=True)\n",
      "    39\t        if scaler is not None:\n",
      "    40\t            scaler.scale(loss).backward()\n",
      "    41\t            htcore.mark_step()\n",
      "    42\t            if args.clip_grad_norm is not None:\n",
      "    43\t                # we should unscale the gradients of optimizer's assigned params if do gradient clipping\n",
      "    44\t                scaler.unscale_(optimizer)\n",
      "    45\t                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
      "    46\t            scaler.step(optimizer)\n",
      "    47\t            htcore.mark_step()\n",
      "    48\t            scaler.update()\n",
      "    49\t        else:\n",
      "    50\t            loss.backward()\n",
      "    51\t            htcore.mark_step()\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 51 | tail -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start multi-HPU training.\n",
    "We're now ready to run the training.  You will see that we've added the logging command at the beginning of the run `GPU_MIGRATION_LOG_LEVEL=1` to show the output.   There are no other changes to the run command are needed.   As you see the training run is started, you will see the log files show exactly where the code changes are happening to change from GPU to Intel Gaudi, including the file name and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=\"/root/datasets/tiny-imagenet-200/\" --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-09-05/17-34-16/gpu_migration_770.log\n",
      "[2024-09-05 17:34:17] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2024-09-05 17:34:17] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=1, rank=0, store=None, group_name=, pg_options=None, device_id=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:18] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:118\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 16\n",
      "CPU RAM       : 113321340 KB\n",
      "------------------------------------------------------------------------------\n",
      "Namespace(use_torch_compile=False, data_path='/root/datasets/tiny-imagenet-200/', model='resnet50', device='cuda', batch_size=256, epochs=1, dl_worker_type='HABANA', workers=8, opt='sgd', lr=0.1, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='custom_lr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=10, output_dir='.', resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=True, world_size=1, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, save_checkpoint=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
      "[2024-09-05 17:34:20] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Took 0.2374558448791504\n",
      "Loading validation data\n",
      "Creating data loaders\n",
      "HabanaDataLoader device type  4\n",
      "Warning: Updated shuffle to True as sampler is DistributedSampler with shuffle True\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f5806f986a0>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform RandomResizedCrop: Random Crop,Resize w:h  224 224  scale:  (0.08, 1.0)  ratio:  (0.75, 1.3333333333333333)  interpolation:  InterpolationMode.BILINEAR\n",
      "transform RandomHorizontalFlip: probability  0.5\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 1 instance id 0\n",
      " Warning!!!!!! : Unsupported device please use legacy/cpu/mixed\n",
      "Falling back to legacy\n",
      "MediaPipe device legacy device_type legacy device_id 0 pipe_name HPUMediaPipe:1\n",
      "MediaDataloader 0/1 seed : 438432134\n",
      "Decode ResizedCrop w:h 224 224\n",
      "MediaDataloader shuffle is  True\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 0 classes 200\n",
      "Failed to initialize Habana Dataloader, error: image list is empty\n",
      "Running with PyTorch Dataloader\n",
      "[2024-09-05 17:34:21] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:254\n",
      "    [context]:     data_loader = data_loader_type(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.utils.data.DataLoader.__init__(dataset=dataset, batch_size=256, shuffle=None, sampler=<torch.utils.data.distributed.DistributedSampler object at 0x7f5806f98fa0>, batch_sampler=None, num_workers=8, collate_fn=None, pin_memory=True, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, prefetch_factor=None, persistent_workers=False, pin_memory_device=, ) --> change pin_memory_device to hpu\n",
      "\u001b[0m\n",
      "Exception ignored in: <function HPUGenericPytorchIterator.__del__ at 0x7f57ba2a9c60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/habana_frameworks/mediapipe/plugins/iterator_pytorch.py\", line 254, in __del__\n",
      "    del self.pipe\n",
      "AttributeError: pipe\n",
      "HabanaDataLoader device type  4\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f5806f99780>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform Resize: w:h  256 256  interpolation:  InterpolationMode.BILINEAR  max_size:  None\n",
      "transform CenterCrop: w:h  224 224\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 1 instance id 0\n",
      " Warning!!!!!! : Unsupported device please use legacy/cpu/mixed\n",
      "Falling back to legacy\n",
      "MediaPipe device legacy device_type legacy device_id 0 pipe_name HPUMediaPipe:2\n",
      "MediaDataloader 0/1 seed : 494720613\n",
      "Decode w:h  256 256  , Crop disabled\n",
      "MediaDataloader shuffle is  False\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 10000 classes 1\n",
      "num_slices 1 slice_index 0\n",
      "random seed used  494720613\n",
      "sliced media files/labels 10000\n",
      "Finding largest file ...\n",
      "largest file is  /root/datasets/tiny-imagenet-200/val/images/val_272.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 1, instance_id = 0.\n",
      "Creating model\n",
      "[2024-09-05 17:34:21] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:179\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "Using HPU Graphs on Gaudi2 for reducing operator accumulation time.\n",
      "[2024-09-05 17:34:21] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:316\n",
      "    [context]:     scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
      "\n",
      "\u001b[93m    [hpu_modified]: torch.cuda.amp.GradScaler.__init__(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True, ) --> set enabled to Flase\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:21] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:359\n",
      "    [context]:         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], broadcast_buffers=False, gradient_as_bucket_view=True, bucket_cap_mb=1024)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.nn.parallel.DistributedDataParallel.__init__(module=module, device_ids=[0], output_device=None, dim=0, broadcast_buffers=False, process_group=None, bucket_cap_mb=1024, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=True, static_graph=False, delay_all_reduce_named_params=None, param_to_hook_all_reduce=None, mixed_precision=None, device_mesh=None, ) --> change device_ids and output_device to None\n",
      "\u001b[0m\n",
      "Start training\n",
      "[2024-09-05 17:34:22] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:113\n",
      "    [context]:         if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557442, ) --> torch.hpu.random.manual_seed_all(5902825849599557442)\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557443, ) --> torch.hpu.random.manual_seed_all(5902825849599557443)\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557444, ) --> torch.hpu.random.manual_seed_all(5902825849599557444)\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557445, ) --> torch.hpu.random.manual_seed_all(5902825849599557445)\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557446, ) --> torch.hpu.random.manual_seed_all(5902825849599557446)\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1055\n",
      "    [context]:                 current_device = torch.cuda.current_device()  # choose cuda for default\n",
      "\n",
      "\u001b[93m    [hpu_modified]: torch.cuda.current_device() --> habana_frameworks.torch.gpu_migration.torch.cuda.current_device()\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557447, ) --> torch.hpu.random.manual_seed_all(5902825849599557447)\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "[2024-09-05 17:34:22] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557448, ) --> torch.hpu.random.manual_seed_all(5902825849599557448)\n",
      "\u001b[0m\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557449, ) --> torch.hpu.random.manual_seed_all(5902825849599557449)\n",
      "\u001b[0m\n",
      "\n",
      "[2024-09-05 17:34:23] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:33\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:23] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:34\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:23] /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/common.py:9\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec(\"torch_xla\"))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:32] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-09-05 17:34:32] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/391]  eta: 1:09:36  lr: 0.1  img/s: 23.968414980254007  loss: 5.4873 (5.4873)  acc1: 0.7812 (0.7812)  acc5: 3.1250 (3.1250)  time: 10.6807  data: 0.9140  max mem: 12938\n",
      "Epoch: [0]  [ 10/391]  eta: 0:06:47  lr: 0.1  img/s: 2367.5591629702326  loss: 5.4873 (7.2862)  acc1: 0.7812 (0.7812)  acc5: 2.3438 (2.7344)  time: 1.0686  data: 0.0832  max mem: 66819\n",
      "Epoch: [0]  [ 20/391]  eta: 0:03:34  lr: 0.1  img/s: 6168.77804122355  loss: 5.4873 (6.6778)  acc1: 0.7812 (0.6510)  acc5: 3.1250 (2.8646)  time: 0.0742  data: 0.0002  max mem: 66819\n",
      "Epoch: [0]  [ 30/391]  eta: 0:02:26  lr: 0.1  img/s: 6176.211748897908  loss: 5.4728 (6.3766)  acc1: 0.3906 (0.5859)  acc5: 2.3438 (2.4414)  time: 0.0411  data: 0.0012  max mem: 66819\n",
      "Epoch: [0]  [ 40/391]  eta: 0:01:51  lr: 0.1  img/s: 5768.29321149316  loss: 5.4728 (6.1820)  acc1: 0.7812 (0.7031)  acc5: 2.7344 (2.5000)  time: 0.0426  data: 0.0100  max mem: 66819\n",
      "Epoch: [0]  [ 50/391]  eta: 0:01:30  lr: 0.1  img/s: 5452.652708446534  loss: 5.4610 (6.0424)  acc1: 0.3906 (0.5859)  acc5: 2.3438 (2.3438)  time: 0.0451  data: 0.0165  max mem: 66819\n",
      "Epoch: [0]  [ 60/391]  eta: 0:01:16  lr: 0.1  img/s: 4293.377029148403  loss: 5.4610 (5.9365)  acc1: 0.7812 (0.6138)  acc5: 2.7344 (2.3996)  time: 0.0528  data: 0.0218  max mem: 66819\n",
      "Epoch: [0]  [ 70/391]  eta: 0:01:05  lr: 0.1  img/s: 5074.806761230119  loss: 5.4036 (5.8575)  acc1: 0.3906 (0.5371)  acc5: 2.3438 (2.1973)  time: 0.0547  data: 0.0256  max mem: 66819\n",
      "Epoch: [0]  [ 80/391]  eta: 0:00:58  lr: 0.1  img/s: 4441.444354224585  loss: 5.4036 (5.7964)  acc1: 0.3906 (0.5208)  acc5: 2.3438 (2.2135)  time: 0.0537  data: 0.0258  max mem: 66819\n",
      "Epoch: [0]  [ 90/391]  eta: 0:00:51  lr: 0.1  img/s: 5022.6885703553635  loss: 5.3447 (5.7503)  acc1: 0.3906 (0.5859)  acc5: 2.3438 (2.3047)  time: 0.0540  data: 0.0235  max mem: 66819\n",
      "Epoch: [0]  [100/391]  eta: 0:00:46  lr: 0.1  img/s: 4461.024470904967  loss: 5.3447 (5.7107)  acc1: 0.3906 (0.5682)  acc5: 2.3438 (2.3082)  time: 0.0538  data: 0.0239  max mem: 66819\n",
      "Epoch: [0]  [110/391]  eta: 0:00:42  lr: 0.1  img/s: 5044.513378691625  loss: 5.3358 (5.6759)  acc1: 0.3906 (0.6185)  acc5: 2.3438 (2.3438)  time: 0.0537  data: 0.0264  max mem: 66819\n",
      "Epoch: [0]  [120/391]  eta: 0:00:38  lr: 0.1  img/s: 4438.871974309535  loss: 5.3358 (5.6456)  acc1: 0.7812 (0.6310)  acc5: 2.3438 (2.3438)  time: 0.0539  data: 0.0269  max mem: 66819\n",
      "Epoch: [0]  [130/391]  eta: 0:00:35  lr: 0.1  img/s: 4412.977371682587  loss: 5.3140 (5.6213)  acc1: 0.3906 (0.5859)  acc5: 2.3438 (2.2879)  time: 0.0575  data: 0.0310  max mem: 66819\n",
      "Epoch: [0]  [140/391]  eta: 0:00:32  lr: 0.1  img/s: 4735.02540506549  loss: 5.3140 (5.5999)  acc1: 0.3906 (0.5469)  acc5: 2.3438 (2.3958)  time: 0.0557  data: 0.0292  max mem: 66819\n",
      "Epoch: [0]  [150/391]  eta: 0:00:30  lr: 0.1  img/s: 4419.605473890274  loss: 5.3140 (5.5822)  acc1: 0.3906 (0.5371)  acc5: 2.3438 (2.4658)  time: 0.0556  data: 0.0290  max mem: 66819\n",
      "Epoch: [0]  [160/391]  eta: 0:00:28  lr: 0.1  img/s: 4486.98700759252  loss: 5.3140 (5.5656)  acc1: 0.3906 (0.5285)  acc5: 2.3438 (2.4127)  time: 0.0571  data: 0.0306  max mem: 66819\n",
      "Epoch: [0]  [170/391]  eta: 0:00:26  lr: 0.1  img/s: 4427.9455866809685  loss: 5.3075 (5.5496)  acc1: 0.3906 (0.5425)  acc5: 2.3438 (2.4523)  time: 0.0571  data: 0.0303  max mem: 66819\n",
      "Epoch: [0]  [180/391]  eta: 0:00:24  lr: 0.1  img/s: 4460.635290696467  loss: 5.3075 (5.5364)  acc1: 0.3906 (0.5140)  acc5: 2.3438 (2.3849)  time: 0.0573  data: 0.0294  max mem: 66819\n",
      "Epoch: [0]  [190/391]  eta: 0:00:22  lr: 0.1  img/s: 4627.076647812039  loss: 5.3062 (5.5244)  acc1: 0.3906 (0.5273)  acc5: 2.3438 (2.4219)  time: 0.0560  data: 0.0275  max mem: 66819\n",
      "Epoch: [0]  [200/391]  eta: 0:00:20  lr: 0.1  img/s: 4612.09296332075  loss: 5.3046 (5.5126)  acc1: 0.3906 (0.5580)  acc5: 2.3438 (2.4554)  time: 0.0551  data: 0.0262  max mem: 66819\n",
      "Epoch: [0]  [210/391]  eta: 0:00:19  lr: 0.1  img/s: 4646.768568174186  loss: 5.3009 (5.5030)  acc1: 0.3906 (0.5327)  acc5: 2.3438 (2.4503)  time: 0.0550  data: 0.0270  max mem: 66819\n",
      "Epoch: [0]  [220/391]  eta: 0:00:17  lr: 0.1  img/s: 4648.617370527173  loss: 5.3006 (5.4941)  acc1: 0.3906 (0.5265)  acc5: 2.3438 (2.4117)  time: 0.0547  data: 0.0277  max mem: 66819\n",
      "Epoch: [0]  [230/391]  eta: 0:00:16  lr: 0.1  img/s: 4592.062733935036  loss: 5.2996 (5.4854)  acc1: 0.3906 (0.5371)  acc5: 2.3438 (2.5065)  time: 0.0550  data: 0.0281  max mem: 66819\n",
      "Epoch: [0]  [240/391]  eta: 0:00:15  lr: 0.1  img/s: 4612.7686015740455  loss: 5.2991 (5.4780)  acc1: 0.3906 (0.5312)  acc5: 2.3438 (2.4844)  time: 0.0552  data: 0.0277  max mem: 66819\n",
      "Epoch: [0]  [250/391]  eta: 0:00:13  lr: 0.1  img/s: 4264.647359135491  loss: 5.2991 (5.4704)  acc1: 0.3906 (0.5859)  acc5: 2.3438 (2.5841)  time: 0.0574  data: 0.0292  max mem: 66819\n",
      "Epoch: [0]  [260/391]  eta: 0:00:12  lr: 0.1  img/s: 4201.8542067778035  loss: 5.2985 (5.4631)  acc1: 0.3906 (0.6510)  acc5: 2.3438 (2.7199)  time: 0.0601  data: 0.0327  max mem: 66819\n",
      "Epoch: [0]  [270/391]  eta: 0:00:11  lr: 0.1  img/s: 3723.7357981097325  loss: 5.2980 (5.4569)  acc1: 0.3906 (0.6417)  acc5: 2.7344 (2.7344)  time: 0.0645  data: 0.0379  max mem: 66819\n",
      "Epoch: [0]  [280/391]  eta: 0:00:10  lr: 0.1  img/s: 4611.041263711913  loss: 5.2958 (5.4500)  acc1: 0.3906 (0.6466)  acc5: 3.1250 (2.7883)  time: 0.0618  data: 0.0354  max mem: 66819\n",
      "Epoch: [0]  [290/391]  eta: 0:00:09  lr: 0.1  img/s: 3633.7572066541497  loss: 5.2929 (5.4430)  acc1: 0.3906 (0.6771)  acc5: 3.1250 (2.8646)  time: 0.0626  data: 0.0362  max mem: 66819\n",
      "Epoch: [0]  [300/391]  eta: 0:00:08  lr: 0.1  img/s: 4019.5990365616904  loss: 5.2874 (5.4373)  acc1: 0.7812 (0.6804)  acc5: 3.1250 (2.9486)  time: 0.0667  data: 0.0405  max mem: 66819\n",
      "Epoch: [0]  [310/391]  eta: 0:00:07  lr: 0.1  img/s: 3298.1177901597853  loss: 5.2865 (5.4317)  acc1: 0.3906 (0.6592)  acc5: 3.1250 (3.0151)  time: 0.0703  data: 0.0436  max mem: 66819\n",
      "Epoch: [0]  [320/391]  eta: 0:00:06  lr: 0.1  img/s: 3982.912476153347  loss: 5.2865 (5.4259)  acc1: 0.3906 (0.6510)  acc5: 3.1250 (3.0066)  time: 0.0706  data: 0.0441  max mem: 66819\n",
      "Epoch: [0]  [330/391]  eta: 0:00:05  lr: 0.1  img/s: 3823.5714120383145  loss: 5.2805 (5.4195)  acc1: 0.3906 (0.6434)  acc5: 3.1250 (3.1135)  time: 0.0653  data: 0.0388  max mem: 66819\n",
      "Epoch: [0]  [340/391]  eta: 0:00:04  lr: 0.1  img/s: 4614.082799298869  loss: 5.2787 (5.4138)  acc1: 0.3906 (0.6362)  acc5: 3.1250 (3.1585)  time: 0.0609  data: 0.0339  max mem: 66819\n",
      "Epoch: [0]  [350/391]  eta: 0:00:03  lr: 0.1  img/s: 4425.379695317243  loss: 5.2774 (5.4078)  acc1: 0.3906 (0.6402)  acc5: 3.1250 (3.2010)  time: 0.0563  data: 0.0291  max mem: 66819\n",
      "Epoch: [0]  [360/391]  eta: 0:00:02  lr: 0.1  img/s: 4278.341660308043  loss: 5.2756 (5.4013)  acc1: 0.7812 (0.6757)  acc5: 4.2969 (3.2728)  time: 0.0585  data: 0.0309  max mem: 66819\n",
      "Epoch: [0]  [370/391]  eta: 0:00:01  lr: 0.1  img/s: 4046.677719190274  loss: 5.2672 (5.3937)  acc1: 0.7812 (0.6785)  acc5: 4.6875 (3.3306)  time: 0.0612  data: 0.0325  max mem: 66819\n",
      "Epoch: [0]  [380/391]  eta: 0:00:00  lr: 0.1  img/s: 4326.007263315743  loss: 5.2589 (5.3867)  acc1: 0.7812 (0.7111)  acc5: 4.6875 (3.4155)  time: 0.0609  data: 0.0314  max mem: 66819\n",
      "Epoch: [0]  [390/391]  eta: 0:00:00  lr: 0.1  img/s: 192.47058722304638  loss: 5.2573 (5.3785)  acc1: 0.7812 (0.7098)  acc5: 4.6875 (3.4503)  time: 0.4449  data: 0.0309  max mem: 66819\n",
      "Epoch: [0] Total time: 0:00:41\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2024-09-05 17:35:05] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:83\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-09-05 17:35:05] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:84\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/40]  eta: 0:04:49  loss: 5.3083 (5.3083)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 7.2437  data: 1.6559  max mem: 66819\n",
      "Test:  Total time: 0:00:08\n",
      "[2024-09-05 17:35:12] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(10240,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(10240,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-09-05 17:35:12] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([40, 212.83860158920288],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([40, 212.83860158920288],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-09-05 17:35:12] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([10240, 0.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([10240, 0.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.000 Acc@5 0.000\n",
      "Training time 0:00:50\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=\"/root/datasets/tiny-imagenet-200/\" --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520ab-cf50-42b3-aa80-41b7b265dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
