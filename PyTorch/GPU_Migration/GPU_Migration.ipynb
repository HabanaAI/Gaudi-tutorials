{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "## BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# Model Migration from GPU to the Intel&reg; Gaudi&reg; 2 AI Processor \n",
    "\n",
    "The GPU Migration toolkit simplifies migrating PyTorch models that run on GPU-based architecture to run on the Intel® Gaudi® AI accelerator. Rather than manually replacing Python API calls that have dependencies on GPU libraries with Gaudi-specific API calls, the toolkit automates this process so you can run your model with fewer modifications.  \n",
    "\n",
    "In this notebook we will demonstrate how to use the GPU Migration toolset on a ResNet50 model which is based on open source implementation of ResNet50.  \n",
    "\n",
    "Refer to the [GPU Migration Toolkit](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html) for more information.  \n",
    "\n",
    "In addtion to this ResNet50 migration, there are addtional GPU Migration example on the Intel Gaudi GitHub page [here](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/examples/gpu_migration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Clone the Model-References repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/GPU_Migration\n",
      "Cloning into 'Model-References'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 18994, done.\u001b[K\n",
      "remote: Counting objects: 100% (4783/4783), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2090/2090), done.\u001b[K\n",
      "remote: Total 18994 (delta 2579), reused 4521 (delta 2379), pack-reused 14211 (from 1)\u001b[K\n",
      "Receiving objects: 100% (18994/18994), 119.29 MiB | 47.73 MiB/s, done.\n",
      "Resolving deltas: 100% (10265/10265), done.\n",
      "Updating files: 100% (1440/1440), done.\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/GPU_Migration\n",
    "!git clone -b 1.17.1 https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset (Optional)\n",
    "To fully run this example you can download [Tiny ImageNet dataset](http://cs231n.stanford.edu/tiny-imagenet-200.zip). It needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch).   \n",
    "Run the cell below to continue downloading the dataset to your local folder and set it up for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d9c58-f1e8-473c-8928-30e34a974b9c",
   "metadata": {},
   "source": [
    "> **Note: You do not need to have the dataset loaded to see the Migration steps and logging.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbdde0f-7630-45e4-b2f8-155c535c3ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-10 20:25:51--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Resolving proxy-us.intel.com (proxy-us.intel.com)... 10.7.211.16\n",
      "Connecting to proxy-us.intel.com (proxy-us.intel.com)|10.7.211.16|:911... connected.\n",
      "Proxy request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
      "--2024-09-10 20:25:51--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
      "Connecting to proxy-us.intel.com (proxy-us.intel.com)|10.7.211.16|:912... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 248100043 (237M) [application/zip]\n",
      "Saving to: ‘tiny-imagenet-200.zip’\n",
      "\n",
      "tiny-imagenet-200.z 100%[===================>] 236.61M  78.0MB/s    in 3.0s    \n",
      "\n",
      "2024-09-10 20:25:54 (78.0 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --progress=bar:force http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!chmod 600 ./tiny-imagenet-200.zip\n",
    "import os;os.makedirs(\"/root/datasets/\", exist_ok=True)\n",
    "!unzip -q ./tiny-imagenet-200.zip  -x \"tiny-imagenet-200/test*\" -d /root/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Navigate to the model example to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2212748b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c93d19",
   "metadata": {},
   "source": [
    "#### Import Habana Torch Library\n",
    "Look into train.py, you will see Line 20 where we import Intel Gaudi Torch core library for enabling lazy mode: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce44d1a1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19\tfrom torchvision.transforms.functional import InterpolationMode\n",
      "    20\timport habana_frameworks.torch.core as htcore\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 20 | tail -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5ef8c3",
   "metadata": {},
   "source": [
    "#### Placing mark_step()\n",
    "For lazy mode runs, you will have to place the mark_step() function after the optimizer and loss.backward calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b273c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    37\t        optimizer.zero_grad(set_to_none=True)\n",
      "    38\t        if scaler is not None:\n",
      "    39\t            scaler.scale(loss).backward()\n",
      "    40\t            htcore.mark_step()\n",
      "    41\t            if args.clip_grad_norm is not None:\n",
      "    42\t                # we should unscale the gradients of optimizer's assigned params if do gradient clipping\n",
      "    43\t                scaler.unscale_(optimizer)\n",
      "    44\t                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
      "    45\t            scaler.step(optimizer)\n",
      "    46\t            htcore.mark_step()\n",
      "    47\t            scaler.update()\n",
      "    48\t        else:\n",
      "    49\t            loss.backward()\n",
      "    50\t            htcore.mark_step()\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 50 | tail -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a18ef-c6d1-486d-af0b-6800ee21ef34",
   "metadata": {},
   "source": [
    "> **Note: Starting from version 1.17.0, The GPU code can now be migrated by setting a single environment variable:**\n",
    ">```bash\n",
    ">PT_HPU_GPU_MIGRATION=1\n",
    ">```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start multi-HPU training.\n",
    "We're now ready to run the training.  You will see that we've added GPU migration enabling environment variable `PT_HPU_GPU_MIGRATION=1` at the beginning of the run and using the logging command `GPU_MIGRATION_LOG_LEVEL=1` to show the output. There are no other changes to the run command are needed. Once the training run has started, you will see the log files show exactly where the code changes are happening to change from GPU to Intel Gaudi, including the file name and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "PT_HPU_GPU_MIGRATION=1 GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=\"/root/datasets/tiny-imagenet-200/\" --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-09-10/21-08-02/gpu_migration_3079.log\n",
      "[2024-09-10 21:08:02] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2024-09-10 21:08:02] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=1, rank=0, store=None, group_name=, pg_options=None, device_id=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:03] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:118\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 16\n",
      "CPU RAM       : 113419484 KB\n",
      "------------------------------------------------------------------------------\n",
      "Namespace(use_torch_compile=False, data_path='/root/datasets/tiny-imagenet-200/', model='resnet50', device='cuda', batch_size=256, epochs=1, dl_worker_type='HABANA', workers=8, opt='sgd', lr=0.1, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='custom_lr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=10, output_dir='.', resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=True, world_size=1, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, save_checkpoint=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
      "[2024-09-10 21:08:06] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Took 0.24302029609680176\n",
      "Loading validation data\n",
      "Creating data loaders\n",
      "HabanaDataLoader device type  4\n",
      "Warning: Updated shuffle to True as sampler is DistributedSampler with shuffle True\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f89fdfc2f20>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform RandomResizedCrop: Random Crop,Resize w:h  224 224  scale:  (0.08, 1.0)  ratio:  (0.75, 1.3333333333333333)  interpolation:  InterpolationMode.BILINEAR\n",
      "transform RandomHorizontalFlip: probability  0.5\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 1 instance id 0\n",
      " Warning!!!!!! : Unsupported device please use legacy/cpu/mixed\n",
      "Falling back to legacy\n",
      "MediaPipe device legacy device_type legacy device_id 0 pipe_name HPUMediaPipe:1\n",
      "MediaDataloader 0/1 seed : 829222115\n",
      "Decode ResizedCrop w:h 224 224\n",
      "MediaDataloader shuffle is  True\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 0 classes 200\n",
      "Failed to initialize Habana Dataloader, error: image list is empty\n",
      "Running with PyTorch Dataloader\n",
      "[2024-09-10 21:08:07] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:253\n",
      "    [context]:     data_loader = data_loader_type(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.utils.data.DataLoader.__init__(dataset=dataset, batch_size=256, shuffle=None, sampler=<torch.utils.data.distributed.DistributedSampler object at 0x7f89fdfc2d40>, batch_sampler=None, num_workers=8, collate_fn=None, pin_memory=True, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, prefetch_factor=None, persistent_workers=False, pin_memory_device=, ) --> change pin_memory_device to hpu\n",
      "\u001b[0m\n",
      "Exception ignored in: <function HPUGenericPytorchIterator.__del__ at 0x7f89b12cbeb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/habana_frameworks/mediapipe/plugins/iterator_pytorch.py\", line 254, in __del__\n",
      "    del self.pipe\n",
      "AttributeError: pipe\n",
      "HabanaDataLoader device type  4\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f89fdfc11e0>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform Resize: w:h  256 256  interpolation:  InterpolationMode.BILINEAR  max_size:  None\n",
      "transform CenterCrop: w:h  224 224\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 1 instance id 0\n",
      " Warning!!!!!! : Unsupported device please use legacy/cpu/mixed\n",
      "Falling back to legacy\n",
      "MediaPipe device legacy device_type legacy device_id 0 pipe_name HPUMediaPipe:2\n",
      "MediaDataloader 0/1 seed : 890314504\n",
      "Decode w:h  256 256  , Crop disabled\n",
      "MediaDataloader shuffle is  False\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 10000 classes 1\n",
      "num_slices 1 slice_index 0\n",
      "random seed used  890314504\n",
      "sliced media files/labels 10000\n",
      "Finding largest file ...\n",
      "largest file is  /root/datasets/tiny-imagenet-200/val/images/val_272.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 1, instance_id = 0.\n",
      "Creating model\n",
      "[2024-09-10 21:08:07] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:179\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "Using HPU Graphs on Gaudi2 for reducing operator accumulation time.\n",
      "[2024-09-10 21:08:07] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:315\n",
      "    [context]:     scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
      "\n",
      "\u001b[93m    [hpu_modified]: torch.cuda.amp.GradScaler.__init__(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True, ) --> set enabled to Flase\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:07] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:358\n",
      "    [context]:         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], broadcast_buffers=False, gradient_as_bucket_view=True, bucket_cap_mb=1024)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.nn.parallel.DistributedDataParallel.__init__(module=module, device_ids=[0], output_device=None, dim=0, broadcast_buffers=False, process_group=None, bucket_cap_mb=1024, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=True, static_graph=False, delay_all_reduce_named_params=None, param_to_hook_all_reduce=None, mixed_precision=None, device_mesh=None, ) --> change device_ids and output_device to None\n",
      "\u001b[0m\n",
      "Start training\n",
      "[2024-09-10 21:08:08] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:113\n",
      "    [context]:         if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557442, ) --> torch.hpu.random.manual_seed_all(5902825849599557442)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557443, ) --> torch.hpu.random.manual_seed_all(5902825849599557443)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557445, ) --> torch.hpu.random.manual_seed_all(5902825849599557445)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557444, ) --> torch.hpu.random.manual_seed_all(5902825849599557444)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557446, ) --> torch.hpu.random.manual_seed_all(5902825849599557446)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1055\n",
      "    [context]:                 current_device = torch.cuda.current_device()  # choose cuda for default\n",
      "\n",
      "\u001b[93m    [hpu_modified]: torch.cuda.current_device() --> habana_frameworks.torch.gpu_migration.torch.cuda.current_device()\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557447, ) --> torch.hpu.random.manual_seed_all(5902825849599557447)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557448, ) --> torch.hpu.random.manual_seed_all(5902825849599557448)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/random.py:45\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=5902825849599557449, ) --> torch.hpu.random.manual_seed_all(5902825849599557449)\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:32\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:33\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:08] /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/common.py:9\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec(\"torch_xla\"))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:19] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:19] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/391]  eta: 1:15:17  lr: 0.1  img/s: 22.15669366086132  loss: 5.4882 (5.4882)  acc1: 0.7812 (0.7812)  acc5: 3.5156 (3.5156)  time: 11.5541  data: 0.7537  max mem: 12938\n",
      "Epoch: [0]  [ 10/391]  eta: 0:07:20  lr: 0.1  img/s: 2202.1538797888625  loss: 5.4882 (7.4963)  acc1: 0.0000 (0.3906)  acc5: 1.5625 (2.5391)  time: 1.1556  data: 0.0687  max mem: 34630\n",
      "Epoch: [0]  [ 20/391]  eta: 0:03:52  lr: 0.1  img/s: 6013.904892485404  loss: 6.3821 (7.1249)  acc1: 0.7812 (0.6510)  acc5: 3.5156 (2.8646)  time: 0.0790  data: 0.0003  max mem: 34630\n",
      "Epoch: [0]  [ 30/391]  eta: 0:02:39  lr: 0.1  img/s: 4376.271416891578  loss: 5.9626 (6.8343)  acc1: 0.7812 (0.6836)  acc5: 3.1250 (2.9297)  time: 0.0501  data: 0.0070  max mem: 34630\n",
      "Epoch: [0]  [ 40/391]  eta: 0:02:03  lr: 0.1  img/s: 3507.375013188484  loss: 5.9626 (6.5491)  acc1: 0.7812 (0.6250)  acc5: 3.1250 (2.8906)  time: 0.0652  data: 0.0218  max mem: 34630\n",
      "Epoch: [0]  [ 50/391]  eta: 0:01:41  lr: 0.1  img/s: 3579.872093821869  loss: 5.4882 (6.3376)  acc1: 0.3906 (0.5859)  acc5: 3.1250 (3.1250)  time: 0.0718  data: 0.0290  max mem: 34630\n",
      "Epoch: [0]  [ 60/391]  eta: 0:01:26  lr: 0.1  img/s: 3610.3411853037906  loss: 5.4882 (6.1909)  acc1: 0.7812 (0.6696)  acc5: 3.1250 (3.1250)  time: 0.0708  data: 0.0299  max mem: 34630\n",
      "Epoch: [0]  [ 70/391]  eta: 0:01:14  lr: 0.1  img/s: 4107.834092355908  loss: 5.4083 (6.0899)  acc1: 0.3906 (0.5859)  acc5: 3.1250 (2.8809)  time: 0.0662  data: 0.0282  max mem: 34630\n",
      "Epoch: [0]  [ 80/391]  eta: 0:01:05  lr: 0.1  img/s: 3770.7580238303185  loss: 5.4083 (6.0080)  acc1: 0.3906 (0.5208)  acc5: 3.1250 (2.6910)  time: 0.0647  data: 0.0267  max mem: 34630\n",
      "Epoch: [0]  [ 90/391]  eta: 0:00:58  lr: 0.1  img/s: 4109.220660036754  loss: 5.3823 (5.9404)  acc1: 0.3906 (0.5078)  acc5: 3.1250 (2.8125)  time: 0.0646  data: 0.0251  max mem: 34630\n",
      "Epoch: [0]  [100/391]  eta: 0:00:53  lr: 0.1  img/s: 3645.897002752737  loss: 5.3823 (5.8831)  acc1: 0.3906 (0.5682)  acc5: 3.1250 (2.8409)  time: 0.0657  data: 0.0254  max mem: 34630\n",
      "Epoch: [0]  [110/391]  eta: 0:00:48  lr: 0.1  img/s: 3987.66358458312  loss: 5.3535 (5.8345)  acc1: 0.3906 (0.6836)  acc5: 3.1250 (2.8646)  time: 0.0668  data: 0.0279  max mem: 34630\n",
      "Epoch: [0]  [120/391]  eta: 0:00:44  lr: 0.1  img/s: 3572.343624977418  loss: 5.3535 (5.7925)  acc1: 0.3906 (0.6611)  acc5: 3.1250 (2.9447)  time: 0.0675  data: 0.0282  max mem: 34630\n",
      "Epoch: [0]  [130/391]  eta: 0:00:40  lr: 0.1  img/s: 4578.778289496128  loss: 5.3313 (5.7583)  acc1: 0.3906 (0.6696)  acc5: 3.1250 (2.9018)  time: 0.0633  data: 0.0187  max mem: 34630\n",
      "Epoch: [0]  [140/391]  eta: 0:00:37  lr: 0.1  img/s: 3606.587952040328  loss: 5.3414 (5.7305)  acc1: 0.3906 (0.6250)  acc5: 3.1250 (2.9167)  time: 0.0629  data: 0.0156  max mem: 34630\n",
      "Epoch: [0]  [150/391]  eta: 0:00:34  lr: 0.1  img/s: 4255.64176160932  loss: 5.3313 (5.7027)  acc1: 0.3906 (0.5859)  acc5: 3.1250 (2.9785)  time: 0.0651  data: 0.0214  max mem: 34630\n",
      "Epoch: [0]  [160/391]  eta: 0:00:31  lr: 0.1  img/s: 4127.661613358079  loss: 5.3313 (5.6786)  acc1: 0.3906 (0.5744)  acc5: 3.1250 (2.9642)  time: 0.0607  data: 0.0213  max mem: 34630\n",
      "Epoch: [0]  [170/391]  eta: 0:00:29  lr: 0.1  img/s: 4263.666864812042  loss: 5.3138 (5.6579)  acc1: 0.3906 (0.5642)  acc5: 3.1250 (2.8646)  time: 0.0606  data: 0.0217  max mem: 34630\n",
      "Epoch: [0]  [180/391]  eta: 0:00:27  lr: 0.1  img/s: 3986.4555045062607  loss: 5.3138 (5.6378)  acc1: 0.3906 (0.5551)  acc5: 3.1250 (2.8372)  time: 0.0615  data: 0.0215  max mem: 34630\n",
      "Epoch: [0]  [190/391]  eta: 0:00:25  lr: 0.1  img/s: 3761.8196449936327  loss: 5.3113 (5.6204)  acc1: 0.3906 (0.5469)  acc5: 3.1250 (2.7930)  time: 0.0656  data: 0.0254  max mem: 34630\n",
      "Epoch: [0]  [200/391]  eta: 0:00:23  lr: 0.1  img/s: 4190.7124284890215  loss: 5.3103 (5.6042)  acc1: 0.3906 (0.5394)  acc5: 3.1250 (2.8274)  time: 0.0642  data: 0.0236  max mem: 34630\n",
      "Epoch: [0]  [210/391]  eta: 0:00:21  lr: 0.1  img/s: 3909.771878776668  loss: 5.3051 (5.5895)  acc1: 0.3906 (0.5327)  acc5: 3.1250 (2.7876)  time: 0.0627  data: 0.0213  max mem: 34630\n",
      "Epoch: [0]  [220/391]  eta: 0:00:20  lr: 0.1  img/s: 3560.724532957057  loss: 5.3051 (5.5775)  acc1: 0.3906 (0.5265)  acc5: 2.7344 (2.7853)  time: 0.0681  data: 0.0273  max mem: 34630\n",
      "Epoch: [0]  [230/391]  eta: 0:00:18  lr: 0.1  img/s: 3677.5632648447963  loss: 5.2997 (5.5647)  acc1: 0.3906 (0.5371)  acc5: 2.7344 (2.8483)  time: 0.0702  data: 0.0317  max mem: 34630\n",
      "Epoch: [0]  [240/391]  eta: 0:00:17  lr: 0.1  img/s: 4069.690397959056  loss: 5.2997 (5.5551)  acc1: 0.3906 (0.5312)  acc5: 2.7344 (2.8125)  time: 0.0657  data: 0.0279  max mem: 34630\n",
      "Epoch: [0]  [250/391]  eta: 0:00:15  lr: 0.1  img/s: 4281.016320016331  loss: 5.3051 (5.5459)  acc1: 0.3906 (0.5108)  acc5: 2.7344 (2.8095)  time: 0.0609  data: 0.0215  max mem: 34630\n",
      "Epoch: [0]  [260/391]  eta: 0:00:14  lr: 0.1  img/s: 3911.1162656687156  loss: 5.2997 (5.5359)  acc1: 0.3906 (0.5208)  acc5: 2.7344 (2.8212)  time: 0.0621  data: 0.0198  max mem: 34630\n",
      "Epoch: [0]  [270/391]  eta: 0:00:13  lr: 0.1  img/s: 3984.8813618953  loss: 5.2997 (5.5275)  acc1: 0.3906 (0.5162)  acc5: 2.7344 (2.7762)  time: 0.0642  data: 0.0229  max mem: 34630\n",
      "Epoch: [0]  [280/391]  eta: 0:00:11  lr: 0.1  img/s: 3707.3200024168978  loss: 5.2940 (5.5191)  acc1: 0.3906 (0.4984)  acc5: 2.7344 (2.7613)  time: 0.0661  data: 0.0274  max mem: 34630\n",
      "Epoch: [0]  [290/391]  eta: 0:00:10  lr: 0.1  img/s: 4324.853765460841  loss: 5.2895 (5.5110)  acc1: 0.3906 (0.4948)  acc5: 2.7344 (2.7995)  time: 0.0637  data: 0.0246  max mem: 34630\n",
      "Epoch: [0]  [300/391]  eta: 0:00:09  lr: 0.1  img/s: 3521.7420612195765  loss: 5.2895 (5.5040)  acc1: 0.3906 (0.5166)  acc5: 2.7344 (2.8352)  time: 0.0654  data: 0.0262  max mem: 34630\n",
      "Epoch: [0]  [310/391]  eta: 0:00:08  lr: 0.1  img/s: 4056.911003824388  loss: 5.2893 (5.4972)  acc1: 0.3906 (0.5005)  acc5: 2.7344 (2.8809)  time: 0.0673  data: 0.0269  max mem: 34630\n",
      "Epoch: [0]  [320/391]  eta: 0:00:07  lr: 0.1  img/s: 3763.1881657476647  loss: 5.2893 (5.4897)  acc1: 0.3906 (0.5327)  acc5: 2.7344 (2.9238)  time: 0.0650  data: 0.0235  max mem: 34630\n",
      "Epoch: [0]  [330/391]  eta: 0:00:06  lr: 0.1  img/s: 3933.3666101429794  loss: 5.2851 (5.4831)  acc1: 0.3906 (0.5285)  acc5: 2.7344 (2.9527)  time: 0.0661  data: 0.0245  max mem: 34630\n",
      "Epoch: [0]  [340/391]  eta: 0:00:05  lr: 0.1  img/s: 3540.929245841458  loss: 5.2830 (5.4764)  acc1: 0.3906 (0.5357)  acc5: 2.7344 (2.9576)  time: 0.0682  data: 0.0260  max mem: 34630\n",
      "Epoch: [0]  [350/391]  eta: 0:00:04  lr: 0.1  img/s: 3972.6796839591316  loss: 5.2808 (5.4705)  acc1: 0.3906 (0.5208)  acc5: 2.7344 (2.9839)  time: 0.0678  data: 0.0259  max mem: 34630\n",
      "Epoch: [0]  [360/391]  eta: 0:00:03  lr: 0.1  img/s: 4198.121666890829  loss: 5.2799 (5.4652)  acc1: 0.3906 (0.5173)  acc5: 2.7344 (2.9772)  time: 0.0622  data: 0.0225  max mem: 34630\n",
      "Epoch: [0]  [370/391]  eta: 0:00:02  lr: 0.1  img/s: 4143.695876841667  loss: 5.2772 (5.4593)  acc1: 0.3906 (0.5448)  acc5: 3.1250 (3.0530)  time: 0.0609  data: 0.0209  max mem: 34630\n",
      "Epoch: [0]  [380/391]  eta: 0:00:01  lr: 0.1  img/s: 3822.11101025526  loss: 5.2772 (5.4540)  acc1: 0.3906 (0.5609)  acc5: 3.1250 (3.1250)  time: 0.0640  data: 0.0254  max mem: 34630\n",
      "Epoch: [0]  [390/391]  eta: 0:00:00  lr: 0.1  img/s: 180.58854725773628  loss: 5.2756 (5.4478)  acc1: 0.3906 (0.6013)  acc5: 3.5156 (3.1841)  time: 0.4761  data: 0.0299  max mem: 34630\n",
      "Epoch: [0] Total time: 0:00:45\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2024-09-10 21:08:55] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:82\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-09-10 21:08:55] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:83\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/40]  eta: 0:05:11  loss: 5.2984 (5.2984)  acc1: 0.0000 (0.0000)  acc5: 0.0000 (0.0000)  time: 7.7955  data: 1.8468  max mem: 34630\n",
      "Test:  Total time: 0:00:09\n",
      "[2024-09-10 21:09:02] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(10240,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(10240,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-09-10 21:09:02] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([40, 212.00530195236206],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([40, 212.00530195236206],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-09-10 21:09:02] /root/Gaudi-tutorials/PyTorch/GPU_Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([10240, 0.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([10240, 0.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.000 Acc@5 0.000\n",
      "Training time 0:00:54\n"
     ]
    }
   ],
   "source": [
    "!PT_HPU_GPU_MIGRATION=1 GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=\"/root/datasets/tiny-imagenet-200/\" --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "990520ab-cf50-42b3-aa80-41b7b265dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
