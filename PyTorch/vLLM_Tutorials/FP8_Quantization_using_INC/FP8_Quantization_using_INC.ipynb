{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP8 Quantization and Inference using Intel¬Æ Neural Compressor (INC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Note:** Before running this tutorial, it is assumed that the reader has already setup the Gaudi machine and Jupyter notebooks as laid out in the [README](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/README.md#important-to-run-these-jupyter-notebooks-you-will-need-to-follow-these-steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take a look at running inference via vLLM on HPU with FP8 precision achieved using [Intel¬Æ Neural Compressor](https://github.com/intel/neural-compressor) (INC) package. Using FP8 data type for inference on large language models halves the required memory bandwidth compared to BF16. In addition, FP8 compute is twice as fast as BF16 compute. These two benefits enable efficient deployment of LLMs using FP8 quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Calibration Process\n",
    "To enable inference with FP8 data type, the Intel Neural Compressor (INC) performs model measurement and quantization and this is called as model calibration procedure.\n",
    "The [vllm-hpu-extension](https://github.com/HabanaAI/vllm-hpu-extension/blob/main/calibration/README.md) provides automated scripts that utilize INC to perform the model calibration as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be quantizing HuggingFace models, specify your [Huggingface credentials](https://huggingface.co/docs/hub/en/security-tokens) (HF_TOKEN) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=\"<YOUR HF_TOKEN HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `NousResearch/Meta-Llama-3.1-70B-Instruct` model as an example here. You may specify your own Huggingface model that you would like to quantize to FP8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_NAME=\"NousResearch/Meta-Llama-3.1-70B-Instruct\"\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration script uses the publically available open_orca dataset from MLCommons to generate a calibration dataset. Run the following cell to download the dataset into the `/root/open_orca` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash download_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the `open_orca_gpt4_tokenized_llama.sampled_24576.pkl` has downloaded by running this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/open_orca/*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install vLLM (Pre-requisite and one-time only)\n",
    "\n",
    "The following cell installs vLLM server for Gaudi. For more information on installing vLLM for Gaudi refer to [Build And Install vLLM](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#2-build-and-install-the-latest-from-vllm-fork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root\n",
    "git clone https://github.com/HabanaAI/vllm-fork.git -b v0.6.6.post1+Gaudi-1.20.0 \n",
    "cd vllm-fork\n",
    "pip install -r --quiet requirements-hpu.txt\n",
    "python3 setup.py develop\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symlink python3 as default python (Needed for Calibration Script later on)\n",
    "!ln -s /usr/bin/python3 /usr/bin/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Calibration Script (one-time for each model)\n",
    "The following command downloads the calibration script and runs calibration on the selected model using the downloaded open_orca .pkl file. It generates `maxabs_quant_g3.json` on Gaudi 3 (or `maxabs_quant_g2.json` on a Gaudi 2)  which is a quantization configuration file in the `g3` folder.\n",
    "\n",
    "**NOTE**: Estimated time for completion of this step is **10 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root\n",
    "git clone https://github.com/HabanaAI/vllm-hpu-extension.git -b v1.20.0\n",
    "cd vllm-hpu-extension\n",
    "pip install -e .\n",
    "cd calibration\n",
    "./calibrate_model.sh -m $MODEL_NAME -d /root/open_orca/open_orca_gpt4_tokenized_llama.sampled_24576.pkl -o g3 -b 128 -t 8 -l 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Benchmark online serving throughput\n",
    "\n",
    "Once the calibration is complete we launch the vLLM server with special directives to load the model and quantize it to FP8. The [benchmark_serving.py](https://github.com/HabanaAI/vllm-fork/blob/habana_main/benchmarks/benchmark_serving.py) script is used for benchmarking the vLLM serving throughput in online mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the vLLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check path of the quantization config generated by the model calibration script from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lhart /root/vllm-hpu-extension/calibration/g3/*/maxabs_quant_*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the path your model's maxabs_quant .json config file from the above command and place it in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the full path of your model's maxabs_quant_[g2,g3].json file\n",
    "\n",
    "QUANT_CONFIG=\"<FULL_PATH_TO_YOUR_MODELS_MAXABS_QUANT>\"\n",
    "## e.g. QUANT_CONFIG=\"/root/vllm-hpu-extension/calibration/g3/meta-llama-3.1-70b-instruct/maxabs_quant_g2.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{QUANT_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the run_server.sh server launch script that takes your model name as input and launches the vLLM server.\n",
    "Note the use of `--quantization inc` and `--kv-cache-dtype fp8_inc` parameters that enable the FP8 quantization using INC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat run_server.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View this special command which will be used to launch the vllm server. Note the use of `QUANT_CONFIG` env variable and the `MODEL_NAME` given as input to the launcher script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"HF_TOKEN={HF_TOKEN} QUANT_CONFIG={QUANT_CONFIG} /bin/bash run_server.sh {MODEL_NAME}\"\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch vLLM server instance as a background process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "process = subprocess.Popen(command, shell=True, start_new_session=True, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Process ID:\", process.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server process would have launched in the background. Note the process ID for reference. The output logs of the server can can be seen in real-time in the ```server_TP8_fp8.log``` file.\n",
    "\n",
    "**NOTE**: Estimated time for server to startup is **10 minutes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run benchmark_serving.py on client side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having waited sufficient time for server to start, check the logs to confirm FP8 quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep \"Start to convert model with fp8_quant\" server_TP8_fp8.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, check the server instance readiness by checking the server output log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -B3 \"INFO:     Uvicorn running on\" server_TP8_fp8.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If above cell's output says **\"Application startup complete\"** , then run the following cell to launch the benchmarking script, else wait a few more minutes for the server to comeup and retry above command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HF_TOKEN={HF_TOKEN} python3 /root/vllm-fork/benchmarks/benchmark_serving.py --backend vllm \\\n",
    "--model $MODEL_NAME \\\n",
    "--dataset-name sonnet \\\n",
    "--dataset-path /root/vllm-fork/benchmarks/sonnet.txt \\\n",
    "--request-rate 4 \\\n",
    "--num-prompts 1000 \\\n",
    "--port 8080 \\\n",
    "--sonnet-input-len 2048 \\\n",
    "--sonnet-output-len 2048 \\\n",
    "--sonnet-prefix-len 100 2>&1 | tee client_2k_2k_fp8.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the benchmarking results in the `client_2k_2k_fp8.log` output file.\n",
    "Run the following cells before exiting this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 <PID_OF_SERVER_FROM_CELL_NUMBER_14_ABOVE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
