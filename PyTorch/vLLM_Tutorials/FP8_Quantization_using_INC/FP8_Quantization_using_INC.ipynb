{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP8 Quantization and Inference using Intel¬Æ Neural Compressor (INC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìù **Note:** Before running this tutorial, it is assumed that the reader has already setup the Gaudi machine and Jupyter notebooks as laid out in the [README](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/README.md#important-to-run-these-jupyter-notebooks-you-will-need-to-follow-these-steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take a look at running inference via vLLM on HPU with FP8 precision achieved using [Intel¬Æ Neural Compressor](https://github.com/intel/neural-compressor) (INC) package. Using FP8 data type for inference on large language models halves the required memory bandwidth compared to BF16. In addition, FP8 compute is twice as fast as BF16 compute. These two benefits enable efficient deployment of LLMs using FP8 quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Calibration Process\n",
    "To enable inference with FP8 data type, the Intel Neural Compressor (INC) performs model measurement and quantization and this is called as model calibration procedure.\n",
    "The [vllm-hpu-extension](https://github.com/HabanaAI/vllm-hpu-extension/blob/main/calibration/README.md) provides automated scripts that utilize INC to perform the model calibration as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Model and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be quantizing HuggingFace models, specify your [Huggingface credentials](https://huggingface.co/docs/hub/en/security-tokens) (HF_TOKEN) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=\"<YOUR HF_TOKEN HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the Huggingface model you would like to quantize to FP8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_NAME=\"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model checkpoint of your selected model is downloaded to `/root/models` directory by launching the `download_model.sh` script below. \n",
    "\n",
    "**NOTE**: This process of downloading the model is time consuming and will continue running in the background after you run this cell. Please immediately proceed to run the subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f\"HF_TOKEN={HF_TOKEN} /bin/bash download_model.sh {MODEL_NAME}\"\n",
    "process = subprocess.Popen(command, shell=True, start_new_session=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Process ID:\", process.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calibration script uses the publically available open_orca dataset from MLCommons to generate a calibration dataset. Run the following cell to download the dataset into the `/root/open_orca` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!download_dataset.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the open_orca_gpt4_tokenized_llama.sampled_24576.pkl has downloaded by running this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/open_orca/*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the model files have downloaded fully. Set the `CHECKPOINT_PATH` variable to point to the location where the `download_model.sh` script has downloaded the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=f'/root/models/{MODEL_NAME}'\n",
    "%env CHECKPOINT_PATH = {CHECKPOINT_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once all the files have been downloaded by the download_model.sh script**, the command below should show the size of the folder as ~263GB for Llama-3.1-8B-Instruct model. If not, then wait for the model to download fully and re-check periodically before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh $CHECKPOINT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install vLLM (Pre-requisite)\n",
    "\n",
    "The following cell installs vLLM server for Gaudi. For more information on installing vLLM for Gaudi refer to [Build And Install vLLM](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#2-build-and-install-the-latest-from-vllm-fork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root\n",
    "git clone https://github.com/HabanaAI/vllm-fork.git -b v0.6.6.post1+Gaudi-1.20.0 \n",
    "cd vllm-fork\n",
    "pip install -r requirements-hpu.txt\n",
    "python3 setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Calibration Script\n",
    "The following command downloads the calibration script and runs calibration on the model downloaded above using the downloaded open_orca .pkl file. It generates `maxabs_quant_g3.json` on Gaudi 3 (or `maxabs_quant_g2.json` on a Gaudi 2)  which is a quantization configuration file in the `g3` folder.\n",
    "\n",
    "**NOTE**: Estimated time for completion of this step is *10 minutes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root\n",
    "git clone https://github.com/HabanaAI/vllm-hpu-extension.git -b v1.20.0\n",
    "cd vllm-hpu-extension\n",
    "pip install -e .\n",
    "cd calibration\n",
    "./calibrate_model.sh -m $CHECKPOINT_PATH -d /root/open_orca/open_orca_gpt4_tokenized_llama.sampled_24576.pkl -o g3 -b 128 -t 8 -l 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Benchmark online serving throughput\n",
    "\n",
    "The [benchmark_serving.py](https://github.com/HabanaAI/vllm-fork/blob/habana_main/benchmarks/benchmark_serving.py) script is useful for benchmarking the vLLM serving throughput in online mode. For more details on online and offline modes of vLLM, refer [documentation](https://docs.habana.ai/en/v1.19.1/PyTorch/Inference_on_PyTorch/vLLM_Inference.html#sending-an-inference-request) and this [tutorial](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/PyTorch/Getting_Started_with_vLLM/Getting_Started_with_vLLM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the vLLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check path of the quantization config generated by the model calibration script from the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /root/vllm-hpu-extension/calibration/g3/meta-llama-3.1-70b-instruct/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the required environment variables before starting the server. Most importantly, point the `QUANT_CONFIG` variable to the full path quantization config .json as seen above. In this example, it is assumed to be `/root/vllm-hpu-extension/calibration/g3/meta-llama-3.1-70b-instruct/maxabs_quant_g2.json` for a Gaudi 2 machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env QUANT_CONFIG=/root/vllm-hpu-extension/calibration/g3/meta-llama-3.1-70b-instruct/maxabs_quant_g2.json\n",
    "\n",
    "%env PT_HPU_ENABLE_LAZY_COLLECTIVES=true\n",
    "%env PT_HPUGRAPH_DISABLE_TENSOR_CACHE=1\n",
    "%env VLLM_ENGINE_ITERATION_TIMEOUT_S=3600\n",
    "%env VLLM_PROMPT_BS_BUCKET_MAX=16\n",
    "%env VLLM_DECODE_BS_BUCKET_MAX=128\n",
    "%env VLLM_DECODE_BLOCK_BUCKET_MIN=2048\n",
    "%env VLLM_DECODE_BLOCK_BUCKET_MAX=4096\n",
    "%env VLLM_PROMPT_SEQ_BUCKET_MAX=2048\n",
    "%env VLLM_PROMPT_SEQ_BUCKET_MIN=2048\n",
    "%env VLLM_SKIP_WARMUP=\"true\"\n",
    "!cd /root/vllm-fork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this special start command to launch the vllm server. Note the use of `--quantization inc` and `--kv-cache-dtype fp8_inc` parameters that enable the FP8 quantization using INC and above `QUANT_CONFIG`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command=f\"python -m vllm.entrypoints.openai.api_server \\\n",
    "    --port 8080 \\\n",
    "    --model {CHECKPOINT_PATH} \\\n",
    "    --tensor-parallel-size 8 \\\n",
    "    --max-num-seqs 128 \\\n",
    "    --disable-log-requests \\\n",
    "    --dtype bfloat16 \\\n",
    "    --block-size 128 \\\n",
    "    --gpu-memory-util 0.9 \\\n",
    "    --num-lookahead-slots 1 \\\n",
    "    --use-v2-block-manager \\\n",
    "    --max-num-batched-tokens 32768 \\\n",
    "    --max-model-len 4096 \\\n",
    "    --quantization inc \\\n",
    "    --kv-cache-dtype fp8_inc \\\n",
    "    --weights-load-device cpu \\\n",
    "    2>&1 | tee server_70b_TP8_fp8.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch vLLM server instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(command, shell=True, start_new_session=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "print(\"Process ID for server:\", process.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server process would have launched in the background. Note the process ID for reference. The output logs of the server can can be seen in real-time in the ```server_70b_TP8_fp8.log``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run benchmark_serving.py on client side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the server instance readiness by checking the server output log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -B3 \"INFO:     Uvicorn running on\" server_70b_TP8_fp8.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If above cell's output says **\"Application startup complete\"** , then run the following cell to launch the benchmarking script, else wait a few more minutes for the server to comeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HF_TOKEN={HF_TOKEN} python3 vllm-fork/benchmarks/benchmark_serving.py --backend vllm \\\n",
    "--model $CHECKPOINT_PATH \\\n",
    "--dataset-name sonnet \\\n",
    "--dataset-path ./sonnet.txt \\\n",
    "--request-rate 4 \\\n",
    "--num-prompts 1000 \\\n",
    "--port 8080 \\\n",
    "--sonnet-input-len 2048 \\\n",
    "--sonnet-output-len 2048 \\\n",
    "--sonnet-prefix-len 100 2>&1 | tee client_70b_2k_2k_fp8.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !HF_TOKEN={HF_TOKEN} python vllm-fork/benchmarks/benchmark_serving.py --backend vllm --model \"{MODEL}\" --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000 --request-rate inf 2>&1 | tee vllm_benchmark_serving_client_stdout.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the bernchmarking results in the `client_70b_2k_2k_fp8.log` output file.\n",
    "Run the following cells before exiting this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 <PID_OF_SERVER_FROM_CELL_NUMBER_5_ABOVE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
