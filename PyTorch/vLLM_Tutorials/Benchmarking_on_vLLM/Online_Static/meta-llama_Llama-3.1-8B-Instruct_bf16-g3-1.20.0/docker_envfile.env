model=meta-llama/Llama-3.1-8B-Instruct
HABANA_VISIBLE_DEVICES=4
PYTCONTAINER=pyt-2.6.0-1.20.0-rel-ub22-vllm-v0.6.6.post1_gaudi-1.20.0-6af2f67
vllm_fork_tag=pre
server_type=vllm
bench_dataset=sonnet
client_config=ril_cfg3.csv
sever_cmd=vllm_bf16_warmopt.sh
dtype=bfloat16
quantization=inc
kv_cache_dtype=fp8_inc
block_size=128
PT_HPU_ENABLE_LAZY_COLLECTIVES=true
EXPERIMENTAL_WEIGHT_SHARING=0
max_model_len=2304
max_num_prefill_seqs=16
num_scheduler_steps=16
max_num_seqs=384
VLLM_SKIP_WARMUP=true
VLLM_DECODE_BLOCK_BUCKET_STEP=256
VLLM_PROMPT_SEQ_BUCKET_MAX=2304
VLLM_DECODE_BLOCK_BUCKET_MAX=6912
VLLM_GRAPH_RESERVED_MEM=0.05
gpu_memory_util=0.99
gpu=gaudi3
HF_TOKEN=Your_HF_TOKEN_Here
DOCKERFILE=Dockerfile-1.20.0-rel-ub22-vllm-v0.6.6.post1+Gaudi-1.20.0-6af2f67
HABANA_VISIBLE_MODULES=7
number_of_cards=1
HF_HOME=/hf_cache
