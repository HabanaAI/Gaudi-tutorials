DOCKER_FILE=Dockerfile-1.20.0-rel-ub24-vllm-v0.6.6.post1+Gaudi-1.20.0-6af2f67
EXPERIMENTAL_WEIGHT_SHARING=0
HABANA_VISIBLE_DEVICES=0,1,2,3
HABANA_VISIBLE_MODULES=3,0,1,6
HF_HOME=/hf_cache
HF_TOKEN=Your_HF_TOKEN_Here
PT_HPU_ENABLE_LAZY_COLLECTIVES=true
PYTCONTAINER=pyt-2.6.0-1.20.0-rel-ub24-vllm-v0.6.6.post1_gaudi-1.20.0-6af2f67
VLLM_DECODE_BLOCK_BUCKET_MAX=8298
VLLM_DECODE_BLOCK_BUCKET_STEP=256
VLLM_GRAPH_RESERVED_MEM=0.05
VLLM_PROMPT_SEQ_BUCKET_MAX=2304
VLLM_SKIP_WARMUP=false
bench_dataset=sonnet
block_size=128
client_config=llama-3.1-70B-Instruct-2k.csv
dtype=bfloat16
gpu=gaudi3
gpu_memory_util=0.99
kv_cache_dtype=fp8_inc
max_model_len=2304
max_num_prefill_seqs=16
max_num_seqs=461
model=meta-llama/Llama-3.1-70B-Instruct
num_scheduler_steps=16
number_of_cards=4
quantization=inc
server_cmd=vllm_bf16_warmopt.sh
server_type=vllm
