DOCKER_FILE=Dockerfile-1.20.0-rel-ub24-vllm-v0.6.6.post1+Gaudi-1.20.0-6af2f67
EXPERIMENTAL_WEIGHT_SHARING=0
HABANA_VISIBLE_DEVICES=0,1,2,3
HABANA_VISIBLE_MODULES=3,0,1,6
HF_HOME=/hf_cache
HF_TOKEN=Your_HF_TOKEN_Here
PT_HPU_ENABLE_LAZY_COLLECTIVES=true
PYTCONTAINER=pyt-2.6.0-1.20.0-rel-ub24-vllm-v0.6.6.post1_gaudi-1.20.0-6af2f67
VLLM_DECODE_BLOCK_BUCKET_MAX=9792
VLLM_DECODE_BLOCK_BUCKET_STEP=256
VLLM_GRAPH_RESERVED_MEM=0.05
VLLM_PROMPT_SEQ_BUCKET_MAX=4352
VLLM_SKIP_WARMUP=false
bench_dataset=sonnet
block_size=128
client_config=llama-3.3-70B-Instruct-4k.csv
dtype=bfloat16
gpu=gaudi3
gpu_memory_util=0.99
kv_cache_dtype=fp8_inc
max_model_len=4352
max_num_prefill_seqs=16
max_num_seqs=288
model=meta-llama/Llama-3.3-70B-Instruct
num_scheduler_steps=16
number_of_cards=4
numprompt_mult=10
quantization=inc
server_cmd=vllm_bf16_warmopt.sh
server_type=vllm
update_recipe=true
