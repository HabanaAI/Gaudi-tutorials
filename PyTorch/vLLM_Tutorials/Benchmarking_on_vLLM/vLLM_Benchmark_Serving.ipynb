{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing vLLM serving Benchmark on IntelÂ® GaudiÂ® 2 AI Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take a look at running inference via vLLM on HPU with FP8 precision achieved using IntelÂ® Neural Compressor (INC) package. This approach require a model calibration procedure to generate measurements, quantization files, and configurations first. \n",
    "\n",
    "It is assumed that the reader has already setup the Gaudi machine and Jupyter notebooks as laid out in the [README](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/README.md#important-to-run-these-jupyter-notebooks-you-will-need-to-follow-these-steps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM Installation and Environment Setup\n",
    "\n",
    "The following cell installs vLLM server for Gaudi. For more information on installing vLLM for Gaudi refer to [Build And Install vLLM](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#2-build-and-install-the-latest-from-vllm-fork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/HabanaAI/vllm-fork.git\n",
    "cd vllm-fork\n",
    "git checkout habana_main\n",
    "pip install -r requirements-hpu.txt\n",
    "python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Benchmark online serving throughput\n",
    "\n",
    "The [benchmark_serving.py](https://github.com/HabanaAI/vllm-fork/blob/habana_main/benchmarks/benchmark_serving.py) script is useful for benchmarking the vLLM serving throughput in online mode. For more details on online and offline modes of vLLM, refer [documentation](https://docs.habana.ai/en/v1.19.1/PyTorch/Inference_on_PyTorch/vLLM_Inference.html#sending-an-inference-request) and this [tutorial](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/PyTorch/Getting_Started_with_vLLM/Getting_Started_with_vLLM.ipynb).\n",
    "\n",
    "This benchmarking script measures the following  metrics:\n",
    "- Request throughput (requests/second)\n",
    "- Input token throughput (tokens/second)\n",
    "- Output token throughput (tokens/second)\n",
    "- Time to first token (TTFT) (milliseconds)\n",
    "- Time per output token (TPOT) (milliseconds)\n",
    "- Inter-token latency (ITL) (milliseconds)\n",
    "\n",
    "For more details on some of these metrics, please refer this [Intel LLM whitepaper](https://www.intel.com/content/dam/develop/public/us/en/documents/llm-with-model-server-white-paper.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Setup and execution\n",
    "Since we will be running HuggingFace models, specify your [Huggingface credentials](https://huggingface.co/docs/hub/en/security-tokens) (HF_TOKEN) below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN=\"<YOUR HF_TOKEN HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model you would like to benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"meta-llama/Meta-Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the vLLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since this is benchmarking in online mode, there are two sides i.e. server and client.\n",
    "In the following cell, run the following command to launch the vLLM OpenAI API server as a **background process**:\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server --model=\"meta-llama/Meta-Llama-3.1-8B\" --swap-space 16 --disable-log-requests --port 8000 --block-size 128\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quick checking of your model on vLLM and for demonstration purposes, set VLLM_SKIP_WARMUP environment variable to \"true\".\n",
    "This will bring up the vLLM server quicker by not running the warmup phase which can be time consuming (over 5-10 minutes) based on your model and vLLM configuration. However, this will result in non-optimal benchmarks.\n",
    "If you prefer to not skip the warmup phase and are prepared to wait 5-10 minutes for the vLLM server to come up, set this variable to \"false\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_SKIP_WARMUP=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f\"VLLM_SKIP_WARMUP={VLLM_SKIP_WARMUP} HF_TOKEN={HF_TOKEN} python -m vllm.entrypoints.openai.api_server --model={MODEL} --swap-space 16 --disable-log-requests --port 8000 --block-size 128\"\n",
    "\n",
    "with open('vllm_server_stdout.log', 'w') as file:\n",
    "    process = subprocess.Popen(command, shell=True, start_new_session=True, stdout=file, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Process ID:\", process.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Note**:\n",
    "> - For optimal performance, it is recommended to run inference with VLLM_SKIP_WARMUP=\"false\" on Gaudi 2 with ```--block-size``` of 128 for BF16 data type.\n",
    "> - To troubleshoot Out of Memory issues on your model, refer this section of [Gaudi documentation on vLLM](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/vLLM_Inference.html#basic-troubleshooting-for-out-of-memory-errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server process would have launched in the background. Note the process ID for reference. The output logs of the server can can be seen in real-time in the ```vllm_server_stdout.log``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run benchmark_serving.py on client side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Download the ShareGPT dataset file needed for this benchmark. The benchmarking script samples the requests from this dataset and sends it to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the client side, run the script in the following cells with the following self-explanatory parameters:\n",
    "```bash\n",
    "    python vllm-fork/benchmarks/benchmark_serving.py \\\n",
    "        --backend vllm \\\n",
    "        --model <your_model> \\\n",
    "        --dataset-name sharegpt \\\n",
    "        --dataset-path <path to dataset> \\\n",
    "        --request-rate <request_rate> \\ # By default <request_rate> is inf\n",
    "        --num-prompts <num_prompts> # By default <num_prompts> is 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“ **Note:** If you set VLLM_SKIP_WARMUP=\"false\" in the server setup phase, please ensure that server has fully come up by running the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep -B3 \"INFO:     Uvicorn running on\" vllm_server_stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If above cell's output says **\"Application startup complete\"** , then run the following cell to launch the benchmarking script, else wait a few more minutes for the server to comeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " !HF_TOKEN={HF_TOKEN} python vllm-fork/benchmarks/benchmark_serving.py --backend vllm --model \"{MODEL}\" --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000 --request-rate inf 2>&1 | tee vllm_benchmark_serving_client_stdout.log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had set VLLM_SKIP_WARMUP=\"true\" in the server setup phase, try restarting this notebook and retry this benchmark with VLLM_SKIP_WARMUP=\"false\" for better results.\n",
    "Run the following cells to stop any lingering processes before re-launching this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 <PID_OF_SERVER_FROM_CELL_NUMBER_5_ABOVE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
