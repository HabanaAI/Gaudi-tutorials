# Parameterize base image components
ARG DOCKER_URL=vault.habana.ai/gaudi-docker
ARG VERSION=1.21.1
ARG BASE_NAME=ubuntu24.04
ARG PT_VERSION=2.6.0
ARG REVISION=latest
ARG REPO_TYPE=habanalabs
ARG VLLM_HPU_EXTENSION_COMMIT=v1.21.0

FROM ${DOCKER_URL}/${VERSION}/${BASE_NAME}/${REPO_TYPE}/pytorch-installer-${PT_VERSION}:${REVISION}
# Parameterize commit/branch for vllm-fork checkout
ARG VLLM_FORK_COMMIT=v0.7.2+Gaudi-1.21.0

ENV OMPI_MCA_btl_vader_single_copy_mechanism=none

RUN apt update && \
    apt install -y gettext moreutils jq && \
    ln -sf /usr/bin/python3 /usr/bin/python

COPY 0001-Fix-number-of-blocks-for-pipeline-parallelism.-1305.patch /root
WORKDIR /root

# Install vllm-fork inside the container
ENV VLLM_TARGET_DEVICE=hpu
RUN git clone https://github.com/HabanaAI/vllm-fork.git && \
    cd vllm-fork && \
    git checkout ${VLLM_FORK_COMMIT} && \
    git apply ../0001-Fix-number-of-blocks-for-pipeline-parallelism.-1305.patch && \
    pip install -v -e .

# Install additional Python packages
RUN pip install datasets && \
    pip install pandas

# Copy utility scripts and configuration
RUN mkdir -p /root/scripts
COPY entrypoint.sh vllm_autocalc.py settings_vllm.csv template_vllm_server_*.sh varlist* perftest* dataset/* /root/scripts
RUN chmod +x /root/scripts/*.sh
WORKDIR /root/scripts

ARG VLLM_HPU_EXTENSION_COMMIT
RUN git clone https://github.com/HabanaAI/vllm-hpu-extension.git vllm_hpu_ext && \
    cd vllm_hpu_ext && \
    git checkout ${VLLM_HPU_EXTENSION_COMMIT}
    
COPY step-5-unify_measurements.py /root/scripts/vllm_hpu_ext/calibration

RUN mkdir -p /root/scripts/recipe_cache
RUN mkdir -p /root/scripts/measurement

# Set entrypoint script
ENTRYPOINT ["/root/scripts/entrypoint.sh"]
