{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5211b44",
   "metadata": {},
   "source": [
    "# Getting Started with vLLM on Intel¬Æ Gaudi¬Æ 2 AI Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc9749",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b884c73",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b190c",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use vLLM, a fast and efficient library for Large Language Model (LLM) inference and serving, with Intel¬Æ Gaudi¬Æ 2 AI Accelerators. vLLM-fork for Gaudi is an adaptation of the original vLLM project, optimized to leverage the power of Gaudi hardware.\n",
    "\n",
    "vLLM offers several advantages for LLM inference:\n",
    "\n",
    "1. High-throughput serving with state-of-the-art performance\n",
    "2. Efficient memory management using PagedAttention\n",
    "3. Continuous batching of incoming requests\n",
    "4. Optimized execution with custom Gaudi implementations for LLM operators\n",
    "5. Support for offline batched inference and nline inference via OpenAI-Compatible Server\n",
    "\n",
    "In this notebook, we'll explore how to set up and use vLLM on Gaudi hardware, demonstrating its capabilities for fast and efficient LLM inference.\n",
    "\n",
    "[Source: [vLLM-fork for Gaudi](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md), [vLLM Project](https://github.com/vllm-project/vllm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461135a1",
   "metadata": {},
   "source": [
    "## Installation and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38625800",
   "metadata": {},
   "source": [
    "> üìù **Note:** In this specific tutorial, we will be launching multiple containers in the style of micro-services to fully demonstrate the different use-cases for vLLM server. Please run this jupyter notebook directly on a bare-metal Gaudi machine instead of from within a regular Gaudi container (This setup is unlike the rest of the tutorials in the parent repo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864374ce",
   "metadata": {},
   "source": [
    "For Gaudi requirements and installation please refer to [Requirements and Installation](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#requirements-and-installation).\n",
    "\n",
    "The following cell clones the vLLM for Gaudi repository in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a96f224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vllm-fork' already exists and is not an empty directory.\n",
      "Already on 'habana_main'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your branch is up to date with 'origin/habana_main'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/HabanaAI/vllm-fork.git\n",
    "cd vllm-fork\n",
    "git checkout habana_main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b36d2",
   "metadata": {},
   "source": [
    "The next cell installs the vLLM server inside a Gaudi docker container.\n",
    "\n",
    "It is highly recommended to use the latest Docker image from Intel Gaudi vault. Refer to the [Run Docker Image](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#run-docker-image) section from [Intel Gaudi documentation](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html#pull-prebuilt-containers) for more details. For more information on installing vLLM for Gaudi refer to [Build And Install vLLM](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#build-and-install-vllm-fork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d9451f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/vllm_installation\" is already in use by container \"00bba6e9cbfe291097b0f81f80931f2e8004a483d3bbd4c7d361cce39566324e\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=0 \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_installation \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:1.20.0-543 \\\n",
    "  /bin/bash -c \"cd /app/vllm-fork/ && pip install -e . && pip install flask && echo $'\\n\\nInstallation completed successfully'\"  # This may take 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a99c1",
   "metadata": {},
   "source": [
    "## Getting Started with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d4e89",
   "metadata": {},
   "source": [
    "In this tutorial section below topics will be covered on how to setup and run on Intel Gaudi\n",
    "- Check vLLM installation\n",
    "- Prerequistes \n",
    "- Run vLLM for Offline batched Inference\n",
    "- Deploy vLLM via Flask \n",
    "- Deploy vLLM for Online Inference via OpenAI-Compatible Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12005bae",
   "metadata": {},
   "source": [
    "### Check vLLM installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcb9407",
   "metadata": {},
   "source": [
    "Check logs from the docker container to see if vLLM and flask have been installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a91c3",
   "metadata": {},
   "source": [
    "Check the logs to verify the message - `Installation completed successfully`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc806a",
   "metadata": {},
   "source": [
    "> üìù **Note:** Please wait for a few minutes before running the cell below. The cell may have to be run multiple times to confirm installation as the log updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed1b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///app/vllm-fork\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting vllm-hpu-extension@ git+https://github.com/HabanaAI/vllm-hpu-extension.git@3fd0250 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Cloning https://github.com/HabanaAI/vllm-hpu-extension.git (to revision 3fd0250) to /tmp/pip-install-ex1093kg/vllm-hpu-extension_41da862d4d414cf2bc2ba7dde18b56bd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/vllm-hpu-extension.git /tmp/pip-install-ex1093kg/vllm-hpu-extension_41da862d4d414cf2bc2ba7dde18b56bd\n",
      "  WARNING: Did not find branch or tag '3fd0250', assuming revision or ref.\n",
      "  Running command git checkout -q 3fd0250\n",
      "  Resolved https://github.com/HabanaAI/vllm-hpu-extension.git to commit 3fd0250\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (6.0.0)\n",
      "Collecting sentencepiece (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (4.67.1)\n",
      "Collecting blake3 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (9.0.0)\n",
      "Collecting transformers<4.49,>=4.48.2 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tokenizers>=0.19.1 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (3.20.3)\n",
      "Collecting fastapi!=0.113.*,!=0.114.0,>=0.107.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (3.11.12)\n",
      "Collecting openai>=1.52.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading openai-1.65.3-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting pydantic>=2.9 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting prometheus_client>=0.18.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading prometheus_client-0.21.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pillow (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lm-format-enforcer<0.11,>=0.10.9 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading lm_format_enforcer-0.10.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting outlines==0.1.11 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading outlines-0.1.11-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting lark==1.2.2 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting xgrammar==0.1.11 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading xgrammar-0.1.11-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (4.12.2)\n",
      "Collecting filelock>=3.16.1 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting partial-json-parser (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting pyzmq (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pyzmq-26.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting msgspec (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.10.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading gguf-0.10.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting importlib_metadata (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting mistral_common>=1.5.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading mistral_common-1.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (6.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (0.8.1)\n",
      "Collecting compressed-tensors==0.9.1 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading compressed_tensors-0.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting depyf==0.18.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading depyf-0.18.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting cloudpickle (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting ray (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading ray-2.43.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting triton==3.1.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.0.1)\n",
      "Collecting tabulate (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: setuptools>=61 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (75.1.0)\n",
      "Collecting setuptools-scm>=8 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading setuptools_scm-8.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from compressed-tensors==0.9.1->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.6.0+hpu.1.20.0.543.git4952fce)\n",
      "Collecting astor (from depyf==0.18.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dill (from depyf==0.18.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting interegular (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (3.1.4)\n",
      "Collecting nest_asyncio (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting referencing (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting jsonschema (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting airportsdata (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading airportsdata-20250224-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting outlines_core==0.1.26 (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (from xgrammar==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.10.4)\n",
      "Collecting pytest (from xgrammar==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting httpx>=0.23.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jinja2 (from outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer<0.11,>=0.10.9->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (24.2)\n",
      "Collecting numpy<2.0.0 (from vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=1.52.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.52.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.52.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.9->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2024.8.30)\n",
      "Collecting tomli>=1 (from setuptools-scm>=8->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.6.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2023.5.5)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.19.1->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<4.49,>=4.48.2->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.18.3)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2025.1)\n",
      "Collecting click>=7.0 (from ray->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai>=1.52.0->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\n",
      "Collecting httpcore==1.* (from httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (2.1.5)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->outlines==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading rpds_py-0.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors==0.9.1->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->compressed-tensors==0.9.1->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.7.0->compressed-tensors==0.9.1->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118) (1.3.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading websockets-15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting iniconfig (from pytest->xgrammar==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->xgrammar==0.1.11->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting rich>=13.7.1 (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]!=0.113.*,!=0.114.0,>=0.107.0; python_version >= \"3.9\"->vllm==0.6.3.dev2480+gf78aeb9da.gaudi118)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading compressed_tensors-0.9.1-py3-none-any.whl (96 kB)\n",
      "Downloading depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "Downloading outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 209.5/209.5 MB 221.3 MB/s eta 0:00:00\n",
      "Downloading xgrammar-0.1.11-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (396 kB)\n",
      "Downloading outlines_core-0.1.26-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "Downloading mistral_common-1.5.3-py3-none-any.whl (6.5 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.5/6.5 MB 274.7 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18.2/18.2 MB 553.6 MB/s eta 0:00:00\n",
      "Downloading openai-1.65.3-py3-none-any.whl (472 kB)\n",
      "Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 4.5/4.5 MB 254.6 MB/s eta 0:00:00\n",
      "Downloading prometheus_client-0.21.1-py3-none-any.whl (54 kB)\n",
      "Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 742.6 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.3/1.3 MB 415.6 MB/s eta 0:00:00\n",
      "Downloading setuptools_scm-8.2.0-py3-none-any.whl (43 kB)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.2/1.2 MB 560.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.0/3.0 MB 276.0 MB/s eta 0:00:00\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9.7/9.7 MB 288.3 MB/s eta 0:00:00\n",
      "Downloading blake3-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading pyzmq-26.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (874 kB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 874.2/874.2 kB 703.0 MB/s eta 0:00:00\n",
      "Downloading ray-2.43.0-cp310-cp310-manylinux2014_x86_64.whl (67.6 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 67.6/67.6 MB 198.2 MB/s eta 0:00:00\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Downloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.0/50.0 MB 237.0 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 913.7/913.7 kB 739.6 MB/s eta 0:00:00\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.3/6.3 MB 530.4 MB/s eta 0:00:00\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
      "Downloading rpds_py-0.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (386 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.8/3.8 MB 482.4 MB/s eta 0:00:00\n",
      "Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "Downloading websockets-15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (180 kB)\n",
      "Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.2/1.2 MB 733.3 MB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: vllm, vllm-hpu-extension\n",
      "  Building editable for vllm (pyproject.toml): started\n",
      "  Building editable for vllm (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for vllm: filename=vllm-0.6.3.dev2480+gf78aeb9da.gaudi118-0.editable-py3-none-any.whl size=12912 sha256=1c0e3006e4fcfc14e2b3b90d6d8f8ef7805f928620e021d27c7945948ef62dde\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i28wgzua/wheels/6a/9b/be/eea2c9d67a548ea071c4aa4238f27665d0826fb0a171ee8aec\n",
      "  Building wheel for vllm-hpu-extension (pyproject.toml): started\n",
      "  Building wheel for vllm-hpu-extension (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for vllm-hpu-extension: filename=vllm_hpu_extension-0.1.dev85+g3fd0250-py3-none-any.whl size=28800 sha256=510077e61ab2db0241626c735f6b8e159db3db32999e0aa5416cb26f17665117\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-i28wgzua/wheels/9a/51/b3/2d19161ce8fe5b486277e24d2e398f8ab99c9fcae072537314\n",
      "Successfully built vllm vllm-hpu-extension\n",
      "Installing collected packages: sentencepiece, blake3, zipp, websockets, vllm-hpu-extension, uvloop, tomli, tabulate, sniffio, shellingham, safetensors, rpds-py, pyzmq, python-multipart, python-dotenv, pygments, pydantic-core, pycountry, prometheus_client, pluggy, pillow, partial-json-parser, numpy, nest_asyncio, msgspec, msgpack, mdurl, lark, jiter, jinja2, interegular, iniconfig, httptools, h11, filelock, exceptiongroup, dnspython, distro, diskcache, dill, cloudpickle, click, astor, annotated-types, airportsdata, uvicorn, triton, tiktoken, setuptools-scm, referencing, pytest, pydantic, opencv-python-headless, markdown-it-py, importlib_metadata, huggingface-hub, httpcore, gguf, email-validator, depyf, anyio, watchfiles, tokenizers, starlette, rich, lm-format-enforcer, jsonschema-specifications, httpx, typer, transformers, rich-toolkit, prometheus-fastapi-instrumentator, openai, jsonschema, fastapi, xgrammar, ray, outlines_core, mistral_common, fastapi-cli, compressed-tensors, outlines, vllm\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.13\n",
      "    Uninstalling pydantic-1.10.13:\n",
      "      Successfully uninstalled pydantic-1.10.13\n",
      "Successfully installed airportsdata-20250224 annotated-types-0.7.0 anyio-4.8.0 astor-0.8.1 blake3-1.0.4 click-8.1.8 cloudpickle-3.1.1 compressed-tensors-0.9.1 depyf-0.18.0 dill-0.3.9 diskcache-5.6.3 distro-1.9.0 dnspython-2.7.0 email-validator-2.2.0 exceptiongroup-1.2.2 fastapi-0.115.11 fastapi-cli-0.0.7 filelock-3.17.0 gguf-0.10.0 h11-0.14.0 httpcore-1.0.7 httptools-0.6.4 httpx-0.28.1 huggingface-hub-0.29.2 importlib_metadata-8.6.1 iniconfig-2.0.0 interegular-0.3.3 jinja2-3.1.5 jiter-0.8.2 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 lark-1.2.2 lm-format-enforcer-0.10.11 markdown-it-py-3.0.0 mdurl-0.1.2 mistral_common-1.5.3 msgpack-1.1.0 msgspec-0.19.0 nest_asyncio-1.6.0 numpy-1.26.4 openai-1.65.3 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 pillow-11.1.0 pluggy-1.5.0 prometheus-fastapi-instrumentator-7.0.2 prometheus_client-0.21.1 pycountry-24.6.1 pydantic-2.10.6 pydantic-core-2.27.2 pygments-2.19.1 pytest-8.3.5 python-dotenv-1.0.1 python-multipart-0.0.20 pyzmq-26.2.1 ray-2.43.0 referencing-0.36.2 rich-13.9.4 rich-toolkit-0.13.2 rpds-py-0.23.1 safetensors-0.5.3 sentencepiece-0.2.0 setuptools-scm-8.2.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.46.0 tabulate-0.9.0 tiktoken-0.9.0 tokenizers-0.21.0 tomli-2.2.1 transformers-4.48.3 triton-3.1.0 typer-0.15.2 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.6.3.dev2480+gf78aeb9da.gaudi118 vllm-hpu-extension-0.1.dev85+g3fd0250 watchfiles-1.0.4 websockets-15.0 xgrammar-0.1.11 zipp-3.21.0\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "neural-compressor-pt 3.3 requires numpy==1.23.5; python_version < \"3.12\", but you have numpy 1.26.4 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "Collecting flask\n",
      "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.5)\n",
      "Collecting itsdangerous>=2.2 (from flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.8)\n",
      "Collecting blinker>=1.9 (from flask)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: itsdangerous, blinker, flask\n",
      "Successfully installed blinker-1.9.0 flask-3.1.0 itsdangerous-2.2.0\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "\n",
      "\n",
      "Installation completed successfully\n"
     ]
    }
   ],
   "source": [
    "!docker logs vllm_installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9586397d",
   "metadata": {},
   "source": [
    "Save the state of the container with vLLM and Flask installed, let's add a `tag` to the image name called `vllm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74dd45ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sha256:f6ee7bb556a3c6d6c96a0de5982687c5d1c0dccf661605b6e23632b5e48d3719\n"
     ]
    }
   ],
   "source": [
    "!docker commit vllm_installation vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef72d3f",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26d9996",
   "metadata": {},
   "source": [
    "A Hugging Face token need to be set as an environment variable. This step is required if gated models like llama2 from Hugging Face are being used. The token in the code should be replaced with a valid personal token if gated models are accessed.\n",
    "\n",
    "The tokens can be accessed from this site - [Huggingface Tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de61d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"YOUR_ACCESS_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9220df",
   "metadata": {},
   "source": [
    "### Run vLLM for Offline batched Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e7eb0",
   "metadata": {},
   "source": [
    "Here is an example of vLLM's offline batched inference capabilities using Llama2 7B on Intel Gaudi. The below script utlizes vLLM in offline mode and processes the input prompts in a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114f35b",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/python\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "# initialize\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=30)\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", enforce_eager=True)\n",
    "\n",
    "# perform the inference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# print outputs\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce5037",
   "metadata": {},
   "source": [
    "Execute the script called `vllm_batch_inference.py` shown above, inside the container that has vLLM with Gaudi support installed to perform batched inference.\n",
    "\n",
    "The script can also be run interactively using the flag `-it` instead of `-d` in the following cell. This way, the inference can be done with different inputs and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139d9f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/vllm_batch_inference\" is already in use by container \"1dc986bc825488368b6cc34afb73a0ba1332542c31bbb6e8565e5a038e5fe162\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=0 \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_batch_inference \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm \\\n",
    "  python /app/scripts/vllm_batch_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1978d5",
   "metadata": {},
   "source": [
    "> **Note:** This may take 5-10 minutes if the model weights have not been downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9eb76",
   "metadata": {},
   "source": [
    "Output is printed for each of the input prompts in the script. Next, stop the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e93b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/app/scripts/vllm_batch_inference.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!docker logs vllm_batch_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c84ab83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm_batch_inference\n"
     ]
    }
   ],
   "source": [
    "!docker stop vllm_batch_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5936d7",
   "metadata": {},
   "source": [
    "### Deploy vLLM via Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc69fa",
   "metadata": {},
   "source": [
    "While the example shown above is great for offline tests, a production setup calls for a more robust solution. Here is an example on how to use a web based framework like Flask, vLLM and Gaudi to serve the model via a REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b258274",
   "metadata": {},
   "source": [
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "app = Flask(__name__)\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", enforce_eager=True)\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.get_json()\n",
    "    prompts = data.get('prompts', [])\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    # Prepare the outputs.\n",
    "    results = []\n",
    "\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "\n",
    "    return jsonify(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200e3f8",
   "metadata": {},
   "source": [
    "Above is a script called `vllm_flaskapp.py` which creates an endpoint called `/generate` through which the text generation requests are served.\n",
    "\n",
    "> **Note:** Please change the port in the script in the event that it is occupied\n",
    "\n",
    "Run the script in the container to start the flask server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12450f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/vllm_flask_server\" is already in use by container \"7b92665a6a0df89cf82296668bed831aaeb549608e1b41d89e4ebb78eba47001\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n"
     ]
    }
   ],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=0 \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_flask_server \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm \\\n",
    "  python /app/scripts/vllm_flaskapp.py  # This step takes 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb592f",
   "metadata": {},
   "source": [
    "The server takes a while to get set up. Please check `docker` logs for `vllm_flask_server` using the command below to check for status. Once the server is set up successfully, the following output can be seen in the logs:\n",
    "\n",
    "```\n",
    "* Running on all addresses (0.0.0.0)\n",
    " * Running on http://127.0.0.1:5000\n",
    " * Running on http://198.175.88.246:5000\n",
    "Press CTRL+C to quit\n",
    "```\n",
    "Please make note of the port that the application is serving at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0892e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/app/scripts/vllm_flaskapp.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# check logs to see if server has started successfully\n",
    "\n",
    "!docker logs vllm_flask_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a88c3e",
   "metadata": {},
   "source": [
    "A POST request is sent to the Flask server, and the response is printed. Please change the port according to the logs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125b270a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f61f025c040>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f61f025c040>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f61f025c040>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://localhost:5000/generate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTell me in one sentence what Berlin is famous for\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f61f025c040>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "# Example of sending a POST request to the Flask server\n",
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.post('http://localhost:5000/generate', json={'prompts': ['Tell me in one sentence what Berlin is famous for']})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87659819",
   "metadata": {},
   "source": [
    "Stop the server once the requests have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f047c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker stop vllm_flask_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59701073",
   "metadata": {},
   "source": [
    "### Deploy vLLM for Online Inference via OpenAI-Compatible Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c5a54",
   "metadata": {},
   "source": [
    "The flask REST API above has it's limitations in terms of handling multiple users, lacks built-in authentication, and requires custom documentation. That's where vLLM's serving capabilities can be utililized for production grade deployment at scale right out of the box.\n",
    "In this section, we will use vllm's built-in capapbilties to deploy a server and use OpenAI client to make requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd9525",
   "metadata": {},
   "source": [
    "The command used to run the vLLM api server is `python -m vllm.entrypoints.openai.api_server --enforce-eager --model=meta-llama/Llama-2-7b-chat-hf` which is run inside the container with vLLM installed.\n",
    "You can specify the address with `--host` and `--port` arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9405da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=0 \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_api_server \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm \\\n",
    "  python -m vllm.entrypoints.openai.api_server --enforce-eager --model=meta-llama/Llama-2-7b-chat-hf --port 8000 # Takes 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d505b08",
   "metadata": {},
   "source": [
    "The server takes a while to get set up. Please check `docker` logs for `vllm_api_server` using the command below to check for status. Once the server is set up successfully, the following output can be seen in the logs:\n",
    "\n",
    "```\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "INFO 07-22 22:19:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3919da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check logs to see if server has started successfully\n",
    "\n",
    "!docker logs vllm_api_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bdc62",
   "metadata": {},
   "source": [
    "Let's use the `requests` library to interact with our vLLM API server. This approach allows us to send HTTP requests directly from our Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ac61a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# send request to the vLLM server using requests\n",
    "VLLM_HOST = \"http://0.0.0.0:8000\"\n",
    "url = f\"{VLLM_HOST}/v1/completions\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"prompt\": \"Tell me in one sentence what Tokyo is famous for\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb04dd",
   "metadata": {},
   "source": [
    "### OpenAI API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e31f5",
   "metadata": {},
   "source": [
    "Install openai client to make requests to the vLLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98b8e1",
   "metadata": {},
   "source": [
    "vLLM can be deployed as a server that implements the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. By default, it starts the server at `http://0.0.0.0:8000`. You can specify the address with --`host` and `--port` arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e5145",
   "metadata": {},
   "source": [
    "OpenAI client to stream requests can be used via `Completions` and `Chat` API.\n",
    "\n",
    "The Completions API from OpenAI is designed for a wide range of text generation tasks, offering significant control over the output through various parameters, making it suitable for applications like content creation and code completion\n",
    "\n",
    "In contrast, the Chat API is specifically optimized for conversational AI, facilitating smoother and more contextually aware dialogues. This API is better suited for chatbots and interactive applications where maintaining context and managing follow-up queries is crucial, ensuring more human-like interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc16ba0c",
   "metadata": {},
   "source": [
    "Completions API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4151cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# we haven't configured authentication, we pass a dummy value\n",
    "openai_api_key = \"EMPTY\"\n",
    "# modify this value to match your host, remember to add /v1 at the end\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                      prompt=\"Tell me in one sentence what Paris is famous for\",\n",
    "                                      max_tokens=50)\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf884e8d",
   "metadata": {},
   "source": [
    "Chat API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24367d04",
   "metadata": {},
   "source": [
    "Please use an instruction-tuned model for the chat API such as `meta-llama/Llama-2-7b-chat-hf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me in one sentence what New York City is famous for\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d356859",
   "metadata": {},
   "source": [
    "### Chat Application using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974db0bd",
   "metadata": {},
   "source": [
    "Optionally, connect to the vLLM server through a Gradio chatbot to see it in action. The same endpoint that was set up in the previous sections is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5de802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(message, history):\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "  \n",
    "    response = client.chat.completions.create(model='meta-llama/Llama-2-7b-chat-hf',\n",
    "    messages= history_openai_format,\n",
    "    temperature=1.0,\n",
    "    stream=True)\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "              partial_message = partial_message + chunk.choices[0].delta.content\n",
    "              yield partial_message\n",
    "\n",
    "gr.ChatInterface(predict).launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24298e0d",
   "metadata": {},
   "source": [
    "Stop the server once the requests have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2268f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker stop vllm_api_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292ba357",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fd7921",
   "metadata": {},
   "source": [
    "1. If the following error message is encountered:\n",
    "   ```\n",
    "   RuntimeError: synStatus=8 [Device not found] Device acquire failed.\n",
    "   ```\n",
    "   Please stop the respective docker container to free up Gaudi memory\n",
    "\n",
    "2. `docker ps` can be a great command to see which containers have been up and for how long\n",
    "\n",
    "3. If a container name is already in use by a container, capture the container id in the error message and run `docker rm CONTAINER_ID`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
