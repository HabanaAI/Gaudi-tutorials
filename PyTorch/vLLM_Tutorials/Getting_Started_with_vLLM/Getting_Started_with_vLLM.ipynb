{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b8277e",
   "metadata": {},
   "source": [
    "# Getting Started with vLLM on Intel® Gaudi® 2 and Intel® Gaudi® 3 AI Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604d3e9",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28b95c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6c9a5a",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use vLLM, a fast and efficient library for Large Language Model (LLM) inference and serving, with Intel® Gaudi® 2 and Intel® Gaudi® 3 AI Accelerators. vLLM-fork for Gaudi is an adaptation of the original vLLM project, optimized to leverage the power of Gaudi hardware.\n",
    "\n",
    "vLLM offers several advantages for LLM inference:\n",
    "\n",
    "1. High-throughput serving with state-of-the-art performance\n",
    "2. Efficient memory management using PagedAttention\n",
    "3. Continuous batching of incoming requests\n",
    "4. Optimized execution with custom Gaudi implementations for LLM operators\n",
    "5. Support for offline batched inference and nline inference via OpenAI-Compatible Server\n",
    "\n",
    "In this notebook, we'll explore how to set up and use vLLM on Gaudi hardware, demonstrating its capabilities for fast and efficient LLM inference.\n",
    "\n",
    "[Source: [vLLM-fork for Gaudi](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md), [vLLM Project](https://github.com/vllm-project/vllm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281c2ac",
   "metadata": {},
   "source": [
    "## Installation and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bf3e08",
   "metadata": {},
   "source": [
    "> 📝 **Note:** In this specific tutorial, we will be launching multiple containers in the style of micro-services to fully demonstrate the different use-cases for vLLM server. Please run this jupyter notebook directly on a bare-metal Gaudi machine instead of from within a regular Gaudi container (This setup is unlike the rest of the tutorials in the parent repo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bdde24",
   "metadata": {},
   "source": [
    "For Gaudi requirements and installation please refer to [Requirements and Installation](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#requirements-and-installation).\n",
    "\n",
    "The following cell clones the vLLM for Gaudi repository in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7edcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/HabanaAI/vllm-fork.git\n",
    "cd vllm-fork\n",
    "git checkout habana_main",
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be219edb",
   "metadata": {},
   "source": [
    "The next cell installs the vLLM server inside a Gaudi docker container.\n",
    "\n",
    "It is highly recommended to use the latest Docker image from Intel Gaudi vault. Refer to the [Run Docker Image](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#run-docker-image) section from [Intel Gaudi documentation](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html#pull-prebuilt-containers) for more details. For more information on installing vLLM for Gaudi refer to [Build And Install vLLM](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#build-and-install-vllm-fork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=0 \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_installation \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:1.20.0-543 \\\n",
    "  /bin/bash -c \"cd /app/vllm-fork/ && pip install -e . && pip install flask && echo $'\\n\\nInstallation completed successfully'\"  # This may take 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c957dc",
   "metadata": {},
   "source": [
    "## Getting Started with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3035efd",
   "metadata": {},
   "source": [
    "In this tutorial section below topics will be covered on how to setup and run on Intel Gaudi\n",
    "- Check vLLM installation\n",
    "- Prerequistes \n",
    "- Run vLLM for Offline batched Inference\n",
    "- Deploy vLLM via Flask \n",
    "- Deploy vLLM for Online Inference via OpenAI-Compatible Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf33ce9",
   "metadata": {},
   "source": [
    "### Check vLLM installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d80fde",
   "metadata": {},
   "source": [
    "Check logs from the docker container to see if vLLM and flask have been installed successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825901a",
   "metadata": {},
   "source": [
    "Check the logs to verify the message - `Installation completed successfully`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84590a72",
   "metadata": {},
   "source": [
    "> 📝 **Note:** Please wait for a few minutes before running the cell below. The cell may have to be run multiple times to confirm installation as the log updates."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9d9b743",
   "metadata": {},
   "source": [
    "!docker logs vllm_installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf9ad55",
   "metadata": {},
   "source": [
    "Save the state of the container with vLLM and Flask installed, let's add a `tag` to the image name called `vllm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17676d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker commit vllm_installation vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d3c41",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b759b4",
   "metadata": {},
   "source": [
    "A Hugging Face token need to be set as an environment variable. This step is required if gated models like llama2 from Hugging Face are being used. The token in the code should be replaced with a valid personal token if gated models are accessed.\n",
    "\n",
    "The tokens can be accessed from this site - [Huggingface Tokens](https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"YOUR_ACCESS_TOKEN\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45fe2d9",
   "metadata": {},
   "source": [
    "### Run vLLM for Offline batched Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec68e3b",
   "metadata": {},
   "source": [
    "Here is an example of vLLM's offline batched inference capabilities using Llama2 7B on Intel Gaudi. The below script utlizes vLLM in offline mode and processes the input prompts in a single batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55d679",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/python\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The president of the United States is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "\n",
    "# initialize\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=30)\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", enforce_eager=True)\n",
    "\n",
    "# perform the inference\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# print outputs\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a7cf0",
   "metadata": {},
   "source": [
    "Execute the script called `vllm_batch_inference.py` shown above, inside the container that has vLLM with Gaudi support installed to perform batched inference.\n",
    "\n",
    "The script can also be run interactively using the flag `-it` instead of `-d` in the following cell. This way, the inference can be done with different inputs and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bef5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=all \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_batch_inference \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm \\\n",
    "  python /app/scripts/vllm_batch_inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00acbc2e",
   "metadata": {},
   "source": [
    "> **Note:** This may take 5-10 minutes if the model weights have not been downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5128a",
   "metadata": {},
   "source": [
    "Output is printed for each of the input prompts in the script. Next, stop the docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs vllm_batch_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop vllm_batch_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c766a8",
   "metadata": {},
   "source": [
    "### Deploy vLLM via Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fc0aac",
   "metadata": {},
   "source": [
    "While the example shown above is great for offline tests, a production setup calls for a more robust solution. Here is an example on how to use a web based framework like Flask, vLLM and Gaudi to serve the model via a REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabb8d2",
   "metadata": {},
   "source": [
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "app = Flask(__name__)\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\", enforce_eager=True)\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate():\n",
    "    data = request.get_json()\n",
    "    prompts = data.get('prompts', [])\n",
    "\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    # Prepare the outputs.\n",
    "    results = []\n",
    "\n",
    "    for output in outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'generated_text': generated_text\n",
    "        })\n",
    "\n",
    "    return jsonify(results)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f6cb9",
   "metadata": {},
   "source": [
    "Above is a script called `vllm_flaskapp.py` which creates an endpoint called `/generate` through which the text generation requests are served.\n",
    "\n",
    "> **Note:** Please change the port in the script in the event that it is occupied\n",
    "\n",
    "Run the script in the container to start the flask server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=all \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_flask_server \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm \\\n",
    "  python /app/scripts/vllm_flaskapp.py  # This step takes 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdc583",
   "metadata": {},
   "source": [
    "The server takes a while to get set up. Please check `docker` logs for `vllm_flask_server` using the command below to check for status. Once the server is set up successfully, the following output can be seen in the logs:\n",
    "\n",
    "```\n",
    "* Running on all addresses (0.0.0.0)\n",
    " * Running on http://127.0.0.1:5000\n",
    " * Running on http://198.175.88.246:5000\n",
    "Press CTRL+C to quit\n",
    "```\n",
    "Please make note of the port that the application is serving at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check logs to see if server has started successfully\n",
    "\n",
    "!docker logs vllm_flask_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a35d7",
   "metadata": {},
   "source": [
    "A POST request is sent to the Flask server, and the response is printed. Please change the port according to the logs above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of sending a POST request to the Flask server\n",
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.post('http://localhost:5000/generate', json={'prompts': ['Tell me in one sentence what Berlin is famous for']})\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176198fb",
   "metadata": {},
   "source": [
    "Stop the server once the requests have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df82e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker stop vllm_flask_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e363a",
   "metadata": {},
   "source": [
    "### Deploy vLLM for Online Inference via OpenAI-Compatible Server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3020663",
   "metadata": {},
   "source": [
    "The flask REST API above has it's limitations in terms of handling multiple users, lacks built-in authentication, and requires custom documentation. That's where vLLM's serving capabilities can be utililized for production grade deployment at scale right out of the box.\n",
    "In this section, we will use vllm's built-in capapbilties to deploy a server and use OpenAI client to make requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a3aa2",
   "metadata": {},
   "source": [
    "The command used to run the vLLM api server is `python -m vllm.entrypoints.openai.api_server --enforce-eager --model=meta-llama/Llama-2-7b-chat-hf` which is run inside the container with vLLM installed.\n",
    "You can specify the address with `--host` and `--port` arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7f05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d \\\n",
    "  --runtime=habana \\\n",
    "  -v $(pwd):/app \\\n",
    "  -e HABANA_VISIBLE_DEVICES=all \\\n",
    "  -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "  -e HF_TOKEN=$HF_TOKEN \\\n",
    "  --cap-add=sys_nice \\\n",
    "  --net=host \\\n",
    "  --ipc=host \\\n",
    "  --name=vllm_api_server \\\n",
    "  vault.habana.ai/gaudi-docker/1.20.0/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:vllm \\\n",
    "  python -m vllm.entrypoints.openai.api_server --enforce-eager --model=meta-llama/Llama-2-7b-chat-hf --port 8000 # Takes 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e1ca9",
   "metadata": {},
   "source": [
    "The server takes a while to get set up. Please check `docker` logs for `vllm_api_server` using the command below to check for status. Once the server is set up successfully, the following output can be seen in the logs:\n",
    "\n",
    "```\n",
    "INFO:     Started server process [1]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "INFO 07-22 22:19:07 metrics.py:334] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ad0bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check logs to see if server has started successfully\n",
    "\n",
    "!docker logs vllm_api_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58744bca",
   "metadata": {},
   "source": [
    "Let's use the `requests` library to interact with our vLLM API server. This approach allows us to send HTTP requests directly from our Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b45382",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# send request to the vLLM server using requests\n",
    "VLLM_HOST = \"http://0.0.0.0:8000\"\n",
    "url = f\"{VLLM_HOST}/v1/completions\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"prompt\": \"Tell me in one sentence what Tokyo is famous for\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33505852",
   "metadata": {},
   "source": [
    "### OpenAI API Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7903b25",
   "metadata": {},
   "source": [
    "Install openai client to make requests to the vLLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326acdfa",
   "metadata": {},
   "source": [
    "vLLM can be deployed as a server that implements the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. By default, it starts the server at `http://0.0.0.0:8000`. You can specify the address with --`host` and `--port` arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a088d40",
   "metadata": {},
   "source": [
    "OpenAI client to stream requests can be used via `Completions` and `Chat` API.\n",
    "\n",
    "The Completions API from OpenAI is designed for a wide range of text generation tasks, offering significant control over the output through various parameters, making it suitable for applications like content creation and code completion\n",
    "\n",
    "In contrast, the Chat API is specifically optimized for conversational AI, facilitating smoother and more contextually aware dialogues. This API is better suited for chatbots and interactive applications where maintaining context and managing follow-up queries is crucial, ensuring more human-like interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460296ca",
   "metadata": {},
   "source": [
    "Completions API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8967870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# we haven't configured authentication, we pass a dummy value\n",
    "openai_api_key = \"EMPTY\"\n",
    "# modify this value to match your host, remember to add /v1 at the end\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "completion = client.completions.create(model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "                                      prompt=\"Tell me in one sentence what Paris is famous for\",\n",
    "                                      max_tokens=50)\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4c953",
   "metadata": {},
   "source": [
    "Chat API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5e28e7",
   "metadata": {},
   "source": [
    "Please use an instruction-tuned model for the chat API such as `meta-llama/Llama-2-7b-chat-hf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://0.0.0.0:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me in one sentence what New York City is famous for\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ead664",
   "metadata": {},
   "source": [
    "### Chat Application using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33d4d7",
   "metadata": {},
   "source": [
    "Optionally, connect to the vLLM server through a Gradio chatbot to see it in action. The same endpoint that was set up in the previous sections is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(message, history):\n",
    "    history_openai_format = []\n",
    "    for human, assistant in history:\n",
    "        history_openai_format.append({\"role\": \"user\", \"content\": human })\n",
    "        history_openai_format.append({\"role\": \"assistant\", \"content\":assistant})\n",
    "    history_openai_format.append({\"role\": \"user\", \"content\": message})\n",
    "  \n",
    "    response = client.chat.completions.create(model='meta-llama/Llama-2-7b-chat-hf',\n",
    "    messages= history_openai_format,\n",
    "    temperature=1.0,\n",
    "    stream=True)\n",
    "\n",
    "    partial_message = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "              partial_message = partial_message + chunk.choices[0].delta.content\n",
    "              yield partial_message\n",
    "\n",
    "gr.ChatInterface(predict).launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9336d",
   "metadata": {},
   "source": [
    "Stop the server once the requests have been made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071e2c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker stop vllm_api_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7027321",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af458c80",
   "metadata": {},
   "source": [
    "1. If the following error message is encountered:\n",
    "   ```\n",
    "   RuntimeError: synStatus=8 [Device not found] Device acquire failed.\n",
    "   ```\n",
    "   Please stop the respective docker container to free up Gaudi memory\n",
    "\n",
    "2. `docker ps` can be a great command to see which containers have been up and for how long\n",
    "\n",
    "3. If a container name is already in use by a container, capture the container id in the error message and run `docker rm CONTAINER_ID`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
