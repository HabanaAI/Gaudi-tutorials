{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "## BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# ResNet50 for PyTorch with GPU Migration\n",
    "\n",
    "In this notebook we will demonstrate ResNet50 model which is based on open source implementation of ResNet50. It has been enabled using an experimental feature called GPU migration toolkit and it can be trained using Pytorch on 8 HPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Clone the Model-References repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
      "fatal: destination path 'Model-References' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bace79a",
   "metadata": {},
   "source": [
    "#### Set the ENV variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41f0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=~/Gaudi-Tutorials/PyTorch/Single_card_tutorials/Model-Migration/Model-References:/usr/lib/habanalabs/:~\n",
      "env: PYTHON=/usr/bin/python3.8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONPATH']='~/Model-References,/usr/lib/habanalabs/'\n",
    "os.environ['DATA_LOADER_AEON_LIB_PATH'] = '/usr/lib/habanalabs/libaeon.so'\n",
    "os.environ['GC_KERNEL_PATH'] = '/usr/lib/habanalabs/libtpc_kernels.so'\n",
    "os.environ['HABANA_PLUGINS_LIB_PATH'] = '/opt/habanalabs/habana_plugins'\n",
    "os.environ['HABANA_SCAL_BIN_PATH'] = '/opt/habanalabs/engines_fw'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4238fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path=['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '~/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages',\"~/.local/bin\",\"/usr/bin/habanatools\",\"/usr/local/sbin\",\"/usr/local/bin\",\"/usr/sbin\",\"/usr/bin\",\"/sbin\",\"/bin\",\"/usr/games\",\"/usr/local/games\",\"/snap/bin\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Naviagte to the model to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2212748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset\n",
    "ImageNet 2012 dataset needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee70dc",
   "metadata": {},
   "source": [
    "#### Import GPU Migration Toolkit package and Habana Torch Library\n",
    "Look into train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c905c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4\timport habana_frameworks.torch.gpu_migration\n",
      "     5\timport datetime\n",
      "     6\timport os\n",
      "     7\timport time\n",
      "     8\timport warnings\n",
      "     9\t\n",
      "    10\timport presets\n",
      "    11\timport torch\n",
      "    12\timport torch.utils.data\n",
      "    13\timport torchvision\n",
      "    14\timport transforms\n",
      "    15\timport utils\n",
      "    16\timport habana_frameworks.torch.utils.experimental as htexp\n",
      "    17\tfrom sampler import RASampler\n",
      "    18\tfrom torch import nn\n",
      "    19\tfrom torch.utils.data.dataloader import default_collate\n",
      "    20\tfrom torchvision.transforms.functional import InterpolationMode\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 20 | tail -n 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed748205",
   "metadata": {},
   "source": [
    "#### Placing mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4e1aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    38\t        optimizer.zero_grad(set_to_none=True)\n",
      "    39\t        if scaler is not None:\n",
      "    40\t            scaler.scale(loss).backward()\n",
      "    41\t            htcore.mark_step()\n",
      "    42\t            if args.clip_grad_norm is not None:\n",
      "    43\t                # we should unscale the gradients of optimizer's assigned params if do gradient clipping\n",
      "    44\t                scaler.unscale_(optimizer)\n",
      "    45\t                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
      "    46\t            scaler.step(optimizer)\n",
      "    47\t            htcore.mark_step()\n",
      "    48\t            scaler.update()\n",
      "    49\t        else:\n",
      "    50\t            loss.backward()\n",
      "    51\t            htcore.mark_step()\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 51 | tail -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start multi-HPU training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-21/00-21-25/gpu_migration_8399.log\n",
      "[2024-03-21 00:21:25] /root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2024-03-21 00:21:27] /root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=1, rank=0, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-21 00:21:27] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-21 00:21:27] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "Namespace(use_torch_compile=False, data_path='/root/software/data/pytorch/imagenet/ILSVRC2012', model='resnet50', device='cuda', batch_size=256, epochs=1, dl_worker_type='HABANA', workers=8, opt='sgd', lr=0.1, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='custom_lr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=10, output_dir='.', resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=True, world_size=1, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, save_checkpoint=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
      "[2024-03-21 00:21:27] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py\", line 576, in <module>\n",
      "    main(args)\n",
      "  File \"/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py\", line 233, in main\n",
      "    dataset, dataset_test, train_sampler, test_sampler = load_data(train_dir, val_dir, args)\n",
      "  File \"/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py\", line 147, in load_data\n",
      "    dataset = torchvision.datasets.ImageFolder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 309, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 144, in __init__\n",
      "    classes, class_to_idx = self.find_classes(self.root)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 218, in find_classes\n",
      "    return find_classes(directory)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 40, in find_classes\n",
      "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/software/data/pytorch/imagenet/ILSVRC2012/train'\n",
      "[2024-03-21 00:21:32,018] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 8399) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 806, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-03-21_00:21:32\n",
      "  host      : hls2-srv01-demolab\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 8399)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-09 23:17:26] train.py:32\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] train.py:33\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/common.py:7\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec('torch_xla'))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/622]  eta: 2:06:32  lr: 0.1  img/s: 20.973556319305924  loss: 7.1048 (7.1048)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.3906)  time: 12.2059  data: 1.2269  max mem: 15838\n",
      "Epoch: [0]  [ 10/622]  eta: 0:31:50  lr: 0.1  img/s: 115.61183054130028  loss: 7.1048 (7.1195)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.7812)  time: 3.1221  data: 0.1968  max mem: 16966\n",
      "Epoch: [0]  [ 20/622]  eta: 0:16:40  lr: 0.1  img/s: 4432.052451134573  loss: 7.1133 (7.1174)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (0.9115)  time: 1.1344  data: 0.0488  max mem: 16966\n",
      "Epoch: [0]  [ 30/622]  eta: 0:11:16  lr: 0.1  img/s: 4516.90829523116  loss: 7.1133 (7.1285)  acc1: 0.3906 (0.4883)  acc5: 0.7812 (0.8789)  time: 0.0546  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [ 40/622]  eta: 0:08:30  lr: 0.1  img/s: 4853.195422435741  loss: 7.1133 (7.0867)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8594)  time: 0.0521  data: 0.0047  max mem: 16966\n",
      "Epoch: [0]  [ 50/622]  eta: 0:06:48  lr: 0.1  img/s: 5000.998223147521  loss: 7.1048 (7.0657)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8464)  time: 0.0493  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 60/622]  eta: 0:05:39  lr: 0.1  img/s: 5282.966572527873  loss: 7.1048 (7.0459)  acc1: 0.3906 (0.3348)  acc5: 0.7812 (0.7812)  time: 0.0472  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [ 70/622]  eta: 0:04:50  lr: 0.1  img/s: 4933.84042494532  loss: 6.9607 (7.0270)  acc1: 0.3906 (0.2930)  acc5: 0.7812 (0.7812)  time: 0.0475  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 80/622]  eta: 0:04:13  lr: 0.1  img/s: 4933.219318243425  loss: 6.9607 (7.0097)  acc1: 0.3906 (0.3038)  acc5: 0.7812 (0.7812)  time: 0.0492  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [ 90/622]  eta: 0:03:44  lr: 0.1  img/s: 5296.3007443733  loss: 6.9274 (6.9888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9766)  time: 0.0475  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [100/622]  eta: 0:03:20  lr: 0.1  img/s: 5273.366650950809  loss: 6.9274 (6.9741)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9233)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [110/622]  eta: 0:03:01  lr: 0.1  img/s: 5202.998810872521  loss: 6.9192 (6.9543)  acc1: 0.3906 (0.3581)  acc5: 0.7812 (1.0417)  time: 0.0462  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [120/622]  eta: 0:02:44  lr: 0.1  img/s: 5171.708403494103  loss: 6.9192 (6.9406)  acc1: 0.3906 (0.3606)  acc5: 0.7812 (1.1118)  time: 0.0467  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [130/622]  eta: 0:02:30  lr: 0.1  img/s: 5266.486614806604  loss: 6.8949 (6.9260)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.3393)  time: 0.0464  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [140/622]  eta: 0:02:20  lr: 0.1  img/s: 2805.1565217414022  loss: 6.8949 (6.9064)  acc1: 0.3906 (0.4167)  acc5: 0.7812 (1.4583)  time: 0.0673  data: 0.0086  max mem: 16966\n",
      "Epoch: [0]  [150/622]  eta: 0:02:09  lr: 0.1  img/s: 5234.665052013101  loss: 6.8713 (6.8888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.4893)  time: 0.0675  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [160/622]  eta: 0:02:00  lr: 0.1  img/s: 5222.205370535011  loss: 6.8713 (6.8671)  acc1: 0.3906 (0.4136)  acc5: 1.1719 (1.5165)  time: 0.0463  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [170/622]  eta: 0:01:52  lr: 0.1  img/s: 5223.231671485778  loss: 6.8270 (6.8482)  acc1: 0.3906 (0.4774)  acc5: 1.1719 (1.6059)  time: 0.0463  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [180/622]  eta: 0:01:44  lr: 0.1  img/s: 4716.0108590957225  loss: 6.8270 (6.8283)  acc1: 0.3906 (0.4729)  acc5: 1.1719 (1.6653)  time: 0.0489  data: 0.0041  max mem: 16966\n",
      "Epoch: [0]  [190/622]  eta: 0:01:38  lr: 0.1  img/s: 5201.287478013948  loss: 6.8005 (6.8103)  acc1: 0.3906 (0.5078)  acc5: 1.1719 (1.8164)  time: 0.0490  data: 0.0040  max mem: 16966\n",
      "Epoch: [0]  [200/622]  eta: 0:01:32  lr: 0.1  img/s: 5223.7449732474115  loss: 6.7767 (6.7917)  acc1: 0.3906 (0.5580)  acc5: 1.9531 (1.9903)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [210/622]  eta: 0:01:26  lr: 0.1  img/s: 5242.224721909762  loss: 6.7362 (6.7727)  acc1: 0.3906 (0.6037)  acc5: 1.9531 (2.1662)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [220/622]  eta: 0:01:21  lr: 0.1  img/s: 5272.3490329002  loss: 6.7361 (6.7572)  acc1: 0.3906 (0.6624)  acc5: 1.9531 (2.2418)  time: 0.0460  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [230/622]  eta: 0:01:16  lr: 0.1  img/s: 5257.62599791603  loss: 6.6319 (6.7398)  acc1: 0.3906 (0.6673)  acc5: 2.3438 (2.2786)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [240/622]  eta: 0:01:12  lr: 0.1  img/s: 5265.554278899224  loss: 6.6249 (6.7193)  acc1: 0.7812 (0.7188)  acc5: 2.7344 (2.3438)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [250/622]  eta: 0:01:08  lr: 0.1  img/s: 5245.37462640924  loss: 6.5275 (6.6944)  acc1: 0.7812 (0.7662)  acc5: 2.7344 (2.5090)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [260/622]  eta: 0:01:04  lr: 0.1  img/s: 5223.676357764838  loss: 6.5193 (6.6699)  acc1: 0.7812 (0.7668)  acc5: 3.1250 (2.5608)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [270/622]  eta: 0:01:01  lr: 0.1  img/s: 4976.946303265318  loss: 6.4692 (6.6476)  acc1: 0.7812 (0.7673)  acc5: 3.1250 (2.6925)  time: 0.0475  data: 0.0031  max mem: 16966\n",
      "Epoch: [0]  [280/622]  eta: 0:00:58  lr: 0.1  img/s: 4616.917909177062  loss: 6.4686 (6.6303)  acc1: 0.7812 (0.8082)  acc5: 3.1250 (2.8691)  time: 0.0506  data: 0.0069  max mem: 16966\n",
      "Epoch: [0]  [290/622]  eta: 0:00:54  lr: 0.1  img/s: 4638.213659670236  loss: 6.4193 (6.6123)  acc1: 0.7812 (0.8464)  acc5: 3.9062 (3.0990)  time: 0.0526  data: 0.0075  max mem: 16966\n",
      "Epoch: [0]  [300/622]  eta: 0:00:52  lr: 0.1  img/s: 4683.1702654478195  loss: 6.4160 (6.5903)  acc1: 0.7812 (0.9451)  acc5: 3.9062 (3.2006)  time: 0.0523  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [310/622]  eta: 0:00:49  lr: 0.1  img/s: 5126.348193291654  loss: 6.3754 (6.5707)  acc1: 1.1719 (1.0010)  acc5: 3.9062 (3.4302)  time: 0.0497  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [320/622]  eta: 0:00:46  lr: 0.1  img/s: 5302.313549730277  loss: 6.3376 (6.5510)  acc1: 1.5625 (1.0535)  acc5: 4.2969 (3.5748)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [330/622]  eta: 0:00:44  lr: 0.1  img/s: 5269.187344039455  loss: 6.2278 (6.5302)  acc1: 1.5625 (1.1029)  acc5: 4.6875 (3.8143)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [340/622]  eta: 0:00:41  lr: 0.1  img/s: 5273.364061093401  loss: 6.1463 (6.5111)  acc1: 1.5625 (1.1719)  acc5: 5.4688 (4.0290)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [350/622]  eta: 0:00:39  lr: 0.1  img/s: 5061.5752637813775  loss: 6.0904 (6.4893)  acc1: 1.5625 (1.1719)  acc5: 5.8594 (4.1775)  time: 0.0470  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [360/622]  eta: 0:00:37  lr: 0.1  img/s: 4311.871557253055  loss: 6.0738 (6.4701)  acc1: 1.9531 (1.2141)  acc5: 6.2500 (4.3919)  time: 0.0524  data: 0.0061  max mem: 16966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [370/622]  eta: 0:00:35  lr: 0.1  img/s: 4708.559641186386  loss: 6.0441 (6.4522)  acc1: 1.9531 (1.2747)  acc5: 6.2500 (4.5539)  time: 0.0542  data: 0.0080  max mem: 16966\n",
      "Epoch: [0]  [380/622]  eta: 0:00:33  lr: 0.1  img/s: 3969.727520264712  loss: 6.0329 (6.4277)  acc1: 1.9531 (1.3822)  acc5: 6.6406 (4.7676)  time: 0.0568  data: 0.0114  max mem: 16966\n",
      "Epoch: [0]  [390/622]  eta: 0:00:31  lr: 0.1  img/s: 5295.522353315996  loss: 5.9631 (6.4052)  acc1: 1.9531 (1.4258)  acc5: 7.8125 (5.0781)  time: 0.0538  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [400/622]  eta: 0:00:29  lr: 0.1  img/s: 3096.166358897938  loss: 5.9305 (6.3836)  acc1: 1.9531 (1.4863)  acc5: 8.2031 (5.3163)  time: 0.0628  data: 0.0150  max mem: 16966\n",
      "Epoch: [0]  [410/622]  eta: 0:00:28  lr: 0.1  img/s: 2408.498940252397  loss: 5.9192 (6.3629)  acc1: 2.7344 (1.5532)  acc5: 9.3750 (5.4874)  time: 0.0918  data: 0.0395  max mem: 16966\n",
      "Epoch: [0]  [420/622]  eta: 0:00:26  lr: 0.1  img/s: 3030.912933452077  loss: 5.8614 (6.3425)  acc1: 2.7344 (1.6443)  acc5: 9.7656 (5.6777)  time: 0.0927  data: 0.0413  max mem: 16966\n",
      "Epoch: [0]  [430/622]  eta: 0:00:25  lr: 0.1  img/s: 2110.917490856389  loss: 5.8441 (6.3240)  acc1: 2.7344 (1.6779)  acc5: 10.5469 (5.8683)  time: 0.1002  data: 0.0546  max mem: 16966\n",
      "Epoch: [0]  [440/622]  eta: 0:00:23  lr: 0.1  img/s: 2323.6120469949924  loss: 5.7918 (6.3044)  acc1: 2.7344 (1.7535)  acc5: 10.5469 (6.1024)  time: 0.1130  data: 0.0703  max mem: 16966\n",
      "Epoch: [0]  [450/622]  eta: 0:00:22  lr: 0.1  img/s: 2431.6198912664154  loss: 5.7758 (6.2853)  acc1: 3.1250 (1.8512)  acc5: 11.3281 (6.2500)  time: 0.1050  data: 0.0635  max mem: 16966\n",
      "Epoch: [0]  [460/622]  eta: 0:00:21  lr: 0.1  img/s: 3050.051113422713  loss: 5.7286 (6.2633)  acc1: 3.1250 (1.9448)  acc5: 11.7188 (6.4910)  time: 0.0919  data: 0.0475  max mem: 16966\n",
      "Epoch: [0]  [470/622]  eta: 0:00:19  lr: 0.1  img/s: 2714.830569262903  loss: 5.5314 (6.2427)  acc1: 3.5156 (2.0589)  acc5: 12.1094 (6.7139)  time: 0.0864  data: 0.0403  max mem: 16966\n",
      "Epoch: [0]  [480/622]  eta: 0:00:18  lr: 0.1  img/s: 2607.0844864483993  loss: 5.5276 (6.2233)  acc1: 3.5156 (2.1923)  acc5: 12.5000 (6.9276)  time: 0.0935  data: 0.0487  max mem: 16966\n",
      "Epoch: [0]  [490/622]  eta: 0:00:16  lr: 0.1  img/s: 2673.644608111059  loss: 5.5198 (6.2070)  acc1: 3.9062 (2.2656)  acc5: 12.8906 (7.1484)  time: 0.0943  data: 0.0500  max mem: 16966\n",
      "Epoch: [0]  [500/622]  eta: 0:00:15  lr: 0.1  img/s: 3352.976666539677  loss: 5.5159 (6.1900)  acc1: 3.9062 (2.3667)  acc5: 12.8906 (7.3529)  time: 0.0833  data: 0.0405  max mem: 16966\n",
      "Epoch: [0]  [510/622]  eta: 0:00:14  lr: 0.1  img/s: 2582.536135938522  loss: 5.4965 (6.1737)  acc1: 4.2969 (2.4114)  acc5: 13.6719 (7.5646)  time: 0.0850  data: 0.0419  max mem: 16966\n",
      "Epoch: [0]  [520/622]  eta: 0:00:12  lr: 0.1  img/s: 2566.7180229319924  loss: 5.4821 (6.1604)  acc1: 4.6875 (2.4690)  acc5: 14.0625 (7.6946)  time: 0.0967  data: 0.0539  max mem: 16966\n",
      "Epoch: [0]  [530/622]  eta: 0:00:11  lr: 0.1  img/s: 3633.3280794191505  loss: 5.4670 (6.1458)  acc1: 5.0781 (2.5174)  acc5: 14.4531 (7.8559)  time: 0.0824  data: 0.0380  max mem: 16966\n",
      "Epoch: [0]  [540/622]  eta: 0:00:10  lr: 0.1  img/s: 5268.827948578792  loss: 5.4391 (6.1269)  acc1: 5.0781 (2.5994)  acc5: 14.8438 (8.1108)  time: 0.0568  data: 0.0120  max mem: 16966\n",
      "Epoch: [0]  [550/622]  eta: 0:00:08  lr: 0.1  img/s: 5298.454265664423  loss: 5.4279 (6.1127)  acc1: 5.4688 (2.6507)  acc5: 16.4062 (8.2729)  time: 0.0458  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [560/622]  eta: 0:00:07  lr: 0.1  img/s: 5227.8957480268955  loss: 5.4072 (6.0961)  acc1: 5.4688 (2.7618)  acc5: 16.4062 (8.5115)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [570/622]  eta: 0:00:06  lr: 0.1  img/s: 5281.303546308593  loss: 5.3751 (6.0820)  acc1: 5.4688 (2.8354)  acc5: 17.1875 (8.7150)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [580/622]  eta: 0:00:04  lr: 0.1  img/s: 5121.330912278744  loss: 5.3453 (6.0645)  acc1: 5.4688 (2.9396)  acc5: 17.1875 (8.9645)  time: 0.0466  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [590/622]  eta: 0:00:03  lr: 0.1  img/s: 5291.805950031172  loss: 5.3375 (6.0483)  acc1: 5.8594 (3.0859)  acc5: 17.1875 (9.1862)  time: 0.0465  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [600/622]  eta: 0:00:02  lr: 0.1  img/s: 5286.735640525827  loss: 5.3339 (6.0343)  acc1: 6.2500 (3.1762)  acc5: 17.1875 (9.3814)  time: 0.0458  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [610/622]  eta: 0:00:01  lr: 0.1  img/s: 5235.686044663241  loss: 5.2932 (6.0187)  acc1: 6.2500 (3.2258)  acc5: 17.1875 (9.5010)  time: 0.0460  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [620/622]  eta: 0:00:00  lr: 0.1  img/s: 5294.193348618199  loss: 5.2774 (6.0045)  acc1: 6.2500 (3.2738)  acc5: 17.5781 (9.6726)  time: 0.0460  data: 0.0017  max mem: 16966\n",
      "Epoch: [0] Total time: 0:01:11\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2023-06-09 23:18:37] train.py:82\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:37] train.py:83\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:25  loss: 7.6194 (7.6194)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.7812)  time: 5.8004  data: 0.0764  max mem: 16966\n",
      "Test:  Total time: 0:00:07\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 195.1865692138672],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 195.1865692138672],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 2200.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 2200.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 10400.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 10400.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.330 Acc@5 1.725\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "[2023-06-09 23:18:44] train.py:415\n",
      "    [context]:                 checkpoint[\"scaler\"] = scaler.state_dict()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.state_dict() --> return state in diable mode\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0:01:23\r\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520ab-cf50-42b3-aa80-41b7b265dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924d5bf-19e9-407b-b7f9-8a585685ac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
