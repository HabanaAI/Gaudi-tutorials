{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4ae3fe2-0325-45d7-b40c-883170b290e0",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "# Dynamic Shapes and how to detect them\n",
    "\n",
    "Usually dynamicity introduces recompilations, which slows down execution. For optimizing a model's speed, it is desirable to identify if it has dynamic inputs or ops and then mitigate it if possible by following steps shown in [this document](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Dynamic_Shapes.html). In this notebook we shall discuss some tools to detect dynamic inputs and ops. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b9fba1-a9a8-4bc7-b43b-9ae897747264",
   "metadata": {},
   "source": [
    "## Types of Dynamicity\n",
    "Before we start looking at optimizations, we should discuss the main places that generate Dynamic Shapes. Dynamic Shapes can be broadly classified into two categories:\n",
    "\n",
    "-  Inputs - Dynamic shapes due to varying input shapes during training, such as varying sentence lengths in language models or differing image resolutions in image model\n",
    "\n",
    "-  Ops - Dynamic shapes due to Ops occur for certain Ops whose output shape depends on the actual input data, rather than only the input shapes, that is Ops with non-inferable output shapes given input shapes.\n",
    "\n",
    "## Follow these two steps to look for Dynamicity in your model\n",
    "#### Step 1:  Check for general recompilations and use Habana's Dynamic Shape automated support feature.  \n",
    "- Set the environment flag as follows `PT_HPU_METRICS_FILE=/root/metricslog.json PT_HPU_METRICS_DUMP_TRIGGERS=process_exit,metric_change`. This will give a broad sense of the recompilations in the model. The metricslog.json file created will show how often a `graph_compilation` is called. For static graphs, a reduction in recompilations is expected after a few steps.\n",
    "- If recompilations continue to exist, set the `PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES=1`, to enable Habana's automated Dynamic Shape control.  This variable can be set to enable the Habana PyTorch bridge and Graph Compiler to automatically manage dynamic shapes in model scripts. The graphs will be automatically bucketed and padded into ranges to achieve a common size, reducing recompilations and improving performance when working with dynamic workloads.  \n",
    "- If recompilations continue to exist, or you encounter instability and want to achieve better performance, go to step 2\n",
    "\n",
    "#### Step 2:  Deeper Analysis of the models Data and OPs\n",
    "The rest of this tutorial will cover the details of how to use these tools for specific analysis of your model.  These tools will allow you to pinpoint areas of dynamicity and make improvements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806573c-0a40-4575-a636-00888829f761",
   "metadata": {},
   "source": [
    "## Detecting Dynamic inputs with the Data Dynamicity Tool\n",
    "\n",
    "In this section we will use the `data_dynamicity` tool, which accepts a torch dataloader and produces a report of how many distinct input shapes it sees, to look at low vs high dynamicity in input datasets. We will also discuss some strategies to mitigate high input dynamicity by padding.\n",
    "\n",
    "### Image datasets\n",
    "\n",
    "#### Low input dynamicity\n",
    "\n",
    "In the example below we see `MNIST` dataset with batchsize of 7 has 2 input shapes, one for batch size = 7 and the other for batch size = 3 (because `MNIST` has `60000` training images, and `60000%7=3`, so we have batch size=3 for the last batch). This is considered very low and acceptable amount of dynamicity in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c3e7e8-a9c5-4c18-a398-16fd4ef94193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habana_frameworks.torch.utils.experimental import data_dynamicity\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Creating a sample MNIST dataloader\n",
    "mnist_ds = torchvision.datasets.MNIST('mnist', download=True, transform=torchvision.transforms.ToTensor())\n",
    "mnist_dl = DataLoader(mnist_ds, batch_size=7, num_workers=2)\n",
    "\n",
    "# Call the dataloader dynamicity tool on the dataloader\n",
    "res = data_dynamicity(mnist_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81676e7a",
   "metadata": {},
   "source": [
    "#### High input dynamicity\n",
    "\n",
    "On the other hand, for the `Flowers 102` dataset, we have images of different shapes. We see 29 different input shapes in the next example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eba44a-2d07-465a-a6d2-53b8566b103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habana_frameworks.torch.utils.experimental import data_dynamicity\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Join a list of images/labels into a single batched tensor\n",
    "# In this case we find the image with the largets dimensions in the batch,\n",
    "# and then pad everything else to that size\n",
    "def collate(batch):\n",
    "   dim1 = min([k[0].shape[1] for k in batch])\n",
    "   dim2 = min([k[0].shape[2] for k in batch])\n",
    "   images = torch.stack([k[0][:,:dim1,:dim2] for k in batch])\n",
    "   labels = torch.tensor([k[1] for k in batch])\n",
    "   return (images,labels)\n",
    "\n",
    "flowers_ds = torchvision.datasets.Flowers102('flowers', download=True, transform=torchvision.transforms.ToTensor())\n",
    "flowers_dl = DataLoader(flowers_ds, batch_size=7, num_workers=2, collate_fn=collate)\n",
    "res = data_dynamicity(flowers_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef898c",
   "metadata": {},
   "source": [
    "Depending on the usecase, we can bucket images to certain fixed sizes, or resize/crop them to a single shape. A centre-crop solution is shown in the example below, which makes the `Flowers 102` dataset more static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habana_frameworks.torch.utils.experimental import data_dynamicity\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate(batch):\n",
    "   images = torch.stack([k[0] for k in batch])\n",
    "   labels = torch.tensor([k[1] for k in batch])\n",
    "   return (images,labels)\n",
    "\n",
    "# Center crop to a fixed size, applied as a transform\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.CenterCrop((300,300))])\n",
    "flowers_ds = torchvision.datasets.Flowers102('flowers', download=True, transform=transform)\n",
    "flowers_dl = DataLoader(flowers_ds, batch_size=7, num_workers=2, collate_fn=collate)\n",
    "res = data_dynamicity(flowers_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010048b1",
   "metadata": {},
   "source": [
    "### Text datasets\n",
    "\n",
    "We often have high input dynamicity for text datasets, because sentence sizes vary a lot. In the example below, we have 443 different shapes for SQUAD dataset when batching with batchsize=7. Within each batch we pad to the largest sentence size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c54d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from habana_frameworks.torch.utils.experimental import data_dynamicity\n",
    "\n",
    "# Pad to max length sentence in each batch\n",
    "def collate(batch):\n",
    "    def pad(item, val, maxlen):\n",
    "        return torch.tensor([i + [val]*(maxlen-len(i)) for i in item])\n",
    "    token = [k['token_type_ids'] for k in batch]\n",
    "    attention = [k['attention_mask'] for k in batch]\n",
    "    inp = [k['input_ids'] for k in batch]\n",
    "    token_lens = [len(i) for i in token]\n",
    "    # Find the max length sentence in this batch\n",
    "    max_len = max(token_lens)\n",
    "    assert token_lens == [len(i) for i in attention] == [len(i) for i in inp]\n",
    "    return {'token_type_ids': pad(token, 0, max_len), 'attention_mask': pad(attention, 0, max_len), 'input_ids': pad(inp, 0, max_len)}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "squad_dataset = load_dataset('squad')\n",
    "tokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)\n",
    "\n",
    "dt = DataLoader(tokenized_dataset['train'], batch_size=7, num_workers=2, collate_fn=collate)\n",
    "res = data_dynamicity(dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181af37c",
   "metadata": {},
   "source": [
    "A very simple way to get static shapes is to pad the data to the longest sentence length. However this is inefficient computationally, because we are wasting compute effort on the padded sections which are thrown away later.\n",
    "\n",
    "In the next example we show the same SQUAD dataset padded to maximum sentence length, and thus exhibiting low input dynamicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from habana_frameworks.torch.utils.experimental import data_dynamicity\n",
    "\n",
    "# Pad to max sentence length in the whole dataset\n",
    "def get_collate(max_sentence):\n",
    "    def collate(batch):\n",
    "        def pad(item, val):\n",
    "            return torch.tensor([i + [val]*(max_sentence-len(i)) for i in item])\n",
    "        token = [k['token_type_ids'] for k in batch]\n",
    "        attention = [k['attention_mask'] for k in batch]\n",
    "        inp = [k['input_ids'] for k in batch]\n",
    "        return {'token_type_ids': pad(token, 0), 'attention_mask': pad(attention, 0), 'input_ids': pad(inp, 0)}\n",
    "    return collate\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "squad_dataset = load_dataset('squad')\n",
    "tokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)\n",
    "# Find max sentence length in the whole dataset\n",
    "max_sentence = max([len(dt['input_ids']) for dt in tokenized_dataset['train']])\n",
    "dt = DataLoader(tokenized_dataset['train'], batch_size=7, num_workers=2, collate_fn=get_collate(max_sentence))\n",
    "res = data_dynamicity(dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc212d0",
   "metadata": {},
   "source": [
    "We can reduce compilations, yet not waste computation by padding to longest sentence by using **bucketing**. Here we select a hyperparameter, which is the number of buckets, and use some algorithm to divide the range between the lengths of the shortest and the longest sentences in the dataset into buckets. Then for each batch we find the longest sentence in the batch and pad it to a bucket just larger than it.\n",
    "\n",
    "For a case study using wav2vec please refer to [this example](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Dynamic_Shapes.html#case-study-using-wav2vec2-for-dynamic-inputs-to-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b453216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from habana_frameworks.torch.utils.experimental import data_dynamicity\n",
    "import numpy as np\n",
    "\n",
    "def get_buckets(sizes, num_buckets):\n",
    "   buckets = np.unique(\n",
    "      np.percentile(\n",
    "            sizes,\n",
    "            np.linspace(0, 100, num_buckets + 1),\n",
    "            interpolation=\"lower\",\n",
    "      )[1:]\n",
    "   )\n",
    "   return buckets\n",
    "\n",
    "# Find the largest sentence in the batch\n",
    "# Then find the bucket just larger than it, and pad everything to that\n",
    "def get_collate(buckets):\n",
    "    def collate(batch):\n",
    "        def pad(item, val):\n",
    "            max_in_batch = max([len(i) for i in item])\n",
    "            nearest_bucket = np.where(buckets>=max_in_batch)[0][0]\n",
    "            return torch.tensor([i + [val]*(buckets[nearest_bucket]-len(i)) for i in item])\n",
    "        token = [k['token_type_ids'] for k in batch]\n",
    "        attention = [k['attention_mask'] for k in batch]\n",
    "        inp = [k['input_ids'] for k in batch]\n",
    "        return {'token_type_ids': pad(token, 0), 'attention_mask': pad(attention, 0), 'input_ids': pad(inp, 0)}\n",
    "    return collate\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "squad_dataset = load_dataset('squad')\n",
    "tokenized_dataset = squad_dataset.map(lambda x: tokenizer(x['context']), batched=True)\n",
    "buckets = get_buckets([len(dt['input_ids']) for dt in tokenized_dataset['train']], 5)\n",
    "dt = DataLoader(tokenized_dataset['train'], batch_size=7, num_workers=2, collate_fn=get_collate(buckets))\n",
    "res = data_dynamicity(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb4da2-8fbf-4efa-a843-6ad808c2e484",
   "metadata": {},
   "source": [
    "You can now observe that the model now has only six input shapes, where the sentence lenghts have been separated into buckets and then using the smallest amount of padding possible to fill each bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cccfb-4d0a-4247-937f-1876bee7f803",
   "metadata": {},
   "source": [
    "## Detecting Dynamic Ops\n",
    "\n",
    "Now that we know how to detect dyanmic inputs, in the next section we shall try to detect dynamic ops in models. Dynamic ops are those operations whose output shapes cannot be predicted just from knowing the input shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208843a-c4ac-45da-8f81-11fc3cf35700",
   "metadata": {},
   "source": [
    "### Simple example\n",
    "In the next example, we have a simple toy model, which we run for 5 steps. The input shape changes at the 4th step, so we expect recompilation there. However the model itself has dynamic ops, so we will see the tool identify the module which might be dynamic.  \n",
    "##### The code examples below can be run in the Terminal window.  Simply copy this code into a python file (dyn_ops.py) and run on the terminal window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a1a1e",
   "metadata": {},
   "source": [
    "<pre>\n",
    "from habana_frameworks.torch.utils.experimental import detect_recompilation_auto_model\n",
    "import torch\n",
    "\n",
    "class InnerNet(torch.nn.Module):\n",
    "   def __init__(self):\n",
    "      super(InnerNet, self).__init__()\n",
    "      self.conv = torch.nn.Conv2d(1, 8, 3, 3)\n",
    "\n",
    "   def forward(self, x):\n",
    "      x = torch.flatten(self.conv(x), 1)\n",
    "      x = x[x>0] # This is dynamic\n",
    "      return x.sum()\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.ReLU(), InnerNet()).to('hpu')\n",
    "net = detect_recompilation_auto_model(net) # wrap model in dynamic op detection tool\n",
    "\n",
    "for bs in [20,20,30,30]: #Input shape changes at 3rd step\n",
    "   inp = torch.rand(bs, 1, 50, 50).to('hpu')\n",
    "   print(net(inp))  \n",
    "net.analyse_dynamicity() # Call this after a few steps to generate the dynamicity report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44011f0c",
   "metadata": {},
   "source": [
    "The tool outputs 2 tables (and corresponding csv files)\n",
    "\n",
    "The first one shows what happens at each step, while the second one shows which module/submodule recompiled the most times. Lets analyse the first table for each step\n",
    "\n",
    "|Step |Recompiling modules |New in |New out|Class|Location|Comment|\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "|0|Net/0|True|True|torch.nn.modules.activation.ReLU|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py|Recompiled due to new input shape|\n",
    "|0|Net/1/conv|True|True|torch.nn.modules.conv.Conv2d|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py|Recompiled due to new input shape|\n",
    "|0|Net/1|True|True|\\_\\_main\\_\\_.InnerNet|dyn_ops.py|Recompiled due to new input shape|\n",
    "|0|Net|True|True|torch.nn.modules.container.Sequential|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py|Recompiled due to new input shape|\n",
    "|1|Net/1|False|False|\\_\\_main\\_\\_.InnerNet|dyn_ops.py|Already processed input shape still recompiled. Maybe dyn ops|\n",
    "|1|Net|False|False|torch.nn.modules.container.Sequential|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py|Already processed input shape still recompiled. Maybe dyn ops. Could be due to dynamic child|\n",
    "|2|Net/0|True|True|torch.nn.modules.activation.ReLU|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py|Recompiled due to new input shape|\n",
    "|2|Net/1/conv|True|True|torch.nn.modules.conv.Conv2d|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py|Recompiled due to new input shape|\n",
    "|2|Net/1|True|False|\\_\\_main\\_\\_.InnerNet|dyn_ops.py|Recompiled due to new input shape\n",
    "|2|Net|True|False|torch.nn.modules.container.Sequential|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py|Recompiled due to new input shape|\n",
    "|3|Net/1|False|False|\\_\\_main\\_\\_.InnerNet|dyn_ops.py|Already processed input shape still recompiled. Maybe dyn ops|\n",
    "|3|Net|False|False|torch.nn.modules.container.Sequential|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py|Already processed input shape still recompiled. Maybe dyn ops. Could be due to dynamic child|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Step 0__: The first two lines of the first table show all modules recompile since it is the first step.\n",
    "\n",
    "__Step 1__: The next two lines show `InnerNet` and `Net` recompile. The \"Comment\" column, however, shows that `InnerNet` might be dynamic because it recompiled even without dynamic children modules, while `Net` might not be dynamic as it might have recompiled because its child (`InnerNet`) has recompiled.\n",
    "\n",
    "__Step 2__: The next four lines show Step 2, where a new input shape is seen, so every module recompiles as expected shown in the \"Comment\" column.\n",
    "\n",
    "__Step 3__: The last two lines for Step 3 again point to `InnerNet` as having dynamic ops.\n",
    "\n",
    "Thus possible outputs from the tool's \"Comment\" column are:\n",
    "1. `Recompiled due to new input shape`\n",
    "2. `Already processed input shape still recompiled and has new output shape. Maybe dyn ops. Could be due to dynamic child`\n",
    "3. `Already processed input shape still recompiled. Maybe dyn ops`\n",
    "4. `Already processed input shape still recompiled and has new output shape. Maybe dyn ops`\n",
    "\n",
    "The first comment is due to some new shape, so recompilation is expected. The second comment is possible recompilation of a module because of recompilation of some child module. The last 2 comments are of interest because they identify modules which have the dynamic op\n",
    "\n",
    "\n",
    "Note that this tool takes time to run, so its recommended to run for a short number of steps. Also it should be run on a 1 card (without distributed). Finally, while the tool can detect recompilation due to inputs (and ignore those), it is recommended to pass in same shape inputs where possible to save time running the tool. With static inputs, the tool can focus only on finding dynamic ops, which is the more interesting case than just dynamic inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea24b05",
   "metadata": {},
   "source": [
    "In the next example, we replace the dynamic portion with a static equivalent. On running the `detect_recompilation_auto_model` tool, we now see dynamicity only from inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132ddbb",
   "metadata": {},
   "source": [
    "<pre>\n",
    "from habana_frameworks.torch.utils.experimental import detect_recompilation_auto_model\n",
    "import torch\n",
    "\n",
    "\n",
    "class InnerNet(torch.nn.Module):\n",
    "   def __init__(self):\n",
    "      super(InnerNet, self).__init__()\n",
    "      self.conv = torch.nn.Conv2d(1, 8, 3, 3)\n",
    "\n",
    "   def forward(self, x):\n",
    "      x = torch.flatten(self.conv(x), 1)\n",
    "      #x = x[x>0] # This is dynamic, replacing in next line with static implementation\n",
    "      x = torch.where(x>0, x, torch.zeros_like(x))\n",
    "      return x.sum()\n",
    "\n",
    "net = torch.nn.Sequential(torch.nn.ReLU(), InnerNet()).to('hpu')\n",
    "net = detect_recompilation_auto_model(net)\n",
    "\n",
    "for bs in [20,20,30,30]: #Input shape changes at 4th step\n",
    "   inp = torch.rand(bs, 1, 50, 50).to('hpu')\n",
    "   print(net(inp))\n",
    "net.analyse_dynamicity() # Call this after a few steps to generate the dynamicity report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad9b01",
   "metadata": {},
   "source": [
    "|Step |Recompiling modules |New in |New out|Class|Location|Comment|\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "|0|Net/0|True|True|torch.nn.modules.activation.ReLU|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py|Recompiled due to new input shape|\n",
    "|0|Net/1/conv|True|True|torch.nn.modules.conv.Conv2d|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py|Recompiled due to new input shape|\n",
    "|0|Net/1|True|True|\\_\\_main\\_\\_.InnerNet|dyn_ops_static.py|Recompiled due to new input shape|\n",
    "|0|Net|True|True|torch.nn.modules.container.Sequential|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py|Recompiled due to new input shape|\n",
    "|2|Net/0|True|True|torch.nn.modules.activation.ReLU|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/activation.py|Recompiled due to new input shape|\n",
    "|2|Net/1/conv|True|True|torch.nn.modules.conv.Conv2d|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py|Recompiled due to new input shape|\n",
    "|2|Net/1|True|False|\\_\\_main\\_\\_.InnerNet|dyn_ops_static.py|Recompiled due to new input shape|\n",
    "|2|Net|True|False|torch.nn.modules.container.Sequential|/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py|Recompiled due to new input shape|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f79c40",
   "metadata": {},
   "source": [
    "### Real model example\n",
    "\n",
    "In the next example we will look at a real model, Faster RCNN, and try to detect dynamic sections in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd51bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ultralytics.com/assets/coco128.zip\n",
    "!unzip coco128.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088e891",
   "metadata": {},
   "source": [
    "<pre>\n",
    "import torchvision, os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import habana_frameworks.torch.core as htcore\n",
    "device = 'hpu'\n",
    "\n",
    "#load model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval() # set to evaluation mode\n",
    "model = model.to(device) # move model to device\n",
    "\n",
    "from habana_frameworks.torch.utils.experimental import detect_recompilation_auto_model\n",
    "model = detect_recompilation_auto_model(model, waittime=0.3)\n",
    "\n",
    "for idx, k in enumerate(os.listdir('coco128/images/train2017/')):\n",
    "    img = Image.open('coco128/images/train2017/' + k).resize((600,600))\n",
    "    img = T.ToTensor()(img).to(device)\n",
    "    print('inp shape:', img.shape)\n",
    "    pred = model([img])\n",
    "    htcore.mark_step()\n",
    "    if idx == 6: # just running first few images\n",
    "        break\n",
    "    print('done img', idx)\n",
    "model.analyse_dynamicity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8f8f5",
   "metadata": {},
   "source": [
    "From the outputs, we see the following \n",
    "\n",
    "| Step      | Recompiling modules | New in | New out | Class | Location | Comment |\n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| 1      | Net/roi_heads/box_roi_pool     | False | False |  torchvision.ops.poolers.MultiScaleRoIAlign | /usr/local/lib/python3.8/dist-packages/torchvision/ops/poolers.py | Already processed input shape still recompiled. Maybe dyn ops |\n",
    "| 1      | Net/roi_heads | False | True | torchvision.models.detection.roi_heads.RoIHeads | /usr/local/lib/python3.8/dist-packages/torchvision/models/detection/roi_heads.py |Already processed input shape still recompiled and has new output shape. Maybe dyn ops |\n",
    "\n",
    "\n",
    " \n",
    " This tells us that the `MultiScaleRoIAlign` and `RoIHeads` classes have some dynamic ops in it. Checking the module we find the following [where](https://github.com/pytorch/vision/blob/v0.15.2/torchvision/ops/poolers.py#L201) op used in `MultiScaleRoIAlign` and another [where](https://github.com/pytorch/vision/blob/v0.15.2/torchvision/models/detection/roi_heads.py#L708) op used for `RoIHeads`. We can try to rewrite these sections as static as discussed here or move the operation to CPU. For such strategies please see this [reference](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/Dynamic_Shapes.html#mitigation-techniques-for-dynamic-ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3e2d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
