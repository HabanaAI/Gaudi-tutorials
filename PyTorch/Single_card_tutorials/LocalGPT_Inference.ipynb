{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646a18d1-97cd-4ffc-8394-3bff4e8bef6e",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dbf1e-14f5-4b1a-a67a-0859d88bf942",
   "metadata": {},
   "source": [
    "# Using LocalGPT with Retrieval Augmented Generation (RAG) on the Intel&reg; Gaudi&reg; 2 AI accelerator with the Llama 2 7B model\n",
    "This tutorial will show how to use the [LocalGPT](https://github.com/PromtEngineer/localGPT) open source initiative on the Intel Gaudi 2 AI accelerator.  LocalGPT allows you to load your own documents and run an interactive chat session with this material using concepts from Retrieval Augmented Generation (RAG).  \n",
    "\n",
    "This allows you to query and summarize your content by loading any .pdf or .txt documents into the `SOURCE DOCUMENTS` folder, using utilities from the ingest.py script to tokenize your content and then the run_localGPT.py script to start the interaction.  \n",
    "\n",
    "The first section shows how RAG works and run thruough the steps of the indexing the local content, retrieval and text generation with a single question and response. \n",
    "\n",
    "The last section uses the full LocalGPT framework with the **meta-llama/Llama-2-70b-chat-hf** model as the reference model that will manage the inference on Gaudi 2.  DeepSpeed inference is used based on the size of the model.\n",
    "\n",
    "To optimize this instantiation of LocalGPT, we have created new content on top of the existing Hugging Face based \"text-generation\" inference task and pipelines, including:\n",
    "\n",
    "1. Using the Hugging Face Optimum Habana Library with the Llama 2 70B model, which is optimized on Gaudi2. \n",
    "2. Using LangChain to import the source document with a custom embedding model, using a `GaudiHuggingFaceEmbeddings` class based on HuggingFaceEmbeddings.\n",
    "3. We are using a custom pipeline class, `GaudiTextGenerationPipeline` that optimizes text-generation tasks for padding and indexing for static shapes, to improve performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1b812-5032-4a26-867e-155575a14991",
   "metadata": {},
   "source": [
    "##### Go to the LocalGPT folder and set environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c604050-d6f5-47a9-adc7-0b9e3c0fe34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/local_gpt\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/local_gpt\n",
    "!export DEBIAN_FRONTEND=\"noninteractive\"\n",
    "!export TZ=Etc/UTC\n",
    "\n",
    "import sys\n",
    "print(sys.path)\n",
    "sys.path=['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '~/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages',\"~/.local/bin\",\"/usr/bin/habanatools\",\"/usr/local/sbin\",\"/usr/local/bin\",\"/usr/sbin\",\"/usr/bin\",\"/sbin\",\"/bin\",\"/usr/games\",\"/usr/local/games\",\"/snap/bin\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e2383-deaf-4349-90e5-31bea9a507b0",
   "metadata": {},
   "source": [
    "##### Install the requirements for LocalGPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadcf84-eb2b-4f04-9cbf-4e6d67d4e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d111e-2118-4bb0-b62d-214e895479a2",
   "metadata": {},
   "source": [
    "##### Install the Optimum Habana Library from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c2833-0bb7-481e-907d-7b28d88bbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optimum-habana==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095322a0",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "LocalGPT uses Retrieval-Augmented Generation (RAG) at it's core. RAG is a relatively new AI technique that combines an information retrieval system with text-generation models/LLMs. It provides an effective way to ground LLMs by using retrieved contexts from an external knowledge base, without having to perform retraining or finetuning.\n",
    "The LocalGPT application workflow can be broken down as follows:\n",
    "* Document Ingestion: This step involves creating an external knowledge base via a vector database. The text present in the documents is parsed, split into chunks and converted to embeddings using an embedding model. The vector embeddings are finally stored in the vector database.\n",
    "\n",
    "![](local_gpt/img/ingest.jpg)\n",
    "\n",
    "* Text Generation: This step involves accepting a query from the user, converting the query to embeddings and retrieving appropriate contexts from the knowledge base. The input prompt to the LLM is the concatenation of the query, contexts and chat history.\n",
    "\n",
    "![](local_gpt/img/documentqa.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2eaba6-2d4b-4007-bb13-ca5113022aad",
   "metadata": {},
   "source": [
    "### Document Ingestion\n",
    "Copy all of your files into the `SOURCE_DOCUMENTS` directory\n",
    "\n",
    "The current default file types are .txt, .pdf, .csv, and .xlsx, if you want to use any other file type, you will need to convert it to one of the default file types.\n",
    "\n",
    "Run the following cells to ingest all the data. This notebook uses LangChain tools to parse the documents and create embeddings locally using the GaudiHuggingFaceEmbeddings class. It then stores the result in a local vector database (DB) using Chroma vector store. \n",
    "\n",
    "If you want to start from an empty database, delete the DB folder and run the next few cells again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ae917-86df-4061-9f9a-d6cb2bb7cd75",
   "metadata": {},
   "source": [
    "##### Load your files as LangChain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7db570f4-acce-4e73-8ac1-7742e621c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from /root/Gaudi-tutorials/PyTorch/Single_card_tutorials/local_gpt/SOURCE_DOCUMENTS\n"
     ]
    }
   ],
   "source": [
    "from constants import SOURCE_DIRECTORY\n",
    "from ingest import load_documents\n",
    "\n",
    "documents = load_documents(SOURCE_DIRECTORY)\n",
    "print(f\"Loaded {len(documents)} documents from {SOURCE_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730b93b-eb2c-45a9-9989-203b7de37be4",
   "metadata": {},
   "source": [
    "##### Split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a90c469-dcae-49aa-8c80-a3043072c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 72 chunks of text\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(texts)} chunks of text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ab0c9-f379-4870-a36e-6f221a6520a1",
   "metadata": {},
   "source": [
    "##### Create embeddings from chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63ebe1ee-f066-4083-aa53-eddb73996be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/InstructorEmbedding/instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Habana modules from /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:842: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from constants import EMBEDDING_INPUT_SIZE, EMBEDDING_MODEL_NAME\n",
    "from gaudi_utils.embeddings import GaudiHuggingFaceEmbeddings\n",
    "\n",
    "from habana_frameworks.torch.utils.library_loader import load_habana_module\n",
    "load_habana_module()\n",
    "\n",
    "embeddings = GaudiHuggingFaceEmbeddings(embedding_input_size=EMBEDDING_INPUT_SIZE, model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": \"hpu\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079b506-0fcf-49ca-9865-0ebfa8170df3",
   "metadata": {},
   "source": [
    "##### Create a Chroma vector database to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b30209-a897-40b5-aaab-fe0c33585c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056399524 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create vector store: 5927.7566589880735 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from constants import PERSIST_DIRECTORY, CHROMA_SETTINGS\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory=PERSIST_DIRECTORY, client_settings=CHROMA_SETTINGS)\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Time taken to create vector store: {(end_time-start_time)*1000} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45867b-da73-4fdf-b821-ac932ca2c1d7",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama 2 model\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses.\n",
    "\n",
    "To be able to run gated models like this Llama-2-7b-chat-hf, you need the following:\n",
    "\n",
    "* Have a HuggingFace account\n",
    "* Agree to the terms of use of the model in its model card on the HF Hub\n",
    "* Set a read token\n",
    "* Login to your account using the HF CLI: run huggingface-cli login before launching your script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffeec3a-1671-4cce-940c-3fe962f0b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#huggingface-cli login --token <your_token_here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01705607",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "Once the Chroma vector database is ready, we can explore the text-generation component of LocalGPT.\n",
    "\n",
    "The next few cells describe all the steps in the text generation process. We use the smallest Llama 2 model **meta-llama/Llama-2-7b-chat-hf** to perform augmented text-generation after retrieving relevant contexts from the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d90d69",
   "metadata": {},
   "source": [
    "##### Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c8060e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:04<00:00, 62.32s/it]\n",
      "Downloading shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [02:05<00:00, 62.80s/it]\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.28it/s]\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "from run_localGPT import load_model\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "llm, _ = load_model(device_type=\"hpu\", model_id=model_id, temperature=0.2, top_p=0.95, model_basename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d001a99",
   "metadata": {},
   "source": [
    "##### Define the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b3ff0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b188c",
   "metadata": {},
   "source": [
    "##### Create the prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5892a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer,\\\n",
    "just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70acaf",
   "metadata": {},
   "source": [
    "##### Initialize a LangChain object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97272db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb87269",
   "metadata": {},
   "source": [
    "##### Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb908353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text provided appears to be the United States Constitution.\n"
     ]
    }
   ],
   "source": [
    "res = qa(\"What is this document about?\")\n",
    "print(res[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a6e92",
   "metadata": {},
   "source": [
    "##### Clean up before running Full LocalGPT below\n",
    "To run the full Local LocalGPT model below, you need to restart the Kernel in the Jupyter Server to ensure that all the Intel Gaudi Accelerators are released.  This can be accomplished by selecting this option in the Kernel menu or the `exit()` command at the bottom of this notebook. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3ac741b-cb79-4fab-a1aa-efa075503d61",
   "metadata": {},
   "source": [
    "### Running the LocalGPT Interactive example with Llama 2 7B Chat \n",
    "\n",
    "### Set the model Usage\n",
    "\n",
    "To change the model, you can modify the \"LLM_ID = <add model here>\" in the `constants.py` file. For this single card example, the default is `meta-llama/Llama-2-7b-chat-hf`.  \n",
    "\n",
    "Since this is interactive, it's a better experince to launch this from a terminal window.  This run_localGPT.py script uses a local LLM (Llama 2 in this case) to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the documentation.  This is the run command to use:\n",
    "\n",
    "`PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python3 gaudi_spawn.py run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`\n",
    "\n",
    "Note: The inference is running sampling mode, so the user can optinally modify the temperature and top_p settings.  The current settings are temperature=0.7, top_p=0.95.  Type \"exit\" at the prompt to stop the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ee22fd-2bca-49b8-8f8d-501075058b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this command in a terminal window to start the interactive chat: `PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python3 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95`, the example below is showing the initial output:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc237333-fbdc-440e-8249-89639af4c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-22 18:25:26,074 - INFO - run_localGPT.py:215 - Running on: hpu\n",
      "2024-03-22 18:25:26,074 - INFO - run_localGPT.py:216 - Display Source Documents set to: False\n",
      "2024-03-22 18:25:26,074 - INFO - run_localGPT.py:45 - temperature set to 0.2, top_p set to 0.95\n",
      "2024-03-22 18:25:26,074 - INFO - run_localGPT.py:46 - Loading Model: meta-llama/Llama-2-7b-chat-hf, on: hpu\n",
      "2024-03-22 18:25:26,074 - INFO - run_localGPT.py:47 - This action can take a few minutes!\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cextension.py:31: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "Fetching 2 files: 100%|████████████████████████| 2/2 [00:00<00:00, 11366.68it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.46it/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056399524 KB\n",
      "------------------------------------------------------------------------------\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "2024-03-22 18:25:57,787 - INFO - run_localGPT.py:152 - Local LLM Loaded\n",
      "2024-03-22 18:25:57,977 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:842: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "2024-03-22 18:25:58,082 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "\n",
      "Enter a query: "
     ]
    }
   ],
   "source": [
    "!PT_HPU_LAZY_ACC_PAR_MODE=1 PT_HPU_ENABLE_LAZY_COLLECTIVES=true python3 run_localGPT.py --device_type hpu --temperature 0.7 --top_p 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe1723-04da-4d59-a299-fe0431a84bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c878ae-7e96-4148-b631-5e9399d9fb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
