{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "##### BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# Model Migration from GPU to the Intel&reg; Gaudi&reg; 2 AI Accelerator \n",
    "\n",
    "The GPU Migration toolkit simplifies migrating PyTorch models that run on GPU-based architecture to run on the Intel® Gaudi® AI accelerator. Rather than manually replacing Python API calls that have dependencies on GPU libraries with Gaudi-specific API calls, the toolkit automates this process so you can run your model with fewer modifications. \n",
    "\n",
    "The GPU Migration toolkit maps specific API calls from the Python libraries and modules listed below to the appropriate equivalents in the Intel Gaudi software:\n",
    "\n",
    "* torch.cuda  \n",
    "* Torch API with GPU related parameters. For example, torch.randn(device=”cuda”)  \n",
    "\n",
    "The toolkit does not optimize the performance of the model, so further modifications may be required. For more details, refer to Model Performance Optimization Guide.\n",
    "\n",
    "In this notebook we will demonstrate how to use the GPU Migration toolset on a ResNet50 model which is based on open source implementation of ResNet50.  \n",
    "\n",
    "Refer to the [GPU Migration Toolkit](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html) for more information.  \n",
    "\n",
    "In addition to this ResNet50 migration, there is a GPU Migration example on the Intel Gaudi GitHub page [here](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/examples/gpu_migration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e904bd5",
   "metadata": {},
   "source": [
    "### Enabling the GPU Migration Toolkit Summary\n",
    "\n",
    "#### Set the library\n",
    "Import the habana_frameworks.torch.gpu_migration package at the beginning of the primary Python script (main.py, train.py, etc.):\n",
    "`import habana_frameworks.torch.gpu_migration`\n",
    "Alternatively, you can use PT_HPU_GPU_MIGRATION=1 environment variable when running the primary Python script (main.py, train.py, etc.): `PT_HPU_GPU_MIGRATION=1 $PYTHON main.py`\n",
    "\n",
    "#### Set the Mark Step\n",
    "Add mark_step(). In Lazy mode, mark_step() must be added in all training scripts right after loss.backward() and optimizer.step().\n",
    "\n",
    "`htcore.mark_step()`   Note that if your model is using torch.compile, this step is not needed. \n",
    "\n",
    "#### Running Migrated code and logging changes \n",
    "Make sure that any device selection argument passed to the script is configured as if the script is running on a GPU. For example, add --cuda or --device gpu in the runtime command of your model. This will guarantee that the GPU Migration toolkit accurately detects and migrates instructions.\n",
    "\n",
    "You can enable the logging feature, included in the GPU Migration toolkit, by setting the `GPU_MIGRATION_LOG_LEVEL` environment variable like this example:   \n",
    "`GPU_MIGRATION_LOG_LEVEL=3 PT_HPU_GPU_MIGRATION=1 $PYTHON main.py`\n",
    "\n",
    "#### For More Information\n",
    "For more information regarding the use and configuration of the GPU Migration Toolkit and its limitations, please refer to the documentation here (https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the `exit()` command to restart the Python kernel to ensure that there are no other processes holding the Intel Gaudi Accelerator as you start to run this notebook.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Running the ResNet50 Example\n",
    "The remainder of the notebook will show how the tool works with the [ResNet50 example](https://github.com/HabanaAI/Model-References/tree/master/PyTorch/examples/gpu_migration) from the GPU Migration examples in Model-References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone -b 1.15.1 https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Navigate to the model example to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset - OPTIONAL\n",
    "To fully run this example you can download the ImageNet 2012 dataset.  It needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch).   You do NOT need to have the dataset loaded to see the Migration steps and logging.    \n",
    "\n",
    "If you do not download the dataset the training will not complete the execution, but you will see the GPU Migration changes in the logfile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee70dc",
   "metadata": {},
   "source": [
    "#### Import GPU Migration Toolkit package and Habana Torch Library\n",
    "Look into train.py, you will see in the first line that we will load the `gpu.migration` library which is already included in the Intel Gaudi Software: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c905c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4\timport habana_frameworks.torch.gpu_migration\n",
      "     5\timport datetime\n",
      "     6\timport os\n",
      "     7\timport time\n",
      "     8\timport warnings\n",
      "     9\t\n",
      "    10\timport presets\n",
      "    11\timport torch\n",
      "    12\timport torch.utils.data\n",
      "    13\timport torchvision\n",
      "    14\timport transforms\n",
      "    15\timport utils\n",
      "    16\timport habana_frameworks.torch.utils.experimental as htexp\n",
      "    17\tfrom sampler import RASampler\n",
      "    18\tfrom torch import nn\n",
      "    19\tfrom torch.utils.data.dataloader import default_collate\n",
      "    20\tfrom torchvision.transforms.functional import InterpolationMode\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 21 | tail -n 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed748205",
   "metadata": {},
   "source": [
    "#### Placing mark_step()\n",
    "You will have to place the mark_step() function after the optimizer and loss.backward calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4e1aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    38\t        optimizer.zero_grad(set_to_none=True)\n",
      "    39\t        if scaler is not None:\n",
      "    40\t            scaler.scale(loss).backward()\n",
      "    41\t            htcore.mark_step()\n",
      "    42\t            if args.clip_grad_norm is not None:\n",
      "    43\t                # we should unscale the gradients of optimizer's assigned params if do gradient clipping\n",
      "    44\t                scaler.unscale_(optimizer)\n",
      "    45\t                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
      "    46\t            scaler.step(optimizer)\n",
      "    47\t            htcore.mark_step()\n",
      "    48\t            scaler.update()\n",
      "    49\t        else:\n",
      "    50\t            loss.backward()\n",
      "    51\t            htcore.mark_step()\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 51 | tail -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start multi-HPU training.\n",
    "We're now ready to run the training.  You will see that we've added the logging command at the beginning of the run: `GPU_MIGRATION_LOG_LEVEL=1` to show the output.   No other changes to the run command are needed.   As you see the training run is started, you will see the log files show exactly where the code changes are happening to change from GPU to Intel Gaudi, including the file name and location.\n",
    "\n",
    "Look for the [context] and [hpu_match] in the log file to see where the code is changed.\n",
    "\n",
    "Remember that if you do not download the dataset the training will not complete the execution, but you will see the GPU Migration changes in the logfile, this is the most important part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-03-21/00-21-25/gpu_migration_8399.log\n",
      "[2024-03-21 00:21:25] /root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2024-03-21 00:21:27] /root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=1, rank=0, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-03-21 00:21:27] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:81\n",
      "    [context]:                 \"backend\": f\"{dist.get_backend(kwargs.get('group'))}\",\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-03-21 00:21:27] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:130\n",
      "    [context]:       backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "Namespace(use_torch_compile=False, data_path='/root/software/data/pytorch/imagenet/ILSVRC2012', model='resnet50', device='cuda', batch_size=256, epochs=1, dl_worker_type='HABANA', workers=8, opt='sgd', lr=0.1, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='custom_lr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=10, output_dir='.', resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=True, world_size=1, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, save_checkpoint=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
      "[2024-03-21 00:21:27] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py\", line 576, in <module>\n",
      "    main(args)\n",
      "  File \"/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py\", line 233, in main\n",
      "    dataset, dataset_test, train_sampler, test_sampler = load_data(train_dir, val_dir, args)\n",
      "  File \"/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py\", line 147, in load_data\n",
      "    dataset = torchvision.datasets.ImageFolder(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 309, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 144, in __init__\n",
      "    classes, class_to_idx = self.find_classes(self.root)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 218, in find_classes\n",
      "    return find_classes(directory)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\", line 40, in find_classes\n",
      "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/software/data/pytorch/imagenet/ILSVRC2012/train'\n",
      "[2024-03-21 00:21:32,018] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 8399) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 806, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-03-21_00:21:32\n",
      "  host      : hls2-srv01-demolab\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 8399)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-09 23:17:26] train.py:32\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] train.py:33\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/common.py:7\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec('torch_xla'))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/622]  eta: 2:06:32  lr: 0.1  img/s: 20.973556319305924  loss: 7.1048 (7.1048)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.3906)  time: 12.2059  data: 1.2269  max mem: 15838\n",
      "Epoch: [0]  [ 10/622]  eta: 0:31:50  lr: 0.1  img/s: 115.61183054130028  loss: 7.1048 (7.1195)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.7812)  time: 3.1221  data: 0.1968  max mem: 16966\n",
      "Epoch: [0]  [ 20/622]  eta: 0:16:40  lr: 0.1  img/s: 4432.052451134573  loss: 7.1133 (7.1174)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (0.9115)  time: 1.1344  data: 0.0488  max mem: 16966\n",
      "Epoch: [0]  [ 30/622]  eta: 0:11:16  lr: 0.1  img/s: 4516.90829523116  loss: 7.1133 (7.1285)  acc1: 0.3906 (0.4883)  acc5: 0.7812 (0.8789)  time: 0.0546  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [ 40/622]  eta: 0:08:30  lr: 0.1  img/s: 4853.195422435741  loss: 7.1133 (7.0867)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8594)  time: 0.0521  data: 0.0047  max mem: 16966\n",
      "Epoch: [0]  [ 50/622]  eta: 0:06:48  lr: 0.1  img/s: 5000.998223147521  loss: 7.1048 (7.0657)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8464)  time: 0.0493  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 60/622]  eta: 0:05:39  lr: 0.1  img/s: 5282.966572527873  loss: 7.1048 (7.0459)  acc1: 0.3906 (0.3348)  acc5: 0.7812 (0.7812)  time: 0.0472  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [ 70/622]  eta: 0:04:50  lr: 0.1  img/s: 4933.84042494532  loss: 6.9607 (7.0270)  acc1: 0.3906 (0.2930)  acc5: 0.7812 (0.7812)  time: 0.0475  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 80/622]  eta: 0:04:13  lr: 0.1  img/s: 4933.219318243425  loss: 6.9607 (7.0097)  acc1: 0.3906 (0.3038)  acc5: 0.7812 (0.7812)  time: 0.0492  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [ 90/622]  eta: 0:03:44  lr: 0.1  img/s: 5296.3007443733  loss: 6.9274 (6.9888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9766)  time: 0.0475  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [100/622]  eta: 0:03:20  lr: 0.1  img/s: 5273.366650950809  loss: 6.9274 (6.9741)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9233)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [110/622]  eta: 0:03:01  lr: 0.1  img/s: 5202.998810872521  loss: 6.9192 (6.9543)  acc1: 0.3906 (0.3581)  acc5: 0.7812 (1.0417)  time: 0.0462  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [120/622]  eta: 0:02:44  lr: 0.1  img/s: 5171.708403494103  loss: 6.9192 (6.9406)  acc1: 0.3906 (0.3606)  acc5: 0.7812 (1.1118)  time: 0.0467  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [130/622]  eta: 0:02:30  lr: 0.1  img/s: 5266.486614806604  loss: 6.8949 (6.9260)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.3393)  time: 0.0464  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [140/622]  eta: 0:02:20  lr: 0.1  img/s: 2805.1565217414022  loss: 6.8949 (6.9064)  acc1: 0.3906 (0.4167)  acc5: 0.7812 (1.4583)  time: 0.0673  data: 0.0086  max mem: 16966\n",
      "Epoch: [0]  [150/622]  eta: 0:02:09  lr: 0.1  img/s: 5234.665052013101  loss: 6.8713 (6.8888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.4893)  time: 0.0675  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [160/622]  eta: 0:02:00  lr: 0.1  img/s: 5222.205370535011  loss: 6.8713 (6.8671)  acc1: 0.3906 (0.4136)  acc5: 1.1719 (1.5165)  time: 0.0463  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [170/622]  eta: 0:01:52  lr: 0.1  img/s: 5223.231671485778  loss: 6.8270 (6.8482)  acc1: 0.3906 (0.4774)  acc5: 1.1719 (1.6059)  time: 0.0463  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [180/622]  eta: 0:01:44  lr: 0.1  img/s: 4716.0108590957225  loss: 6.8270 (6.8283)  acc1: 0.3906 (0.4729)  acc5: 1.1719 (1.6653)  time: 0.0489  data: 0.0041  max mem: 16966\n",
      "Epoch: [0]  [190/622]  eta: 0:01:38  lr: 0.1  img/s: 5201.287478013948  loss: 6.8005 (6.8103)  acc1: 0.3906 (0.5078)  acc5: 1.1719 (1.8164)  time: 0.0490  data: 0.0040  max mem: 16966\n",
      "Epoch: [0]  [200/622]  eta: 0:01:32  lr: 0.1  img/s: 5223.7449732474115  loss: 6.7767 (6.7917)  acc1: 0.3906 (0.5580)  acc5: 1.9531 (1.9903)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [210/622]  eta: 0:01:26  lr: 0.1  img/s: 5242.224721909762  loss: 6.7362 (6.7727)  acc1: 0.3906 (0.6037)  acc5: 1.9531 (2.1662)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [220/622]  eta: 0:01:21  lr: 0.1  img/s: 5272.3490329002  loss: 6.7361 (6.7572)  acc1: 0.3906 (0.6624)  acc5: 1.9531 (2.2418)  time: 0.0460  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [230/622]  eta: 0:01:16  lr: 0.1  img/s: 5257.62599791603  loss: 6.6319 (6.7398)  acc1: 0.3906 (0.6673)  acc5: 2.3438 (2.2786)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [240/622]  eta: 0:01:12  lr: 0.1  img/s: 5265.554278899224  loss: 6.6249 (6.7193)  acc1: 0.7812 (0.7188)  acc5: 2.7344 (2.3438)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [250/622]  eta: 0:01:08  lr: 0.1  img/s: 5245.37462640924  loss: 6.5275 (6.6944)  acc1: 0.7812 (0.7662)  acc5: 2.7344 (2.5090)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [260/622]  eta: 0:01:04  lr: 0.1  img/s: 5223.676357764838  loss: 6.5193 (6.6699)  acc1: 0.7812 (0.7668)  acc5: 3.1250 (2.5608)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [270/622]  eta: 0:01:01  lr: 0.1  img/s: 4976.946303265318  loss: 6.4692 (6.6476)  acc1: 0.7812 (0.7673)  acc5: 3.1250 (2.6925)  time: 0.0475  data: 0.0031  max mem: 16966\n",
      "Epoch: [0]  [280/622]  eta: 0:00:58  lr: 0.1  img/s: 4616.917909177062  loss: 6.4686 (6.6303)  acc1: 0.7812 (0.8082)  acc5: 3.1250 (2.8691)  time: 0.0506  data: 0.0069  max mem: 16966\n",
      "Epoch: [0]  [290/622]  eta: 0:00:54  lr: 0.1  img/s: 4638.213659670236  loss: 6.4193 (6.6123)  acc1: 0.7812 (0.8464)  acc5: 3.9062 (3.0990)  time: 0.0526  data: 0.0075  max mem: 16966\n",
      "Epoch: [0]  [300/622]  eta: 0:00:52  lr: 0.1  img/s: 4683.1702654478195  loss: 6.4160 (6.5903)  acc1: 0.7812 (0.9451)  acc5: 3.9062 (3.2006)  time: 0.0523  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [310/622]  eta: 0:00:49  lr: 0.1  img/s: 5126.348193291654  loss: 6.3754 (6.5707)  acc1: 1.1719 (1.0010)  acc5: 3.9062 (3.4302)  time: 0.0497  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [320/622]  eta: 0:00:46  lr: 0.1  img/s: 5302.313549730277  loss: 6.3376 (6.5510)  acc1: 1.5625 (1.0535)  acc5: 4.2969 (3.5748)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [330/622]  eta: 0:00:44  lr: 0.1  img/s: 5269.187344039455  loss: 6.2278 (6.5302)  acc1: 1.5625 (1.1029)  acc5: 4.6875 (3.8143)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [340/622]  eta: 0:00:41  lr: 0.1  img/s: 5273.364061093401  loss: 6.1463 (6.5111)  acc1: 1.5625 (1.1719)  acc5: 5.4688 (4.0290)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [350/622]  eta: 0:00:39  lr: 0.1  img/s: 5061.5752637813775  loss: 6.0904 (6.4893)  acc1: 1.5625 (1.1719)  acc5: 5.8594 (4.1775)  time: 0.0470  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [360/622]  eta: 0:00:37  lr: 0.1  img/s: 4311.871557253055  loss: 6.0738 (6.4701)  acc1: 1.9531 (1.2141)  acc5: 6.2500 (4.3919)  time: 0.0524  data: 0.0061  max mem: 16966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [370/622]  eta: 0:00:35  lr: 0.1  img/s: 4708.559641186386  loss: 6.0441 (6.4522)  acc1: 1.9531 (1.2747)  acc5: 6.2500 (4.5539)  time: 0.0542  data: 0.0080  max mem: 16966\n",
      "Epoch: [0]  [380/622]  eta: 0:00:33  lr: 0.1  img/s: 3969.727520264712  loss: 6.0329 (6.4277)  acc1: 1.9531 (1.3822)  acc5: 6.6406 (4.7676)  time: 0.0568  data: 0.0114  max mem: 16966\n",
      "Epoch: [0]  [390/622]  eta: 0:00:31  lr: 0.1  img/s: 5295.522353315996  loss: 5.9631 (6.4052)  acc1: 1.9531 (1.4258)  acc5: 7.8125 (5.0781)  time: 0.0538  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [400/622]  eta: 0:00:29  lr: 0.1  img/s: 3096.166358897938  loss: 5.9305 (6.3836)  acc1: 1.9531 (1.4863)  acc5: 8.2031 (5.3163)  time: 0.0628  data: 0.0150  max mem: 16966\n",
      "Epoch: [0]  [410/622]  eta: 0:00:28  lr: 0.1  img/s: 2408.498940252397  loss: 5.9192 (6.3629)  acc1: 2.7344 (1.5532)  acc5: 9.3750 (5.4874)  time: 0.0918  data: 0.0395  max mem: 16966\n",
      "Epoch: [0]  [420/622]  eta: 0:00:26  lr: 0.1  img/s: 3030.912933452077  loss: 5.8614 (6.3425)  acc1: 2.7344 (1.6443)  acc5: 9.7656 (5.6777)  time: 0.0927  data: 0.0413  max mem: 16966\n",
      "Epoch: [0]  [430/622]  eta: 0:00:25  lr: 0.1  img/s: 2110.917490856389  loss: 5.8441 (6.3240)  acc1: 2.7344 (1.6779)  acc5: 10.5469 (5.8683)  time: 0.1002  data: 0.0546  max mem: 16966\n",
      "Epoch: [0]  [440/622]  eta: 0:00:23  lr: 0.1  img/s: 2323.6120469949924  loss: 5.7918 (6.3044)  acc1: 2.7344 (1.7535)  acc5: 10.5469 (6.1024)  time: 0.1130  data: 0.0703  max mem: 16966\n",
      "Epoch: [0]  [450/622]  eta: 0:00:22  lr: 0.1  img/s: 2431.6198912664154  loss: 5.7758 (6.2853)  acc1: 3.1250 (1.8512)  acc5: 11.3281 (6.2500)  time: 0.1050  data: 0.0635  max mem: 16966\n",
      "Epoch: [0]  [460/622]  eta: 0:00:21  lr: 0.1  img/s: 3050.051113422713  loss: 5.7286 (6.2633)  acc1: 3.1250 (1.9448)  acc5: 11.7188 (6.4910)  time: 0.0919  data: 0.0475  max mem: 16966\n",
      "Epoch: [0]  [470/622]  eta: 0:00:19  lr: 0.1  img/s: 2714.830569262903  loss: 5.5314 (6.2427)  acc1: 3.5156 (2.0589)  acc5: 12.1094 (6.7139)  time: 0.0864  data: 0.0403  max mem: 16966\n",
      "Epoch: [0]  [480/622]  eta: 0:00:18  lr: 0.1  img/s: 2607.0844864483993  loss: 5.5276 (6.2233)  acc1: 3.5156 (2.1923)  acc5: 12.5000 (6.9276)  time: 0.0935  data: 0.0487  max mem: 16966\n",
      "Epoch: [0]  [490/622]  eta: 0:00:16  lr: 0.1  img/s: 2673.644608111059  loss: 5.5198 (6.2070)  acc1: 3.9062 (2.2656)  acc5: 12.8906 (7.1484)  time: 0.0943  data: 0.0500  max mem: 16966\n",
      "Epoch: [0]  [500/622]  eta: 0:00:15  lr: 0.1  img/s: 3352.976666539677  loss: 5.5159 (6.1900)  acc1: 3.9062 (2.3667)  acc5: 12.8906 (7.3529)  time: 0.0833  data: 0.0405  max mem: 16966\n",
      "Epoch: [0]  [510/622]  eta: 0:00:14  lr: 0.1  img/s: 2582.536135938522  loss: 5.4965 (6.1737)  acc1: 4.2969 (2.4114)  acc5: 13.6719 (7.5646)  time: 0.0850  data: 0.0419  max mem: 16966\n",
      "Epoch: [0]  [520/622]  eta: 0:00:12  lr: 0.1  img/s: 2566.7180229319924  loss: 5.4821 (6.1604)  acc1: 4.6875 (2.4690)  acc5: 14.0625 (7.6946)  time: 0.0967  data: 0.0539  max mem: 16966\n",
      "Epoch: [0]  [530/622]  eta: 0:00:11  lr: 0.1  img/s: 3633.3280794191505  loss: 5.4670 (6.1458)  acc1: 5.0781 (2.5174)  acc5: 14.4531 (7.8559)  time: 0.0824  data: 0.0380  max mem: 16966\n",
      "Epoch: [0]  [540/622]  eta: 0:00:10  lr: 0.1  img/s: 5268.827948578792  loss: 5.4391 (6.1269)  acc1: 5.0781 (2.5994)  acc5: 14.8438 (8.1108)  time: 0.0568  data: 0.0120  max mem: 16966\n",
      "Epoch: [0]  [550/622]  eta: 0:00:08  lr: 0.1  img/s: 5298.454265664423  loss: 5.4279 (6.1127)  acc1: 5.4688 (2.6507)  acc5: 16.4062 (8.2729)  time: 0.0458  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [560/622]  eta: 0:00:07  lr: 0.1  img/s: 5227.8957480268955  loss: 5.4072 (6.0961)  acc1: 5.4688 (2.7618)  acc5: 16.4062 (8.5115)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [570/622]  eta: 0:00:06  lr: 0.1  img/s: 5281.303546308593  loss: 5.3751 (6.0820)  acc1: 5.4688 (2.8354)  acc5: 17.1875 (8.7150)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [580/622]  eta: 0:00:04  lr: 0.1  img/s: 5121.330912278744  loss: 5.3453 (6.0645)  acc1: 5.4688 (2.9396)  acc5: 17.1875 (8.9645)  time: 0.0466  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [590/622]  eta: 0:00:03  lr: 0.1  img/s: 5291.805950031172  loss: 5.3375 (6.0483)  acc1: 5.8594 (3.0859)  acc5: 17.1875 (9.1862)  time: 0.0465  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [600/622]  eta: 0:00:02  lr: 0.1  img/s: 5286.735640525827  loss: 5.3339 (6.0343)  acc1: 6.2500 (3.1762)  acc5: 17.1875 (9.3814)  time: 0.0458  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [610/622]  eta: 0:00:01  lr: 0.1  img/s: 5235.686044663241  loss: 5.2932 (6.0187)  acc1: 6.2500 (3.2258)  acc5: 17.1875 (9.5010)  time: 0.0460  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [620/622]  eta: 0:00:00  lr: 0.1  img/s: 5294.193348618199  loss: 5.2774 (6.0045)  acc1: 6.2500 (3.2738)  acc5: 17.5781 (9.6726)  time: 0.0460  data: 0.0017  max mem: 16966\n",
      "Epoch: [0] Total time: 0:01:11\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2023-06-09 23:18:37] train.py:82\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:37] train.py:83\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:25  loss: 7.6194 (7.6194)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.7812)  time: 5.8004  data: 0.0764  max mem: 16966\n",
      "Test:  Total time: 0:00:07\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 195.1865692138672],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 195.1865692138672],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 2200.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 2200.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 10400.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 10400.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.330 Acc@5 1.725\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "[2023-06-09 23:18:44] train.py:415\n",
      "    [context]:                 checkpoint[\"scaler\"] = scaler.state_dict()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.state_dict() --> return state in diable mode\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0:01:23\r\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 1 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990520ab-cf50-42b3-aa80-41b7b265dc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resorces running on Intel Gaudi are released \n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924d5bf-19e9-407b-b7f9-8a585685ac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
