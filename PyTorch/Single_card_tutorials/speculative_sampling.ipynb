{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2025 Habana Labs, Ltd. an Intel Company.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speculative Sampling Inference using Intel&reg; Gaudi&reg; AI Accelerator with PyTorch\n",
    "\n",
    "In this notebook we will demonstrate how you can run inference on the Intel Gaudi Accelerator with the speculative sampling method of text generation model using Optimum Habana and Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the `exit()` command to restart the Python kernel to ensure that there are no other processes holding the Intel Gaudi Accelerator as you start to run this notebook. You will see a warning that the kernel has died, this is expected.\n",
    "exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will clone `optimum-habana` repository branch to this docker, let us change to the appropriate directory where our text-to-image generation script and model resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone https://github.com/huggingface/optimum-habana\n",
    "%cd optimum-habana && git checkout v1.16.0\n",
    "%cd ./optimum-habana/examples/text-generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to install all the Python package dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assisted Decoding uses .generate() function from text-generation pipeline, except, whenever assistant_model parameter is provided, it uses the assisted_decoding generation function instead of greedy sampling. \n",
    "\n",
    "It is possible to replace the model_name_or_path and assistant_model parameters with the models of your choice. The model and assistant models must both have the same tokenizer. Depending on the model size and number of parameters associated with them, they can be run on single card or multiple Gaudi2 cards. Below are some examples of model/assistant-model pairs you could try on single Gaudi2: \n",
    "\n",
    "1. gpt2 and distilgpt2\n",
    "2. EleutherAI/pythia-1.4b-deduped and  EleutherAI/pythia-160m-deduped\n",
    "3. meta-llama/Llama-2-7b-hf and TinyLlama/TinyLlama-1.1B-step-50K-105b\n",
    "4. bigcode/starcoder and bigcode/tiny_starcoder_py\n",
    "5. lmsys/vicuna-13b-v1.3 and double7/vicuna-68m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the given command with the provided prompt or replace it with a prompt of your choice. Run the following cell to generate text from provided prompt using speculative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python run_generation.py \\\n",
    "--model_name_or_path gpt2 \\\n",
    "--assistant_model distilgpt2 \\\n",
    "--batch_size 1 \\\n",
    "--max_new_tokens 100 \\\n",
    "--use_hpu_graphs \\\n",
    "--use_kv_cache \\\n",
    "--num_return_sequences 1 \\\n",
    "--temperature 0 \\\n",
    "--prompt \"Alice and Bob\" \\\n",
    "--sdp_on_bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resources running on Intel Gaudi are released \n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "92265e7bf95517031b05ae8ffa1541004d740e0704a96d3b488bf9f3a9b868ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
