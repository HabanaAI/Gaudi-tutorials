{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c2adb6-5a7b-48e6-ab13-1f50f8c18572",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff5422-73e6-4026-b5d2-57ef2e79b10e",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) application using Intel&reg; Gaudi&reg; 2 AI Processor\n",
    "\n",
    "A scalable Retrieval Augmented Generation (RAG) application using huggingface tools as an way of deploying optimized applications utilizing the Intel Gaudi 2 accelerator.\n",
    "\n",
    "### Introduction\n",
    "This tutorial will show how to build a RAG application using Intel Gaudi 2. The Application will be built from easily accessible huggingface tools such as: text-generation-inference (TGI) and text-embeddings-inference (TEI). To make the code easier to understand, Langchain will be used.  The User interface at the end of the tutorial will use Gradio to submit your queries. This application will be in a docker environment, but can be easily deployed to a kubernetes cluster.\n",
    "\n",
    "Retrieval-augmented generation (RAG) is a method that enhances the precision and dependability of generative AI models by incorporating facts from external sources. This technique addresses the limitations of large language models (LLMs), which, despite their ability to generate responses to general prompts rapidly, may not provide in-depth or specific information. By enabling access to external knowledge sources, RAG improves factual consistency, increases the reliability of generated responses, and helps to mitigate the issue of \"hallucination\" in more complex and knowledge-intensive tasks.\n",
    "\n",
    "This Tutorial will show the steps of building the full RAG pipeline on Intel Gaudi 2.  To first ingest a text file and run an embedding model to create a vector store index and database.  Then to start to run a query, it will run the embedding model again on the query, match it with the contents of the database and send the overall prompt and query response to the Falcon 7B Large Language model to generate a full formed response. \n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"./RAG/img/rag-overview.png\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096077b-df4c-4c33-9655-cf2c6b8899d0",
   "metadata": {},
   "source": [
    "## 1. Intial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb693996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the `exit()` command to restart the Python kernel to ensure that there are no other processes holding the Intel Gaudi Accelerator as you start to run this notebook.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a0ea1-09f9-44ca-aff5-d20b21ee995b",
   "metadata": {},
   "source": [
    "#### Setup the docker environment in this notebook:\n",
    "At this point you have cloned the Gaudi-tutorials notebook inside your docker image and have opened this notebook.  Now start to follow the steps.  Note that you will need to install docker again inside the Intel Gaudi container to manage the execution of the RAG tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085eaa61-6cad-4309-9432-f9ae8edff7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS STEP IF YOU ARE RUNNING DIRECLTY FROM THE JUPYTER NOTEBOOKS IN THE Intel Tiber Developer Cloud\n",
    "#!apt-get update\n",
    "#!apt-get install docker.io curl -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c1151-a505-4797-adb7-7582255bb58d",
   "metadata": {},
   "source": [
    "## 2. Loading the Tools for RAG\n",
    "There are three steps in creating the RAG environment, text generation, text embedding and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8617c5-7cc4-4304-9d81-bae2c4e8afbd",
   "metadata": {},
   "source": [
    "### Text Generation Inference (TGI)\n",
    "The first building block of application will be text-generation-inference. Its purpose will be serving the LLM model that will answer a question based on context. To run it, we need to download the docker image:\n",
    "\n",
    "Please note: The Hugging Face Text Generation Inference depends on software that is subject to non-open source licenses.  If you use or redistribute this software, it is your sole responsibility to ensure compliance with such licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c6bad-efcd-4dc9-b7ee-6749e4d5be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull ghcr.io/huggingface/tgi-gaudi:2.0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422efe4-4135-43bb-b36c-0aa5067306b9",
   "metadata": {},
   "source": [
    "### After downloading the image you will run it:\n",
    "\n",
    "#### How to access and use the Falcon 7B Instruct model\n",
    "For the LLM model as part of the Text Generation Inference, the [Falcon 7B instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) model will be used. This model has equivalent performance to Llama based models, but does not require an additional Hugging Face authentication token\n",
    "\n",
    "This docker command will start the Hugging Face TGI service and download the Falcon 7B model.  This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf144b1-7390-4a73-be07-7be1feb683c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 9001:80 \\\n",
    "    --runtime=habana \\\n",
    "    --name gaudi-tgi \\\n",
    "    -e HABANA_VISIBLE_DEVICES=0 \\\n",
    "    -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "    -e HF_TOKEN=\"{hf_token}\" \\\n",
    "    --cap-add=sys_nice \\\n",
    "    --ipc=host \\\n",
    "    ghcr.io/huggingface/tgi-gaudi:2.0.5 \\\n",
    "    --model-id tiiuae/falcon-7b-instruct \\\n",
    "    --max-input-tokens 1024 --max-total-tokens 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d895042",
   "metadata": {},
   "source": [
    "The docker logs command below will show the status of the `gaudi-tgi` docker image. If you do not see `Connected` message like the example below and you see some type of error, you need to stop the docker, remove the docker and run the command above again.\n",
    "\n",
    "To stop and remove the docker, run this command in a terminal window: \n",
    "\n",
    "`docker stop gaudi-tgi && docker rm gaudi-tgi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9901f4a-ed3d-45e3-9c4d-5dac71116ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs gaudi-tgi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db319c26-0252-4114-b209-24df991deee4",
   "metadata": {},
   "source": [
    "After running the docker server, it will take some time to download the model and load it into the device. To check the status run: `docker logs gaudi-tgi` and you should see:\n",
    "\n",
    "```\n",
    "2024-02-23T16:24:35.125179Z  INFO shard-manager: text_generation_launcher: Waiting for shard to be ready... rank=0\n",
    "2024-02-23T16:24:40.729388Z  INFO shard-manager: text_generation_launcher: Shard ready in 65.710470677s rank=0\n",
    "2024-02-23T16:24:40.796775Z  INFO text_generation_launcher: Starting Webserver\n",
    "2024-02-23T16:24:42.589516Z  WARN text_generation_router: router/src/main.rs:355: `--revision` is not set\n",
    "2024-02-23T16:24:42.589551Z  WARN text_generation_router: router/src/main.rs:356: We strongly advise to set it to a known supported commit.\n",
    "2024-02-23T16:24:42.842098Z  INFO text_generation_router: router/src/main.rs:377: Serving revision e852bc2e78a3fe509ec28c6d76512df3012acba7 of model Intel/neural-chat-7b-v3-1\n",
    "2024-02-23T16:24:42.845898Z  INFO text_generation_router: router/src/main.rs:219: Warming up model\n",
    "2024-02-23T16:24:42.846613Z  WARN text_generation_router: router/src/main.rs:230: Model does not support automatic max batch total tokens\n",
    "2024-02-23T16:24:42.846620Z  INFO text_generation_router: router/src/main.rs:252: Setting max batch total tokens to 16000\n",
    "2024-02-23T16:24:42.846623Z  INFO text_generation_router: router/src/main.rs:253: Connected\n",
    "2024-02-23T16:24:42.846626Z  WARN text_generation_router: router/src/main.rs:258: Invalid hostname, defaulting to 0.0.0.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af7431-24af-4a8c-9234-8ccf3b17aaa1",
   "metadata": {},
   "source": [
    "Once the setup is complete, you can verify that that the text generation is working by sending a request to it (note that the first request could be slow due to graph compilation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad5600d-915f-425b-a979-d603c8ea6270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\":\"\\nThe Earth is round because it is the result of the gravitational forces acting on the planet. Gravity pulls everything towards the center of the Earth, causing the planet to become round.\"}"
     ]
    }
   ],
   "source": [
    "!curl 127.0.0.1:9001/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"why is the earth round?\",\"parameters\":{\"max_new_tokens\":200}}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee60a02-7ea9-490b-b0b2-f9f120b26a2a",
   "metadata": {},
   "source": [
    "### Text Embedding Interface (TEI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf035ef-9167-4dad-99d9-10ef906d8de5",
   "metadata": {},
   "source": [
    "The next building block will be text-embeddings-inference. Its purpose will be serving an embeddings model that will produce embeddings for a vector database. We will use the standard Hugging Face text-embeddings-inference (TEI) and run this on the CPU, since we only have access to one Intel Gaudi Card in this notebook.  To run it, we execute the following docker image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b2b90-12de-4441-9c8c-105cb7b8279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 9002:80 \\\n",
    "    --name cpu-tei \\\n",
    "    -v $PWD/RAG/data:/data \\\n",
    "    --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.2 \\\n",
    "    --model-id BAAI/bge-large-en-v1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46fadacb-ccbf-4483-833f-d996b32d0c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-10-22T01:14:57.796889Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/main.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m140:\u001b[0m Args { model_id: \"BAA*/***-*****-**-v1.5\", revision: None, tokenization_workers: None, dtype: None, pooling: None, max_concurrent_requests: 512, max_batch_tokens: 16384, max_batch_requests: None, max_client_batch_size: 32, auto_truncate: false, hf_api_token: None, hostname: \"0505654999f7\", port: 80, uds_path: \"/tmp/text-embeddings-inference-server\", huggingface_hub_cache: Some(\"/data\"), payload_limit: 2000000, api_key: None, json_output: false, otlp_endpoint: None, cors_allow_origin: None }\n",
      "\u001b[2m2024-10-22T01:14:57.797002Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mhf_hub\u001b[0m\u001b[2m:\u001b[0m \u001b[2m/usr/local/cargo/git/checkouts/hf-hub-1aadb4c6e2cbe1ba/b167f69/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m55:\u001b[0m Token file not found \"/root/.cache/huggingface/token\"    \n",
      "\u001b[2m2024-10-22T01:14:58.756011Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload_artifacts\u001b[0m\u001b[2m:\u001b[0m \u001b[2mtext_embeddings_core::download\u001b[0m\u001b[2m:\u001b[0m \u001b[2mcore/src/download.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m20:\u001b[0m Starting download\n",
      "\u001b[2m2024-10-22T01:15:02.495907Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[1mdownload_artifacts\u001b[0m\u001b[2m:\u001b[0m \u001b[2mtext_embeddings_core::download\u001b[0m\u001b[2m:\u001b[0m \u001b[2mcore/src/download.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m37:\u001b[0m Model artifacts downloaded in 3.73989669s\n",
      "\u001b[2m2024-10-22T01:15:02.504879Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m169:\u001b[0m Maximum number of tokens per request: 512\n",
      "\u001b[2m2024-10-22T01:15:02.524453Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_core::tokenization\u001b[0m\u001b[2m:\u001b[0m \u001b[2mcore/src/tokenization.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m23:\u001b[0m Starting 80 tokenization workers\n",
      "\u001b[2m2024-10-22T01:15:02.739598Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m194:\u001b[0m Starting model backend\n",
      "\u001b[2m2024-10-22T01:15:02.740088Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_backend_candle\u001b[0m\u001b[2m:\u001b[0m \u001b[2mbackends/candle/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m124:\u001b[0m Starting Bert model on Cpu\n",
      "\u001b[2m2024-10-22T01:15:03.588273Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_embeddings_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m211:\u001b[0m Backend does not support a batch size > 4\n",
      "\u001b[2m2024-10-22T01:15:03.588296Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_embeddings_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m212:\u001b[0m forcing `max_batch_requests=4`\n",
      "\u001b[2m2024-10-22T01:15:03.588517Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtext_embeddings_router\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/lib.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m263:\u001b[0m Invalid hostname, defaulting to 0.0.0.0\n",
      "\u001b[2m2024-10-22T01:15:03.590490Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_router::http::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/http/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1555:\u001b[0m Starting HTTP server: 0.0.0.0:80\n",
      "\u001b[2m2024-10-22T01:15:03.590500Z\u001b[0m \u001b[32m INFO\u001b[0m \u001b[2mtext_embeddings_router::http::server\u001b[0m\u001b[2m:\u001b[0m \u001b[2mrouter/src/http/server.rs\u001b[0m\u001b[2m:\u001b[0m\u001b[2m1556:\u001b[0m Ready\n"
     ]
    }
   ],
   "source": [
    "!docker logs cpu-tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb11f7-843f-497e-a10a-8e9481538187",
   "metadata": {},
   "source": [
    "Note that here you need also wait for the model to load.  You can run `docker logs cpu-tei` to confirm that the model is setup and running.  To confirm that the model is working, we can send a request to it.   Running the command below will show embeddings of the input prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d780cfd-4688-4d9d-8abc-c0028f345ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 127.0.0.1:9002/embed \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"What is Deep Learning?\"}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d656f11-bee9-4fa6-bb60-e94254ac67e2",
   "metadata": {},
   "source": [
    "### PGVector\n",
    "Third building block is a vector database, in this tutorial the choice was PGVector. Setting up the docker should be straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f514fee-0afc-4b9d-9abc-4373f29f0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull pgvector/pgvector:pg16\n",
    "!docker run \\\n",
    "    --name postgres_vectordb \\\n",
    "    -d \\\n",
    "    -e POSTGRES_PASSWORD=postgres \\\n",
    "    -p 9003:5432 \\\n",
    "    pgvector/pgvector:pg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c6ffb-2560-4f83-b576-615b5d6b247f",
   "metadata": {},
   "source": [
    "Check the status of starting the vectordb container. If successful you should see the message \"database system is ready to accept connections\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7270aba-8dc3-4ee7-807f-36b83cdca319",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs postgres_vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19bca6-2a8d-4b9c-925f-3b8a325cecc1",
   "metadata": {},
   "source": [
    "### Application Front End \n",
    "The last building block will be a frontend that will serve as a http server. The frontend is implemented in python using the Gradio interface. To setup an environment we need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d73d8634-57e6-40c0-a60f-2e05830121f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet -r RAG/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff367a-c01d-4f10-96f8-f9eeb799c9d8",
   "metadata": {},
   "source": [
    "## 3. Data preparation\n",
    "\n",
    "To have a good quality RAG application, we need to prepare data. The data processing stage for vector database will extract text information from documents (for example PDFs, CSVs) then split it into chunks not exceeding max length, with additional metadata (for example filename or file creation date). Then it will upload preprocessed data to the vector database.Â \n",
    "\n",
    "In the process of data preprocessing, text splitting plays a crucial role. It involves breaking down the text into smaller, semantically meaningful chunks for further processing and analysis. Here are some common methods of text splitting:\n",
    "\n",
    "- **By Character**: This method involves splitting the text into individual characters. It's a straightforward approach, but it may not always be the most effective, as it doesn't take into account the semantic meaning of words or phrases.\n",
    "\n",
    "- **Recursive**: Recursive splitting involves breaking down the text into smaller parts repeatedly until a certain condition is met. This method is particularly useful when dealing with complex structures in the text, as it allows for a more granular level of splitting.\n",
    "\n",
    "- **HTML Specific**: When dealing with HTML content, text splitting can be done based on specific HTML tags or elements. This method is useful for extracting meaningful information from web pages or other HTML documents.\n",
    "\n",
    "- **Code Specific**: In the context of programming code, text can be split based on specific code syntax or structures. This method is particularly useful for code analysis or for building tools that work with code.\n",
    "\n",
    "- **By Tokens**: Tokenization is a common method of text splitting in Natural Language Processing (NLP). It involves breaking down the text into individual words or tokens. This method is effective for understanding the semantic meaning of the text, as it allows for the analysis of individual words and their context.\n",
    "\n",
    "In conclusion, the choice of text splitting method depends largely on the nature of the text and the specific requirements of the task at hand. It's important to choose a method that effectively captures the semantic meaning of the text and facilitates further processing and analysis.\n",
    "\n",
    "In this tutorial we will use the **Recursive** method. For better understanding of the topic you can check the https://langchain-text-splitter.streamlit.app/ app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bba2f03-4d3a-4e37-8ab5-c7ae89df93fc",
   "metadata": {},
   "source": [
    "### Database Population\n",
    "Database population is a step where we load documents, embed them and then load into a database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5847ab8c-9959-49ee-8f6b-4125ff78be36",
   "metadata": {},
   "source": [
    "#### Data Loading\n",
    "For ease of use, we'll use helper functions from langchain. Note that langchain_community is also required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45cb0608-0162-4114-91e8-f3e93d1a4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab9197-0d1a-42fd-af24-4cdb6264b2c0",
   "metadata": {},
   "source": [
    "#### Loading Documents with embeddings\n",
    "Here we need to create huggingface TEI client and PGVector client. For PGVector, collection name corresponds to table name, within connection string there is connection protocol: `postgresql+psycopg2`, next is user, password, host, port and database name. For ease of use, pre_delete_collection is set to true to prevent duplicates in database.   \n",
    "\n",
    "**NOTE: the first time this is run, it may show a message \"Collection not found\", this is expected.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591325c-ab5d-495c-a012-1a82fbe6b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEndpointEmbeddings(model=\"http://localhost:9002\", huggingfacehub_api_token=\"EMPTY\")\n",
    "store = PGVector(\n",
    "    collection_name=\"documents\",\n",
    "    connection=\"postgresql+psycopg2://postgres:postgres@localhost:9003/postgres\",\n",
    "    embeddings=embeddings,\n",
    "    pre_delete_collection=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea35e8-ed40-4bba-a748-732d0a25efab",
   "metadata": {},
   "source": [
    "#### Data Loading and Splitting\n",
    "Data is loaded from text files from `data/`, then documents are splitted into chunks of 512 characters in size and finally loaded into the database. Note that documents can have metadata that can be stored in the vector database. \n",
    "\n",
    "You can load a new text file in the `data/` folder to run the RAG pipeline on new content by running the following cell again with new data.  This cell will create a new Database to run your query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc4db026-a84c-4f3c-9b4a-0e6985e3b01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAG/data/state_of_the_union.txt...\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "def load_file_to_db(path: str, store: PGVector):\n",
    "    loader = TextLoader(path)\n",
    "    document = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "    for chunk in text_splitter.split_documents(document):\n",
    "        store.add_documents([chunk])\n",
    "\n",
    "for doc in Path(\"RAG/data/\").glob(\"*.txt\"):\n",
    "    print(f\"Loading {doc}...\")\n",
    "    load_file_to_db(str(doc), store)\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b623d46-8c10-40a7-bacf-ee6c7d7511db",
   "metadata": {},
   "source": [
    "## 4. Running the Application\n",
    "To start the application run the following commands below to setup the functions to call the TGI, TEI and Vector databases.  \n",
    "Load a text file in the `./RAG/data` folder and then run the cell above and the application will ingest and start the chat application to ask a question to the document.  \n",
    "You will see that it's directly accessing the TGI and TEI libraries to ingest, create the embeddings and vector database, then run the query through the database and then use the LLM to generate an answer to your query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198187a7-6c65-4048-a8a9-10758dd6c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from text_generation import Client\n",
    "\n",
    "rag_prompt_intel_raw = \"\"\"### System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. \n",
    "\n",
    "### User: Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "### Assistant: \"\"\"\n",
    "\n",
    "def get_sources(question):\n",
    "    embeddings = HuggingFaceEndpointEmbeddings(model=\"http://localhost:9002\", huggingfacehub_api_token=\"EMPTY\")\n",
    "    store = PGVector(\n",
    "        collection_name=\"documents\",\n",
    "        connection_string=\"postgresql+psycopg2://postgres:postgres@localhost:9003/postgres\",\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "    return store.similarity_search(f\"Represent this sentence for searching relevant passages: {question}\", k=2)\n",
    "\n",
    "def sources_to_str(sources):\n",
    "    return \"\\n\".join(f\"{i+1}. {s.page_content}\" for i, s in enumerate(sources))\n",
    "\n",
    "def get_answer(question, sources):\n",
    "    client = Client(\"http://localhost:9001\") #change this to 9009 for the new model\n",
    "    context = \"\\n\".join(s.page_content for s in sources)\n",
    "    prompt = rag_prompt_intel_raw.format(question=question, context=context)\n",
    "    return client.generate(prompt, max_new_tokens=1024, stop_sequences=[\"### User:\", \"</s>\"]).generated_text\n",
    "\n",
    "default_question = \"What is this the summary of this document?\"\n",
    "\n",
    "def rag_answer(question, history):\n",
    "    question = question[\"text\"]\n",
    "    sources = get_sources(question)\n",
    "    answer = get_answer(question, sources)\n",
    "    return f\"{answer}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459095e",
   "metadata": {},
   "source": [
    "### Run the UI to query the RAG\n",
    "This sets up a simple UI to submit queries to the RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Define colors\n",
    "INTEL_BLUE = \"#0071C5\"\n",
    "LIGHT_BLUE = \"#5DADEC\"\n",
    "\n",
    "# Create UI elements\n",
    "title = widgets.HTML(\n",
    "    value=\"<h1 style='color:{}; text-align:center;'>Question Answering Mini App</h1>\".format(INTEL_BLUE)\n",
    ")\n",
    "question_input = widgets.Textarea(\n",
    "    placeholder='Enter your question here',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='100px'),\n",
    "    style={'button_color': INTEL_BLUE}\n",
    ")\n",
    "output_area = widgets.Output()\n",
    "loading_spinner = widgets.HTML(\n",
    "    value=\"<i class='fa fa-spinner fa-spin' style='font-size:24px; color:{}'></i>\".format(INTEL_BLUE),\n",
    "    layout=widgets.Layout(display='none')\n",
    ")\n",
    "\n",
    "# Define the interaction function\n",
    "def on_submit_button_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        display(loading_spinner)\n",
    "        loading_spinner.layout.display = 'inline-block'\n",
    "        \n",
    "        def process_question():\n",
    "            try:\n",
    "                question = question_input.value\n",
    "                history = []  # Assuming history is empty for simplicity\n",
    "                answer = rag_answer({\"text\": question}, history)\n",
    "                with output_area:\n",
    "                    loading_spinner.layout.display = 'none'\n",
    "                    output_area.append_stdout(f\"Answer:\\n{answer}\\n\")\n",
    "            except Exception as e:\n",
    "                with output_area:\n",
    "                    loading_spinner.layout.display = 'none'\n",
    "                    output_area.append_stdout(f\"Error: {str(e)}\\n\")\n",
    "        \n",
    "        thread = threading.Thread(target=process_question)\n",
    "        thread.start()\n",
    "\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "# Display the UI elements\n",
    "display(\n",
    "    widgets.VBox([\n",
    "        title,\n",
    "        question_input,\n",
    "        submit_button,\n",
    "        loading_spinner,\n",
    "        output_area\n",
    "    ])\n",
    ")\n",
    "\n",
    "# HTML to enhance loading spinner style\n",
    "display(HTML('''\n",
    "<style>\n",
    "    @keyframes spinner {\n",
    "        to {transform: rotate(360deg);}\n",
    "    }\n",
    "    .fa-spinner {\n",
    "        margin: 0 auto;\n",
    "        display: block;\n",
    "        animation: spinner 1s linear infinite;\n",
    "    }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d96277-7f61-408b-a296-5977afb52f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resorces running on Intel Gaudi are released and the Docker images are deleted \n",
    "!docker stop gaudi-tgi && docker rm gaudi-tgi\n",
    "!docker stop cpu-tei && docker rm cpu-tei\n",
    "!docker stop postgres_vectordb && docker rm postgres_vectordb\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
