{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00676b-8d19-4e0d-b61d-68ccc14c294d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714683-1b37-4048-bf91-2b5af1c18d40",
   "metadata": {},
   "source": [
    "# Intel® Gaudi® Accelerator Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db2bc-12ef-47a0-83f8-cfba2a8af03c",
   "metadata": {},
   "source": [
    "\n",
    "This example demonstrates a few simple ways to run models on Intel(R) Gaudi accelerators, using the Intel Gaudi [Model References](https://github.com/HabanaAI/Model-References) and the Hugging Face [Optimum Habana Library](https://github.com/huggingface/optimum-habana).\n",
    "\n",
    "If you would like to follow along with a video walkthrough of setting up a node and running the examples in this document, click [here](https://developer.habana.ai/intel-developer-cloud/).\n",
    "\n",
    "For more information about Intel Gaudi hardware and software, please see the [documentation](https://docs.habana.ai/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722f072",
   "metadata": {},
   "source": [
    "## Running on Intel Gaudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafa71e",
   "metadata": {},
   "source": [
    "There are three ways to run models on Gaudi:\n",
    "1. Using complete, optimized models available in the Intel Gaudi Model References (https://github.com/HabanaAI/Model-References).\n",
    "2. Using the Hugging Face Optimum Habana Library (https://github.com/huggingface/optimum-habana) for transformer-based models.\n",
    "3. Using the GPU Migration Tool (https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html) to migrate a public model to Gaudi.\n",
    "\n",
    "In this notebook, we will step through examples of the first two options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf8b87e",
   "metadata": {},
   "source": [
    "### Using a Model from the Model References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d0051",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to install the datasets library and Model-References repository from GitHub and run the \"hello-world\" model from the examples library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "exit()   # need to restart the kernel to ensure that datasets directory is intialized properly. You will see a warning that the kernel has died, this is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca618d-e3bc-4c36-998d-28ec94b10b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone -b 1.21.0 https://github.com/HabanaAI/Model-References.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76764c-9a4d-4d44-92f9-d9dfee9906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Model-References/PyTorch/examples/computer_vision/hello_world/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de0ff4",
   "metadata": {},
   "source": [
    "We now run the simple example with the MNIST dataset on one Intel Gaudi card.   Please note that the download of the MNIST dataset will be from an Amazon AWS shared storage and not from the yann.lecun.com. The HTTP error associated with the download is not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc8e49-f883-4162-a74a-bdeb7d309598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mnist.py --batch-size=64 --epochs=1 --lr=1.0 --gamma=0.7 --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37216139-611e-4b07-a90f-cb351f18b185",
   "metadata": {},
   "source": [
    "### Fine-tuning with Hugging Face Optimum Habana Library\n",
    "The Optimum Habana library is the interface between the Hugging Face Transformers and Diffusers libraries and the Gaudi 2 card. It provides a set of tools enabling easy model loading, training and inference on single and multi-card settings for different downstream tasks. The following example uses the text-classification task to fine-tune a BERT-Large model with the MRPC (Microsoft Research Paraphrase Corpus) dataset and also run Inference.\n",
    "\n",
    "Follow the below steps to install the stable release from the Optimum Habana examples and library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420be01-fb88-466a-9fe7-87006340905f",
   "metadata": {},
   "source": [
    "1. Clone the Optimum-Habana project and check out the latest stable release.  This repository gives access to the examples that are optimized for Intel Gaudi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07346c1a-1fea-4b62-8a79-760ca7a64073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.16.0 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ded02-2aa1-4725-b1fc-cf917e05d9aa",
   "metadata": {},
   "source": [
    "2. Install Optimum-Habana library. This will install the latest stable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e35cc-0ea1-4205-ae70-323252169c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optimum-habana==1.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f0546-e74c-4363-b443-a0f59504d973",
   "metadata": {},
   "source": [
    "The following example is based on the Optimum-Habana Text Classification task example. Change to the text-classification directory and install the additional software requirements for this specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24537f40-8daa-4ac9-ad19-bf1cfaaf29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/text-classification\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~/optimum-habana/examples/text-classification/\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88922f6b-527f-4bc0-bb47-4adea631ef9c",
   "metadata": {},
   "source": [
    "### Execute Single-Card Training\n",
    "This run instruction will fine-tune the BERT-Large Model on one Intel Gaudi card:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305374a-21fe-4376-ab39-0c6de4222eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc   \\\n",
    "--do_train   \\\n",
    "--do_eval   \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128   \\\n",
    "--output_dir ./output/mrpc/  \\\n",
    "--use_habana  \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16   \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir \\\n",
    "--throughput_warmup_steps 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23453-cbec-4582-98dd-0dc202499da7",
   "metadata": {},
   "source": [
    "### Inference Example Run\n",
    "Using inference will run the same evaluation metrics (accuracy, F1 score) as shown above. This will display how well the model has performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c55ed-33dd-4b0a-9fe5-ecfab8ba51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_eval \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ./output/mrpc/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae666d1-7e11-43fe-a73a-0f1edf047eb5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You now have access to all the Models in Model-References and Optimum-Habana repositories, you can start to look at other models.  Remember that all the models in these repositories are fully documented so they are easy to use.\n",
    "* To explore more models from the Model References, start [here](https://github.com/HabanaAI/Model-References).  \n",
    "* To run more examples using Hugging Face go [here](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models).  \n",
    "* To migrate other models to Gaudi 2, refer to PyTorch Model Porting in the [documentation](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a7a4-214e-4b1b-8ec8-5a202758709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resources running on Intel Gaudi are released \n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
