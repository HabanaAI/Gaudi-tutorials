{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00676b-8d19-4e0d-b61d-68ccc14c294d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714683-1b37-4048-bf91-2b5af1c18d40",
   "metadata": {},
   "source": [
    "# Intel® Gaudi® Accelerator Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db2bc-12ef-47a0-83f8-cfba2a8af03c",
   "metadata": {},
   "source": [
    "\n",
    "This document provides instructions on setting up the Intel Gaudi 2 AI accelerator Instance on the Intel® Tiber&trade; Developer Cloud or any on-premise Intel Gaudi Node. You will be running models from the Intel Gaudi software Model References and the Hugging Face Optimum Habana library.\n",
    "\n",
    "Please follow along with the [video](https://developer.habana.ai/intel-developer-cloud/) on our Developer Page to walk through the steps below.  This assumes that you have setup the latest Intel Gaudi PyTorch Docker image.\n",
    "\n",
    "To set up a multi-node instance with two or more Gaudi nodes, refer to Setting up Multiple Gaudi Nodes in the [Quick Start Guide Documentation](https://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodeshttps://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodes).  \n",
    "\n",
    "The first step is to install the datasets library and Model-References repository from GitHub and run the \"hello-world\" model from the examples library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "exit()   # need to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca618d-e3bc-4c36-998d-28ec94b10b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone -b 1.15.1 https://github.com/HabanaAI/Model-References.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76764c-9a4d-4d44-92f9-d9dfee9906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Model-References/PyTorch/examples/computer_vision/hello_world/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de0ff4",
   "metadata": {},
   "source": [
    "We now run the simple example with the MNIST dataset on one Intel Gaudi card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc8e49-f883-4162-a74a-bdeb7d309598",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run mnist.py --batch-size=64 --epochs=1 --lr=1.0 --gamma=0.7 --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37216139-611e-4b07-a90f-cb351f18b185",
   "metadata": {},
   "source": [
    "### Fine-tuning with Hugging Face Optimum Habana Library\n",
    "The Optimum Habana library is the interface between the Hugging Face Transformers and Diffusers libraries and the Gaudi 2 card. It provides a set of tools enabling easy model loading, training and inference on single and multi-card settings for different downstream tasks. The following example uses the text-classification task to fine-tune a BERT-Large model with the MRPC (Microsoft Research Paraphrase Corpus) dataset and also run Inference.\n",
    "\n",
    "Follow the below steps to install the stable release from the Optimum Habana examples and library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420be01-fb88-466a-9fe7-87006340905f",
   "metadata": {},
   "source": [
    "1. Clone the Optimum-Habana project and check out the lastest stable release.  This repository gives access to the examples that are optimized for Intel Gaudi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07346c1a-1fea-4b62-8a79-760ca7a64073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone -b v1.11.1 https://github.com/huggingface/optimum-habana.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ded02-2aa1-4725-b1fc-cf917e05d9aa",
   "metadata": {},
   "source": [
    "2. Install Optimum-Habana library. This will install the latest stable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e35cc-0ea1-4205-ae70-323252169c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum-habana==1.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f0546-e74c-4363-b443-a0f59504d973",
   "metadata": {},
   "source": [
    "The following example is based on the Optimum-Habana Text Classification task example. Change to the text-classification directory and install the additional software requirements for this specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24537f40-8daa-4ac9-ad19-bf1cfaaf29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-classification\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-classification/\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88922f6b-527f-4bc0-bb47-4adea631ef9c",
   "metadata": {},
   "source": [
    "### Execute Single-Card Training\n",
    "This run instruction will fine-tune the BERT-Large Model on one Intel Gaudi card:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305374a-21fe-4376-ab39-0c6de4222eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc   \\\n",
    "--do_train   \\\n",
    "--do_eval   \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128   \\\n",
    "--output_dir ./output/mrpc/  \\\n",
    "--use_habana  \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16   \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir \\\n",
    "--throughput_warmup_steps 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23453-cbec-4582-98dd-0dc202499da7",
   "metadata": {},
   "source": [
    "### Inference Example Run\n",
    "Using inference will run the same evaluation metrics (accuracy, F1 score) as shown above. This will display how well the model has performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c55ed-33dd-4b0a-9fe5-ecfab8ba51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_eval \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ./output/mrpc/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae666d1-7e11-43fe-a73a-0f1edf047eb5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You now have access to all the Models in Model-References and Optimum-Habana repositories, you can start to look at other models.  Remember that all the models in these repositories are fully documented so they are easy to use.\n",
    "* To explore more models from the Model References, start [here](https://github.com/HabanaAI/Model-References).  \n",
    "* To run more examples using Hugging Face go [here](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models).  \n",
    "* To migrate other models to Gaudi 2, refer to PyTorch Model Porting in the [documentation](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a7a4-214e-4b1b-8ec8-5a202758709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resorces running on Intel Gaudi are released \n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
