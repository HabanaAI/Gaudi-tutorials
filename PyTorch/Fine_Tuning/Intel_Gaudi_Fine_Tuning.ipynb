{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9599e2f3-6b9c-4578-9501-7d5c65df408a",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d33aa-8f5b-4de3-bdaa-b2bfac88e1e8",
   "metadata": {},
   "source": [
    "# Fine Tuning Use Case on Intel® Gaudi® 2 AI Accelerator\n",
    "show how to run a typical model Fine Tuning use case on the Intel Gaudi Accelerator.  You will see how to select a model, setup the environment, execute the workload.  Intel Gaudi supports PyTorch as the main framework for Fine Tuning.  \n",
    "\n",
    "This example will Fine Tune the Llama 3 70B model using Parameter Efficient Fine Tuining (PEFT) \n",
    "\n",
    "In this example, you will see how to select a model, setup the environment, execute the workload and then see a price-performance comparison.   Intel Gaudi supports PyTorch as the main framework for Inference.  \n",
    "\n",
    "Running Fine Tuning on the Intel Gaudi Accelerator is quite simple, and the code below will take you step-by-step through all the items needed, in summary here:  \n",
    "\n",
    "•\tGet Access to an Intel Gaudi node, using the Intel® Tiber™ Developer Cloud is recommended.  \n",
    "•\tRun the Intel Gaudi PyTorch Docker image; this ensures that all the SW is installed and configured properly.  \n",
    "•\tSelect the model for execution by loading the desired Model Repository and appropriate libraries for model acceleration.   \n",
    "•\tRun the model and extract the details for evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051c556",
   "metadata": {},
   "source": [
    "### Accessing The Intel Gaudi Node\n",
    "To access an Intel Gaudi node in the Intel Tiber Developer cloud, you will go to [Intel Developer Cloud Console](https://console.cloud.intel.com/hardware) and access the hardware instances to select the Intel® Gaudi® 2 platform for deep learning and follow the steps to start and connect to the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2c8e9",
   "metadata": {},
   "source": [
    "\n",
    "### Docker Setup\n",
    "Now that you have access to the node, you will use the latest Intel Gaudi docker image by first calling the docker run command which will automatically download and run the docker:\n",
    "\n",
    "```\n",
    "docker run -itd --name Gaudi_Docker --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest\n",
    "```\n",
    "\n",
    "We then start the docker and enter the docker environment by issuing the following command: \n",
    "```\n",
    "docker exec -it Gaudi_Docker bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba423dc7",
   "metadata": {},
   "source": [
    "### Model Setup \n",
    "Now that we’re running in a docker environment, we can now install the remaining libraries and model repositories:\n",
    "Install the DeepSpeed Library; DeepSpeed is used to improve memory consumption on Intel Gaudi while running large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7533c24",
   "metadata": {},
   "source": [
    "Now go to the root directory and clone the GitHub Examples and install the Hugging Face Optimum Habana library. Notice that we’re selecting the latest validated release of Optimum Habana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b601dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.14.1 https://github.com/huggingface/optimum-habana\n",
    "!pip install optimum-habana==1.14.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af7756",
   "metadata": {},
   "source": [
    "Finally, we transition to the language example and install the final set of requirements to run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f01637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/language-modeling\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce32ad2",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama 3.1 model\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “META LLAMA 3 COMMUNITY LICENSE AGREEMENT”. For guidance on the intended use of the LLAMA 3 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://llama.meta.com/llama3/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses. To be able to run gated models like this Llama-3.1-70b, you need the following:\n",
    "\n",
    "•\tHave a HuggingFace account and agree to the terms of use of the model in its model card on the HF Hub  \n",
    "•\tCreate a read token and request access to the Llama 3.1 model from meta-llama  \n",
    "•\tLogin to your account using the HF CLI:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <YOUR HUGGINGFACE TOKEN HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c887f9a",
   "metadata": {},
   "source": [
    "### Fine Tuning a simple GPT model\n",
    "Let’s start with a simple example of fine tuning from the Hugging Face language modeling page.   This is using the wikitext dataset to fine tune the gpt2 model.  The fine tuning of this model takes only a few minutes and you can see the fine tuned model output in the `test_clm` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65176b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 run_clm.py \\\n",
    "  --model_name_or_path gpt2 \\\n",
    "  --dataset_name wikitext \\\n",
    "  --dataset_config_name wikitext-2-raw-v1 \\\n",
    "  --per_device_train_batch_size 4 \\\n",
    "  --per_device_eval_batch_size 4 \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --overwrite_output_dir \\\n",
    "  --report_to none \\\n",
    "  --output_dir ./test-clm \\\n",
    "  --gaudi_config_name Habana/gpt2 \\\n",
    "  --use_habana \\\n",
    "  --use_lazy_mode \\\n",
    "  --use_hpu_graphs \\\n",
    "  --throughput_warmup_steps 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587dfce6",
   "metadata": {},
   "source": [
    "### Fine Tuning the Llama 3.1 70B Model \n",
    "We’re now ready to start running the full Llama 3.1 70 model for fine tuning.  Since the Llama 3.1 70B is a large model, we’ll employ the DeepSpeed library to more efficiently manage the memory usage of the local HBM memory on each Intel Gaudi card. We’ll deploy some additional techniques for Fine Tuning:  \n",
    "\n",
    "•\tParameter Efficient Fine Tuning (PEFT) is a strategy for adapting large pre-trained language models to specific tasks.  Instead of fine-tuning the entire pre-trained model, PEFT adds a task-specific layer or a few task-specific layers on top of the pre-trained model. These additional layers are relatively smaller and have fewer parameters compared to the base model.  \n",
    "•\tDeepSpeed significantly optimizes training efficiency, reducing both computational and memory requirements. It enables the handling of extremely large models by providing advanced parallelism techniques and memory optimization strategies  \n",
    "•\tFlash Attention is used to reduce memory usage and enhancing computational speed through a fused implementation.  This includes the use of the FusedSDPA (Scaled Dot Product Attention) applies similar principles to the Gaudi processor environment, optimizing the scaled dot product attention function with reduced memory usage and faster performance while maintaining compatibility with standard PyTorch functionality.  \n",
    "•\tSetting epochs = 2; this is enough to ensure that the training loss is below 1.0, running any more epoch is not needed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a264c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PT_HPU_MAX_COMPOUND_OP_SIZE=10 DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED=1 \\\n",
    "python3 ../gaudi_spawn.py --use_deepspeed  --world_size 8  run_lora_clm.py \\\n",
    "  --model_name_or_path meta-llama/Llama-3.1-70B-Instruct \\\n",
    "  --deepspeed llama2_ds_zero3_config.json \\\n",
    "  --dataset_name tatsu-lab/alpaca \\\n",
    "  --bf16 True \\\n",
    "  --output_dir ./llama3_1_fine_tuning_output \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_len 2048 \\\n",
    "  --per_device_train_batch_size 10 \\\n",
    "  --per_device_eval_batch_size 10 \\\n",
    "  --gradient_checkpointing \\\n",
    "  --evaluation_strategy epoch \\\n",
    "  --eval_delay 2 \\\n",
    "  --save_strategy no \\\n",
    "  --learning_rate 0.0018 \\\n",
    "  --warmup_ratio 0.03 \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --logging_steps 1 \\\n",
    "  --dataset_concatenation \\\n",
    "  --attn_softmax_bf16 True \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --use_habana \\\n",
    "  --use_lazy_mode \\\n",
    "  --pipelining_fwd_bwd \\\n",
    "  --throughput_warmup_steps 3 \\\n",
    "  --report_to none \\\n",
    "  --lora_rank 4 \\\n",
    "  --lora_target_modules \"q_proj\" \"v_proj\" \"k_proj\" \"o_proj\" \\\n",
    "  --validation_split_percentage 4 \\\n",
    "  --use_flash_attention True \\\n",
    "  --flash_attention_causal_mask True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc88f55",
   "metadata": {},
   "source": [
    "The result of the run shows that the Fine Tuning of the model required only 38 minutes and achieved 2.2 samples (or sentences) per second.\n",
    "```\n",
    "***** train metrics *****\n",
    "  epoch                       =        2.0\n",
    "  max_memory_allocated (GB)   =      94.53\n",
    "  memory_allocated (GB)       =      27.15\n",
    "  total_flos                  =  1037280GF\n",
    "  total_memory_available (GB) =      94.62\n",
    "  train_loss                  =     1.1525\n",
    "  train_runtime               = 0:38:47.30\n",
    "  train_samples_per_second    =      2.221\n",
    "  train_steps_per_second      =      0.028\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b497c2",
   "metadata": {},
   "source": [
    "See the output in the llama3_fine_tuning_output folder, you will see the resulting files from Fine Tuning.  The full model is the `adapter_model.safetensors` which contains the additional weights generated by the Parameter Efficient Fine Tuning.  These weights can used for Inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd llama3_1_fine_tuning_output\n",
    "%ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19f978",
   "metadata": {},
   "source": [
    "In this case this Fine Tuned model can now be applied to an inference use case like text-generation where you can use this fine tuned model. \n",
    "\n",
    "### Next Steps \n",
    "Now that you have run a full inference case, you can go back to the Hugging Face Optimum Habana validated models to see more options for running inference with this model or Fine Tune other models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e1747-57ea-44aa-a564-4af8d303d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
