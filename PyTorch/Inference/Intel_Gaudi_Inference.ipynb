{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9599e2f3-6b9c-4578-9501-7d5c65df408a",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d33aa-8f5b-4de3-bdaa-b2bfac88e1e8",
   "metadata": {},
   "source": [
    "# Inference on the Intel&reg; Gaudi&reg; 2 AI Accelerator\n",
    "This example will show how to run model inference on the Llama 2 70B model using the Hugging Face Optimum Habana library.   The Optimum Habana library is optimized for Deep Learning inference on tasks such as text generation, language modeling, question answering and more; these contain fully optimized and fully documented model examples and should be used as a starting point for model execution.  For all the examples and models, please refer to the [Optimum Habana GitHub](https://github.com/huggingface/optimum-habana#validated-models).  \n",
    "\n",
    "In this example, you will see how to select a model, setup the environment, execute the workload and then see a price-performance comparison.   Intel Gaudi supports PyTorch as the main framework for Inference.  \n",
    "\n",
    "Running inference on the Intel Gaudi Accelerator is quite simple, and the code below will take you step-by-step through all the items needed, in summary here:  \n",
    "\n",
    "•\tGet Access to an Intel Gaudi node, using the Intel® Tiber™ Developer Cloud is recommended.  \n",
    "•\tRun the Intel Gaudi PyTorch Docker image; this ensures that all the SW is installed and configured properly.  \n",
    "•\tSelect the model for execution by loading the desired Model Repository and appropriate libraries for model acceleration.   \n",
    "•\tRun the model and extract the details for evaluation.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e160",
   "metadata": {},
   "source": [
    "### Accessing The Intel Gaudi Node\n",
    "To access an Intel Gaudi node in the Intel Tiber Developer cloud, you will go to [Intel Developer Cloud Console](https://console.cloud.intel.com/hardware) and access the hardware instances to select the Intel® Gaudi® 2 platform for deep learning and follow the steps to start and connect to the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d79263",
   "metadata": {},
   "source": [
    "\n",
    "### Docker Setup\n",
    "Now that you have access to the node, you will use the latest Intel Gaudi docker image by first calling the docker run command which will automatically download and run the docker:\n",
    "\n",
    "```\n",
    "docker run -itd --name Gaudi_Docker --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest\n",
    "```\n",
    "\n",
    "We then start the docker and enter the docker environment by issuing the following command: \n",
    "```\n",
    "docker exec -it Gaudi_Docker bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2237ab3",
   "metadata": {},
   "source": [
    "### Model Setup \n",
    "Now that we’re running in a docker environment, we can now install the remaining libraries and model repositories:\n",
    "Start in the root directory and install the DeepSpeed Library; DeepSpeed is used to improve memory consumption on Intel Gaudi while running large language models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0750c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b669e1c",
   "metadata": {},
   "source": [
    "Now install the Hugging Face Optimum Habana library and GitHub Examples, notice that we’re selecting the latest validated release of Optimum Habana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.14.1 https://github.com/huggingface/optimum-habana\n",
    "!pip install optimum-habana==1.14.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50cdb2",
   "metadata": {},
   "source": [
    "Finally, we transition to the text-generation example and install the final set of requirements to run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61daaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/text-generation\n",
    "!pip install -r requirements.txt\n",
    "!pip install -r requirements_lm_eval.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9cc46",
   "metadata": {},
   "source": [
    "### How to access and Use the Llama 2 model\n",
    "Use of the model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses. \n",
    "To be able to run gated models like this Llama-2-70b-hf, you need the following:   \n",
    "\n",
    "•\tHave a HuggingFace account and agree to the terms of use of the model in its model card on the HF Hub  \n",
    "•\tCreate a read token and request access to the Llama 2 model from meta-llama  \n",
    "•\tLogin to your account using the HF CLI:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <YOUR HUGGINGFACE HUB TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbd059",
   "metadata": {},
   "source": [
    "### Running the Llama 2 70B Model using the BF16 Datatype\n",
    "We’re now ready to start running the model for inference.  In this first example, we’ll start with the standard inference example using BF16.  Since the Llama 2 70B is a large model, we’ll employ the DeepSpeed library with a set of default settings to more efficiently manage the memory usage of the local HBM memory on each Intel Gaudi card: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689e5f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt for text generation:  who is president of usa\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a prompt for text generation: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../gaudi_spawn.py --use_deepspeed --world_size 8 run_generation.py \\\n",
    "--model_name_or_path meta-llama/Llama-2-70b-hf \\\n",
    "--max_new_tokens 1024 \\\n",
    "--bf16 \\\n",
    "--use_hpu_graphs \\\n",
    "--use_kv_cache \\\n",
    "--batch_size 1 \\\n",
    "--attn_softmax_bf16 \\\n",
    "--limit_hpu_graphs \\\n",
    "--reuse_cache \\\n",
    "--trim_logits \\\n",
    "--prompt \"{prompt}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2e563",
   "metadata": {},
   "source": [
    "You will see the output at the end of the run showing the througput, memory usage and graph compilation time.  You can refer to the Readme of the [text-generation task example](https://github.com/huggingface/optimum-habana/tree/v1.12.0/examples/text-generation) for more options for running inference with Llama 2 and other Large Language Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c6172",
   "metadata": {},
   "source": [
    "## Running the Llama 2 70B Model using the FP8 Datatype\n",
    "Now we’ll now be using the FP8 datatype.  Using FP8 can give significantly better performance as compared to BF16.  The first step is to run quantization measurement.  To learn more about Intel Gaudi FP8 quantization, you can refer to the [user guide](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_FP8.html).  This is provided by running the local quantization tool using the maxabs_measure.json file that is already loaded on the Hugging Face GitHub library: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404692f3-1266-4dcb-8885-ec6016aa5bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!QUANT_CONFIG=./quantization_config/maxabs_measure.json TQDM_DISABLE=1 \\\n",
    "python3 ../gaudi_spawn.py --use_deepspeed --world_size 4 \\\n",
    "run_lm_eval.py --model_name_or_path meta-llama/Llama-2-70b-hf \\\n",
    "-o acc_70b_bs1_measure4.txt \\\n",
    "--attn_softmax_bf16 \\\n",
    "--use_hpu_graphs \\\n",
    "--trim_logits \\\n",
    "--use_kv_cache \\\n",
    "--bucket_size=128 \\\n",
    "--bucket_internal \\\n",
    "--bf16 \\\n",
    "--batch_size 1 \\\n",
    "--use_flash_attention \\\n",
    "--flash_attention_recompute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f258a8c",
   "metadata": {},
   "source": [
    "This generates a set of measurement values in a folder called `hqt_output` that will show what ops have been converted to the FP8 datatype. \n",
    "\n",
    "```\n",
    "-rw-r--r--  1 root root 347867 Jul 13 07:52 measure_hooks_maxabs_0_4.json\n",
    "-rw-r--r--  1 root root 185480 Jul 13 07:52 measure_hooks_maxabs_0_4.npz\n",
    "-rw-r--r--  1 root root  40297 Jul 13 07:52 measure_hooks_maxabs_0_4_mod_list.json\n",
    "-rw-r--r--  1 root root 347892 Jul 13 07:52 measure_hooks_maxabs_1_4.json\n",
    "-rw-r--r--  1 root root 185480 Jul 13 07:52 measure_hooks_maxabs_1_4.npz\n",
    "-rw-r--r--  1 root root  40297 Jul 13 07:52 measure_hooks_maxabs_1_4_mod_list.json\n",
    "-rw-r--r--  1 root root 347903 Jul 13 07:52 measure_hooks_maxabs_2_4.json\n",
    "-rw-r--r--  1 root root 185480 Jul 13 07:52 measure_hooks_maxabs_2_4.npz\n",
    "-rw-r--r--  1 root root  40297 Jul 13 07:52 measure_hooks_maxabs_2_4_mod_list.json\n",
    "-rw-r--r--  1 root root 347880 Jul 13 07:52 measure_hooks_maxabs_3_4.json\n",
    "-rw-r--r--  1 root root 185480 Jul 13 07:52 measure_hooks_maxabs_3_4.npz\n",
    "-rw-r--r--  1 root root  40297 Jul 13 07:52 measure_hooks_maxabs_3_4_mod_list.json\n",
    "```\n",
    "We now can use these measurements to run the throughput execution of the model.   In this case a standard input prompt is used.  You will notice that the quantization .json config file is now used (instead of the measurement file) and additional input and output parameters are added.  In this case you will see `--max_new_tokens 2048` which determines the size of the output text generated, and `-max_input_tokens 128`  which defines the size of the number of input tokens.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!QUANT_CONFIG=./quantization_config/maxabs_quant.json TQDM_DISABLE=1 \\\n",
    "python3 ../gaudi_spawn.py --use_deepspeed --world_size 4 \\\n",
    "run_generation.py --model_name_or_path meta-llama/Llama-2-70b-hf \\\n",
    "--attn_softmax_bf16 \\\n",
    "--use_hpu_graphs \\\n",
    "--trim_logits \\\n",
    "--use_kv_cache \\\n",
    "--bucket_size=128 \\\n",
    "--bucket_internal \\\n",
    "--max_new_tokens 2048 \\\n",
    "--max_input_tokens 128 \\\n",
    "--bf16 \\\n",
    "--batch_size 130  \\\n",
    "--use_flash_attention \\\n",
    "--flash_attention_recompute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98217bb7",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "Now that you have run a full inference case, you can go back to the Hugging Face Optimum Habana [validated models](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models) to see more options for running inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070789b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
