{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c051f912",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b6012",
   "metadata": {},
   "source": [
    "# Inference on Gaudi - Example2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e4869",
   "metadata": {},
   "source": [
    "This notebook is used as an example to show inference on the Gaudi Accelerator. This is using a simple model and the MNIST dataset.\n",
    "\n",
    "This tutorial will show how to infer an MNIST model with HPU GRAPH using HPU Graph APIs and Stream APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d94b8",
   "metadata": {},
   "source": [
    "Download pretrained model checkpoints from vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c83a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://vault.habana.ai/artifactory/misc/inference/mnist/mnist-epoch_20.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032945e",
   "metadata": {},
   "source": [
    "Import all neccessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64868a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Enable PT_HPU_LAZY_MODE=1\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '1'\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import habana_frameworks.torch as ht\n",
    "import habana_frameworks.torch.core as htcore\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943d83d",
   "metadata": {},
   "source": [
    "Define a simple Net model for MNIST.\n",
    "Create the model, and load the pre-trained checkpoint.\n",
    "Optimize the model for eval, and move the model to Gaudi(hpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f2a4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1   = nn.Linear(784, 256)\n",
    "        self.fc2   = nn.Linear(256, 64)\n",
    "        self.fc3   = nn.Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1,28*28)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6819209c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "checkpoint = torch.load('mnist-epoch_20.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.eval()\n",
    "\n",
    "model = model.to(\"hpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d38f6",
   "metadata": {},
   "source": [
    "Create a MNIST datasets for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501d0838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "data_path = './data'\n",
    "test_kwargs = {'batch_size': 32}\n",
    "dataset1 = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset1,**test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27596215",
   "metadata": {},
   "source": [
    "Create HPUGraph and HPUStream\n",
    "\n",
    "The HPUGraph API provides a performance optimization technique to reduce PyTorch host overhead. This is done by capturing the PyTorch execution on a stream for the first iteration and replaying that in subsequent ones. The replay avoids the PyTorch overhead of accumulating the ops in the model and makes the execution device bound.\n",
    "\n",
    "For further details on Stream APIs and HPU Graph APIs, refer to HPU Graph APIs and Stream APIs in the documentation.\n",
    "\n",
    "The example below shows the capture and replay of HPU Graphs using the following functions:\n",
    "\n",
    "capture_begin() Begins capturing HPU work on the current stream.\n",
    "capture_end() Ends capturing HPU work on the current stream.\n",
    "replay() Replays the HPU work captured by this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530e4830",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = ht.hpu.HPUGraph()\n",
    "s = ht.hpu.Stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444ac15",
   "metadata": {},
   "source": [
    "Do warm run and capture the HPUGraph \n",
    "\n",
    "Here we are using capture and replay of HPU Graphs:\n",
    "capture_begin() Begins capturing HPU work on the current stream.\n",
    "capture_end() Ends capturing HPU work on the current stream.\n",
    "replay() Replays the HPU work captured by this graph.\n",
    "\n",
    "The initial warmup step is run before replaying the graph in the subsequent infernce pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b474ef1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_input = None\n",
    "output = None\n",
    "\n",
    "#WARMRUN \n",
    "static_input = torch.randn(32, 1, 28, 28, device='hpu')\n",
    "with ht.hpu.stream(s):\n",
    "    g.capture_begin()\n",
    "    output = model(static_input)\n",
    "    g.capture_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72b587",
   "metadata": {},
   "source": [
    "Run inference by replaying the graph. \n",
    "\n",
    "Here, we need to copy the input data to the input placeholder(static_input), and then replay. Output will be saved in the output placeholder(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f2da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = 0 \n",
    "for batch_idx, (data, label) in enumerate(test_loader):  \n",
    "    static_input.copy_(data)\n",
    "    g.replay()\n",
    "    if output.shape[0] != label.shape[0]:\n",
    "        output = output[:label.shape[0]]\n",
    "    correct += output.max(1)[1].eq(label).sum()\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(100. * correct / (len(test_loader) * 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "444ba661-d480-446d-b719-220a476261f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
