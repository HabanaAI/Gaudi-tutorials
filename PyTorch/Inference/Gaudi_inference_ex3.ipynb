{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d469ed19",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Habana Labs, Ltd. an Intel Company.\n",
    "SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f88919",
   "metadata": {},
   "source": [
    "# Inference on Gaudi - Example3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca080a25",
   "metadata": {},
   "source": [
    "This notebook is used as an example to show inference on the Gaudi Accelerator. \n",
    "\n",
    "This tutorial will show inference mode with HPU GRAPH with built-in wrapper, by using a simple model and the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7714dada",
   "metadata": {},
   "source": [
    "Download pretrained model checkpoints from vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590066cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://vault.habana.ai/artifactory/misc/inference/mnist/mnist-epoch_20.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42e2dc",
   "metadata": {},
   "source": [
    "Import all neccessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d6831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "import habana_frameworks.torch as ht\n",
    "import habana_frameworks.torch.core as htcore\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3efcd8",
   "metadata": {},
   "source": [
    "Define a simple Net model for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca66e033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1   = nn.Linear(784, 256)\n",
    "        self.fc2   = nn.Linear(256, 64)\n",
    "        self.fc3   = nn.Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        out = x.view(-1,28*28)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e68e09",
   "metadata": {},
   "source": [
    "Create the model, and load the pre-trained checkpoint.\n",
    "Optimize the model for eval, and move the model to the Gaudi Accelerator (“hpu”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ba6c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "checkpoint = torch.load('mnist-epoch_20.pth')\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f7f9f",
   "metadata": {},
   "source": [
    "Wrap the model with HPU graph, and move it to HPU\n",
    "Here we are using \"wrap_in_hpu_graph\" to wrap module forward function with HPU Graphs. This wrapper captures, caches and replays the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b5ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ht.hpu.wrap_in_hpu_graph(model)\n",
    "model = model.to(\"hpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583e1ea",
   "metadata": {},
   "source": [
    "Create a MNIST datasets for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2485d7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "data_path = './data'\n",
    "test_kwargs = {'batch_size': 32}\n",
    "dataset1 = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset1,**test_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e1796",
   "metadata": {},
   "source": [
    "Do a warm run : here HPU graph will be captured and cached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9daea76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup_input = torch.randn(32, 1, 28, 28, device='hpu')\n",
    "warmup_output = model(warmup_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa45ba7",
   "metadata": {},
   "source": [
    "Run inference.\n",
    "\n",
    "Here, we already wrap the model with the HPU graph with wrap_in_hpu_graph as shown above, so there is no need to copy and replay the stream. It will be all done in the background. We are also using asynchronos copies here as shown below (copy with \"non_blocking=True\" followed by mark_step), to further optimize the inference. Please refer to the guideline below for more information [here](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Optimization.html#using-asynchronous-copies). Adding mark_step after model() is not required with HPU Graphs as it is handled implicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf93d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct = 0 \n",
    "for batch_idx, (data, label) in enumerate(test_loader):  \n",
    "    data = data.to(\"hpu\", non_blocking=True)\n",
    "    htcore.mark_step()\n",
    "    output = model(data)\n",
    "    correct += output.max(1)[1].eq(label).sum()\n",
    "\n",
    "print('Accuracy: {:.2f}%'.format(100. * correct / (len(test_loader) * 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d90f7f-c473-4f04-8487-49b8d5a541dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
