{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c2adb6-5a7b-48e6-ab13-1f50f8c18572",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62fb01-b39e-4bd5-a70c-426579ad9f19",
   "metadata": {},
   "source": [
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdde1fc-2079-4023-9d2f-1f0c7d5adede",
   "metadata": {},
   "source": [
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff5422-73e6-4026-b5d2-57ef2e79b10e",
   "metadata": {},
   "source": [
    "## Text Generation Inferenece (TGI) using the Intel&reg; Gaudi&reg; 2 AI Processor\n",
    "Using the Text Generation for Inference (TGI-gaudi) from Hugging Face to easily setup an LLM chatbot or text generation service\n",
    "\n",
    "### Introduction\n",
    "This tutorial will show how to setup and run the TGI-gaudi framework.  TGI-gaudi is a powerful framework designed for deploying and serving large-scale language models efficiently. TGI enables seamless interaction with state-of-the-art models, making it easier for developers to integrate advanced natural language processing capabilities into their applications. This tutorial will guide you through the basics of TGI-gaudi, demonstrating how to set up and use it to generate text responses based on user inputs. We will cover essential concepts, provide code examples, and show you how to customize and control the behavior of your text generation models using TGI. By the end of this tutorial, you'll have a solid understanding of TGI and how to harness its potential for various text generation tasks.  This includes an example using Llama 3 8B Instucts model with the default values as well as an Optimized Llama 2 13B model optimized to suppport the maximum concurrent users in a reasonable time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096077b-df4c-4c33-9655-cf2c6b8899d0",
   "metadata": {},
   "source": [
    "### 1. Intial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c1f50-008c-48ed-a3f4-8f0853a5f5da",
   "metadata": {},
   "source": [
    "There are the initial steps to ensure that your build environment is set correctly:\n",
    "\n",
    "1. Set the appropriate ports for access when you ssh into the Intel Gaudi 2 node.  you need to ensure that the following ports are open:\n",
    "* 8888 (for running this jupyter notebook)\n",
    "* 7680 (for run the gradio server)\n",
    "Do to this, you need to add the following in your overall ssh commmand when connecting to the Intel Gaudi Node:\n",
    "\n",
    "`ssh -L 8888:localhost:8888 -L 7860:localhost:7860 .... `\n",
    "   \n",
    "2. Before you load this Notebook, you will run the standard docker image but you need to include the `/var/run/docker.sock` file.  Use these Run and exec commands below to start your docker. \n",
    "\n",
    "`docker run -itd --name tgi-tutorial --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v /var/run/docker.sock:/var/run/docker.sock  vault.habana.ai/gaudi-docker/1.16.2/ubuntu22.04/habanalabs/pytorch-installer-2.2.2:latest`  \n",
    "\n",
    "`docker exec -it tgi-tutorial bash`\n",
    "\n",
    "`cd ~ && git clone https://github.com/HabanaAI/Gaudi-tutorials`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256a0ea1-09f9-44ca-aff5-d20b21ee995b",
   "metadata": {},
   "source": [
    "#### Setup the docker environment in this notebook:\n",
    "At this point you have cloned the Gaudi-tutorials notebook inside your docker image and have opened this notebook.  You will need to install docker again inside the Intel Gaudi container to manage the execution of the TGI-gaudi docker image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085eaa61-6cad-4309-9432-f9ae8edff7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial\n",
    "!apt-get update\n",
    "!apt-get install docker.io curl -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c1151-a505-4797-adb7-7582255bb58d",
   "metadata": {},
   "source": [
    "### 2. Loading the Text Generation Inference (TGI-gaudi) Environment. \n",
    "We pull the latest TGI-gaudi image.  This image contains the TGI server and launcher that you will access with POST commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c6bad-efcd-4dc9-b7ee-6749e4d5be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull ghcr.io/huggingface/tgi-gaudi:2.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d422efe4-4135-43bb-b36c-0aa5067306b9",
   "metadata": {},
   "source": [
    "#### After building image you will run run it:\n",
    "\n",
    "##### How to access and Use the Llama 3 model\n",
    "To use the [Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) model, you will need a HuggingFace account, agree to the terms of use of the model in its model card on the HF Hub, and create a read token.  You then copy that token to the HUGGING_FACE_HUB_TOKEN variable below. \n",
    "\n",
    "You will select an LLM model that you wish to use.  In this case, we have selected the Llama 3 8B Instruct model from Meta Labs. This model will fit in one Intel Gaudi   \n",
    "\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “META LLAMA 3 COMMUNITY LICENSE AGREEMENT”. For guidance on the intended use of the LLAMA 3 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link  https://llama.meta.com/llama3/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf144b1-7390-4a73-be07-7be1feb683c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 9001:80 \\\n",
    "    -v ~/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial/data:/data \\\n",
    "    --runtime=habana \\\n",
    "    --name gaudi-tgi \\\n",
    "    -e HABANA_VISIBLE_DEVICES=all \\\n",
    "    -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "    -e HUGGING_FACE_HUB_TOKEN=\"<your_hugging_face_token_here>\" \\\n",
    "    -e ENABLE_HPU_GRAPH=True \\\n",
    "    -e BATCH_BUCKET_SIZE=8  \\\n",
    "    -e PREFILL_BATCH_BUCKET_SIZE=4  \\\n",
    "    -e PAD_SEQUENCE_TO_MULTIPLE_OF=128  \\\n",
    "    --cap-add=sys_nice \\\n",
    "    --ipc=host \\\n",
    "    ghcr.io/huggingface/tgi-gaudi:2.0.1 \\\n",
    "    --model-id meta-llama/Meta-Llama-3-8B-Instruct  \\\n",
    "    --max-input-tokens 1024 --max-total-tokens 2048   \\\n",
    "\t--max-batch-prefill-tokens 1074 --max-batch-total-tokens 16536 \\\n",
    "    --rope-scaling linear --rope-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb71d09-828a-4022-9626-fd2d08217d74",
   "metadata": {},
   "source": [
    "### Important Parameters to setup the TGI-gaudi image\n",
    "You will notice several Environment variables (-e) and model configuration variables included in the command above. It's important to undersand what these do and should be tuned for best performance.  For the full descsiption of Environment Variables please see the TGI-Gaudi [README](https://github.com/huggingface/tgi-gaudi?tab=readme-ov-file#environment-variables) and for the model configuration variables, you can review the TGI [documentation](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher)  \n",
    "\n",
    "#### Model Configuration Variables\n",
    "**The Maximum sequence length is controlled by the first two arguments:**  \n",
    "**--max-total-tokens** \n",
    "This is the most important value to set as it defines the \"memory budget\" of running clients requests. Clients will send input sequences and ask to generate `max_new_tokens` on top. With a value of `1512` users can send either a prompt of `1000` and ask for `512` new tokens, or send a prompt of `1` and ask for `1511` max_new_tokens.  The example above sets the total tokens at 2048\n",
    "\n",
    "**--max-input-tokens** \n",
    "This is the maximum allowed input length (expressed in number of tokens) for users. The larger this value, the longer prompt users can send which can impact the overall memory required to handle the load. Please note that some models have a finite range of sequence they can handle. Default to min(max_position_embeddings - 1, 4095).  For this example, \n",
    "\n",
    "**--max-batch-prefill-tokens** \n",
    "Limits the number of tokens for the prefill operation. Since this operation take the most memory and is compute bound, it is interesting to limit the number of requests that can be sent. Default to `max_input_tokens + 50` to give a bit of room\n",
    "\n",
    "**--max-batch-total-tokens**\n",
    "This is one critical control to allow maximum usage of the available hardware.  This represents the total amount of potential tokens within a batch. When using padding (not recommended) this would be equivalent of `batch_size` * `max_total_tokens`.  Overall this number should be the largest possible amount that fits the remaining memory (after the model is loaded). Since the actual memory overhead depends on other parameters like if you're using quantization, flash attention or the model implementation, text-generation-inference cannot infer this number automatically.\n",
    "\n",
    "**--max-batch-size**\n",
    "Enforce a maximum number of requests per batch Specific flag for hardware targets that do not support unpadded inference\n",
    "\n",
    "#### Environment Variables\n",
    "The settings in the example above are all set to the default values.\n",
    "| Environment Variable | Default | Description |\n",
    "|----------------------|---------|-------------|    \n",
    "| `ENABLE_HPU_GRAPH` | True | Enable hpu graph or disable it.  It's recommended to leave this enabled for best performance. |\n",
    "| `LIMIT_HPU_GRAPH` | False | Skip HPU graph usage for prefill to save memory, set to True for large sequence/decoding lengths. |\n",
    "| `BATCH_BUCKET_SIZE` | 8 | Batch size for decode operation will be rounded to the nearest multiple of this number. This limits the number of cached graphs. | \n",
    "| `PREFILL_BATCH_BUCKET_SIZE` | 4 | Batch size for prefill operation will be rounded to the nearest multiple of this number. This limits the number of cached graphs. |\n",
    "| `PAD_SEQUENCE_TO_MULTIPLE_OF` | 128 | For prefill operation, sequences will be padded to a multiple of provided value. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db319c26-0252-4114-b209-24df991deee4",
   "metadata": {},
   "source": [
    "### 3. Using the TGI-gaudi client\n",
    "#### Wait until the TGI-gaudi service is connected before starting to use it:\n",
    "After running the docker server, it will take some time to download the model and load it into the device. To check the status run: `docker logs gaudi-tgi` in a separate terminal window and you should see:\n",
    "```\n",
    "2024-05-22T19:31:48.297054Z  INFO text_generation_router: router/src/main.rs:496: Serving revision c4a54320a52ed5f88b7a2f84496903ea4ff07b45 of model meta-llama/Meta-Llama-3-8B-Instruct\n",
    "2024-05-22T19:31:48.297067Z  INFO text_generation_router: router/src/main.rs:279: Using config Some(Llama)\n",
    "2024-05-22T19:31:48.297073Z  INFO text_generation_router: router/src/main.rs:291: Using the Hugging Face API to retrieve tokenizer config\n",
    "2024-05-22T19:31:48.302174Z  INFO text_generation_router: router/src/main.rs:340: Warming up model\n",
    "2024-05-22T19:31:48.302222Z  WARN text_generation_router: router/src/main.rs:355: Model does not support automatic max batch total tokens\n",
    "2024-05-22T19:31:48.302231Z  INFO text_generation_router: router/src/main.rs:377: Setting max batch total tokens to 16536\n",
    "2024-05-22T19:31:48.302239Z  INFO text_generation_router: router/src/main.rs:378: Connected\n",
    "2024-05-22T19:31:48.302246Z  WARN text_generation_router: router/src/main.rs:392: Invalid hostname, defaulting to 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af7431-24af-4a8c-9234-8ccf3b17aaa1",
   "metadata": {},
   "source": [
    "#### Simple cURL Command\n",
    "Once the setup is complete, you can verify that that the text generation is working by sending a simple cURL request to it (note that first request could be slow due to graph compilation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad5600d-915f-425b-a979-d603c8ea6270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\":\"3 people standing at the edge of the cliff. They were all staring out at the sea, their faces pale and worried. I approached them cautiously, not wanting to startle them.\\n\\\"Hey, what's going on?\\\" I asked, trying to sound calm.\\n\\nOne of them turned to me, a young woman with a look of desperation in her eyes. \\\"We've been searching for our friend,\\\" she said. \\\"She went out to swim and never came back. We've been looking for her everywhere, but we can't find her.\\\"\\n\\nI felt a chill run down my spine. \\\"How long has she been missing?\\\" I\"}"
     ]
    }
   ],
   "source": [
    "!curl 127.0.0.1:9001/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"I ran down the path and saw \",\"parameters\":{\"max_new_tokens\":128}}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723aec23-56d9-4002-8dff-643e0e8f1d2a",
   "metadata": {},
   "source": [
    "#### Python Command\n",
    "You can also use Python to do the same thing while adding more parameters.  In both of these cases you ure sending a request to the TGI-gaudi client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d9c93dd-e0fb-49de-a069-56c50087a789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Riding a bike is one of the most enjoyable and liberating experiences I've ever had. The wind in my hair, the sun on my face, and the feeling of freedom as I pedal along the road or trail is exhilarating. I love the sense of accomplishment when I reach my destination, whether it's a scenic overlook or a favorite park. The exercise is great too, and I feel invigorated and refreshed after a long ride. Plus, it's a great way to clear my mind and relieve stress. Whether I'm cruising through the city or exploring the countryside, riding a bike is always a thrill.\n",
      "Write a paragraph about the importance of recycling\n",
      "Recycling is a crucial practice that plays a vital role in protecting our planet. By recycling, we can conserve natural resources, reduce landfill waste, and decrease greenhouse gas emissions. Recycling also helps to preserve the environment by reducing the need for extracting, processing, and transporting raw materials. Additionally, recycling helps to save energy and water\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'inputs': 'Write short paragraph about riding a bike',\n",
    "    'parameters': {\n",
    "        'max_new_tokens': 200,\n",
    "        'temperature': 0.7,\n",
    "        'top_p': 0.5\n",
    "    },\n",
    "}\n",
    "\n",
    "response = requests.post('http://127.0.0.1:9001/generate', headers=headers, json=data)\n",
    "\n",
    "generated_text = response.json()[\"generated_text\"]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64020c89-9d06-44a3-afe7-9454ddddb8a3",
   "metadata": {},
   "source": [
    "#### Application Front end for Text Generation and Serving\n",
    "Finally, we setup a Gradio front end for engagement with TGI-gaudi. Remeber to pass in port 7860 in the initial ssh command `ssh -L 7860:localhost:7860 ...` to the Intel Gaudi node to be able to view the Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73d8634-57e6-40c0-a60f-2e05830121f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4811b363-65c1-4fce-8ff1-23bfdbfa3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24dfff65-49d2-4e54-ba0f-215dd1b3aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "import argparse\n",
    "import json\n",
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "gaudi_device_url = f\"http://127.0.0.1:9001/generate\"\n",
    "            \n",
    "def text_gen(inputs, output_tokens, temperature, top_p, url=gaudi_device_url):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    payload = {'inputs': inputs, 'parameters': {'max_new_tokens': output_tokens,'temperature': temperature, 'top_p': top_p}}\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "    generated_text = response.json()[\"generated_text\"]\n",
    "    return generated_text\n",
    "\n",
    "inputs = [\n",
    "        gr.Textbox(label=\"Prompt\", value=\"What is the meaning of life?\"),  # Default question\n",
    "        gr.Number(label=\"Output Token Size (Max 1024)\", value=64),  # Default number of tokens\n",
    "        gr.Number(label=\"Temperature\", value=0.9, visible=False), # Default temperature value, can be changed here\n",
    "        gr.Number(label=\"Top_p\", value=0.7, visible=False)  # Default top_p value, can be changed here\n",
    "]\n",
    "outputs = gr.Markdown(label=\"Response\")\n",
    "\n",
    "demo = gr.Interface(\n",
    "        fn=text_gen,\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        title=\"Text Generation with Llama 3 8B Model on Intel&reg; Gaudi&reg; 2\", \n",
    "        description=\"Have a chat with Intel Gaudi thru TGI\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07479c69",
   "metadata": {},
   "source": [
    "When you are done running experiments, you can stop the container to free resources that will be used for the performance example in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop gaudi-tgi\n",
    "!docker rm gaudi-tgi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bcd7d-8bfc-4ce7-8ea9-0176ce07b122",
   "metadata": {},
   "source": [
    "### Performance Example\n",
    "As stated above, there are several parameters that can be used to manage the use of TGI to obtain the best performance, here's an example of the Llama 2 13B model with the environment variables and model variables tuned to support the largest number of concurrent users of the TGI-gaudi. \n",
    "\n",
    "To use the Llama 2 model, you will need a HuggingFace account, agree to the terms of use of the model in its model card on the HF Hub, and create a read token. You then copy that token to the HUGGING_FACE_API_KEY variable below.\n",
    "\n",
    "Use of the pretrained model is subject to compliance with third party licenses, including the “Llama 2 Community License Agreement” (LLAMAV2). For guidance on the intended use of the LLAMA2 model, what will be considered misuse and out-of-scope uses, who are the intended users and additional terms please review and read the instructions in this link https://ai.meta.com/llama/license/. Users bear sole liability and responsibility to follow and comply with any third party licenses, and Habana Labs disclaims and will bear no liability with respect to users’ use or compliance with third party licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a43aae-ad6a-48a2-9c11-2b5af3f07fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecb1d8-e8dc-4d23-b13d-f3bcbfa515eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d -p 9002:80 \\\n",
    "    -v ~/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial/data:/data \\\n",
    "    --runtime=habana \\\n",
    "    --name gaudi-tgi-perf \\\n",
    "    -e HABANA_VISIBLE_DEVICES=\"all\"  \\\n",
    "    -e HUGGING_FACE_HUB_TOKEN=\"<your_hugging_face_token_here>\" \\\n",
    "    -e PT_HPU_ENABLE_LAZY_COLLECTIVES=true \\\n",
    "    -e BATCH_BUCKET_SIZE=16 \\\n",
    "    -e PREFILL_BATCH_BUCKET_SIZE=1 \\\n",
    "    -e PAD_SEQUENCE_TO_MULTIPLE_OF=1024 \\\n",
    "    -e OMPI_MCA_btl_vader_single_copy_mechanism=none \\\n",
    "    --cap-add=sys_nice \\\n",
    "    --ipc=host \\\n",
    "    ghcr.io/huggingface/tgi-gaudi:2.0.1 \\\n",
    "    --model-id meta-llama/Llama-2-13b-hf \\\n",
    "     --max-batch-prefill-tokens 4096 --max-batch-total-tokens 18432 \\\n",
    "    --max-input-length 1024 --max-total-tokens 1152 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df1a0e-21ae-4165-aa3c-62a08fe001fe",
   "metadata": {},
   "source": [
    "Like the previous example, open a separate terminal window and use `docker logs gaudi-tgi-perf` to check the status and wait for the server to be ready.\n",
    "\n",
    "First, do a quick test to ensure that the TGI-gaudi is working with this new performance configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7db5b04-95e5-4a38-94dd-a67406b8b506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\":\"200 people standing in the middle of the field. I was so excited to see them all there. I ran up to the front and saw my friend, who was the one who invited me. I was so happy to see her. I was so happy to see everyone. I was so happy to be there.\\nI was so happy to be there. I was so happy to be there. I was so happy to be there. I was so happy to be there. I was so happy to be there. I was so happy to be there. I was so happy to be there. I was so happy to be\"}"
     ]
    }
   ],
   "source": [
    "!curl 127.0.0.1:9002/generate \\\n",
    "    -X POST \\\n",
    "    -d '{\"inputs\":\"I ran down the path and saw \",\"parameters\":{\"max_new_tokens\":128}}' \\\n",
    "    -H 'Content-Type: application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f49aa-3305-4c9f-95f4-e6472ee38223",
   "metadata": {},
   "source": [
    "Now we need to install the **llmperf** tool to make measurements on the TGI performance.  This tool simulates mutiple users making queries to the TGI-gaudi interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc0050f7-6894-48da-9cb5-e99859dfdfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial\n",
      "/root/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial/llmperf\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/TGI_Gaudi_tutorial\n",
    "!git clone -b v2.0 https://github.com/ray-project/llmperf\n",
    "%cd llmperf/\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55924f6-d92a-4e37-8ed2-cb918320dea2",
   "metadata": {},
   "source": [
    "Since this is a Hugging Face model, we now set the appropriate API values to launch the llperf benchmark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b8e069-c12b-44cc-a9bf-3fd895d1e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACE_API_BASE']=\"http://localhost:9002/generate_stream\"\n",
    "os.environ['HUGGINGFACE_API_KEY']=\"<your_hugging_face_token_here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537823cc-2d87-49b8-bc68-764e4d2422f0",
   "metadata": {},
   "source": [
    "Now run the llmperf benchmark script.   You will notice that the `num-concurrent-requests` is set to a value of 44, which will represent the largest number of users supported on one Intel Gaudi card with this specific model, assuming a mean of 128 input tokens.   In this case the goal is to have 90% of the full response complete in less than 10 seconds.  You can see in the `end_to_end_latency_s` results show that the p90 value is ~7.2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d781f211-ed42-4bc3-86ab-bb9efb0a82db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-05-25 22:59:20,524\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "132it [00:46,  2.87it/s]                                                        \n",
      "\\Results for token benchmark for huggingface/meta-llama/Llama-2-13b-chat-hf queried with the litellm api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.03644459797231871\n",
      "    p50 = 0.04851207363988\n",
      "    p75 = 0.053379669053811085\n",
      "    p90 = 0.05615809373684897\n",
      "    p95 = 0.05927348466492583\n",
      "    p99 = 0.08432977966120812\n",
      "    mean = 0.04450413569916239\n",
      "    min = 0.0\n",
      "    max = 0.15354078621603548\n",
      "    stddev = 0.01744576215587963\n",
      "ttft_s\n",
      "    p25 = 0.5576231110026129\n",
      "    p50 = 1.6096200764877722\n",
      "    p75 = 2.7494891796959564\n",
      "    p90 = 3.6670344557613133\n",
      "    p95 = 4.055443324754014\n",
      "    p99 = 4.4736988103901965\n",
      "    mean = 1.72034467037619\n",
      "    min = 0.0\n",
      "    max = 4.52720589004457\n",
      "    stddev = 1.3181433390672246\n",
      "end_to_end_latency_s\n",
      "    p25 = 4.569637373788282\n",
      "    p50 = 6.087061490048654\n",
      "    p75 = 6.812031272042077\n",
      "    p90 = 7.181748718651943\n",
      "    p95 = 7.464364082238171\n",
      "    p99 = 7.782859607657883\n",
      "    mean = 5.505389457630262\n",
      "    min = 0.3500262850429863\n",
      "    max = 7.995516473194584\n",
      "    stddev = 1.736964262406489\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 18.410594529061406\n",
      "    p50 = 20.031836752434657\n",
      "    p75 = 26.476713275956577\n",
      "    p90 = 29.147287107308674\n",
      "    p95 = 30.210381675602054\n",
      "    p99 = 30.944191636442888\n",
      "    mean = 21.011994663771915\n",
      "    min = 0.3317217500184351\n",
      "    max = 31.728728358451388\n",
      "    stddev = 6.910601663892157\n",
      "number_input_tokens\n",
      "    p25 = 1024.0\n",
      "    p50 = 1024.0\n",
      "    p75 = 1024.0\n",
      "    p90 = 1024.0\n",
      "    p95 = 1024.0\n",
      "    p99 = 1024.0\n",
      "    mean = 1024.0\n",
      "    min = 1024\n",
      "    max = 1024\n",
      "    stddev = 0.0\n",
      "number_output_tokens\n",
      "    p25 = 128.0\n",
      "    p50 = 128.0\n",
      "    p75 = 129.0\n",
      "    p90 = 129.0\n",
      "    p95 = 129.0\n",
      "    p99 = 129.0\n",
      "    mean = 117.03787878787878\n",
      "    min = 1\n",
      "    max = 129\n",
      "    stddev = 35.90772850597038\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 335.4771304065723\n",
      "Number Of Completed Requests: 132\n",
      "Completed Requests Per Minute: 171.983874219694\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python3 token_benchmark_ray.py \\\n",
    "    --model huggingface/meta-llama/Llama-2-13b-chat-hf \\\n",
    "    --mean-input-tokens 1024 \\\n",
    "    --stddev-input-tokens 0 \\\n",
    "    --mean-output-tokens 128 \\\n",
    "    --stddev-output-tokens 0 \\\n",
    "    --max-num-completed-requests 100 \\\n",
    "    --timeout 2400 \\\n",
    "    --num-concurrent-requests 44 \\\n",
    "    --results-dir result_outputs \\\n",
    "    --llm-api litellm \\\n",
    "    --additional-sampling-params {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605bced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop gaudi-tgi-perf\n",
    "!docker rm gaudi-tgi-perf\n",
    "\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
