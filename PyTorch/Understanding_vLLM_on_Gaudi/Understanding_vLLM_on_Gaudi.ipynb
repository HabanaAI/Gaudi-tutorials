{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding vLLM on Intel® Gaudi® 2 AI Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a follow up to the [Getting Started with vLLM](https://github.com/HabanaAI/Gaudi-tutorials/blob/main/PyTorch/Getting_Started_with_vLLM/Getting_Started_with_vLLM.ipynb) tutorial and demonstrates the effects of tuning vLLM specific parameters for Intel® Gaudi® 2 AI Accelerators on the overall performance of the vLLM engine. This notebook walks you through the server bringup phase and explains how to read and understand the logs regarding memory consumption trends. This tutorial also utilises the vLLM's in-built ```benchmark_serving.py``` script which measures various throughput and latency metrics in an online serving setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vLLM Installation and Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gaudi requirements and installation please refer to [Requirements and Installation](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#requirements-and-installation).\n",
    "\n",
    "It is highly recommended to use the latest Docker image from Intel Gaudi vault. Refer to the [Run Docker Image](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#run-docker-image) section from [Intel Gaudi documentation](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html#pull-prebuilt-containers) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell installs vLLM server for Gaudi. For more information on installing vLLM for Gaudi refer to [Build And Install vLLM](https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md#build-and-install-vllm-fork)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/HabanaAI/vllm-fork.git \n",
    "cd vllm-fork &&\n",
    "git checkout v0.5.3.post1+Gaudi-1.18.0 &&\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter your [Huggingface Hub Token](https://huggingface.co/docs/hub/en/security-tokens) (HF_TOKEN) needed for accessing certain models like Llama3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "HF_TOKEN=\"<YOUR HF_TOKEN HERE>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model you would like to benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"meta-llama/Meta-Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the vLLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be running this basic command (below) to load the model specified earlier and launch the OpenAI compatible server instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server --model=models/Meta-Llama-3.1-8B --port 8000 --block-size 128\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: For optimal performance, it is recommended to run inference on Gaudi 2 with ```--block-size``` of 128 for BF16 data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We launch the vLLM server as a separate process in the background by running the following cells and logging its output in a separate file called ```vllm_server_stdout.log```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd vllm-fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "limit_output": 100,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 10681\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "command = f\"HF_TOKEN={HF_TOKEN} python -m vllm.entrypoints.openai.api_server --model={MODEL} --port 8000 --block-size 128\"\n",
    "\n",
    "with open('vllm_server_stdout.log', 'w') as file:\n",
    "    process = subprocess.Popen(command, shell=True, start_new_session=True, stdout=file, stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Process ID:\", process.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The server is now beginning initialization and will proceed to warmup phase.\n",
    "**To read the server logs, run the following cell after waiting a couple of minutes allowing the server to load weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO 11-03 21:23:07 habana_model_runner.py:587] Pre-loading model weights on hpu:0 took 15.05 GiB of device memory (15.05 GiB/94.62 GiB used) and 3.147 GiB of host memory (57.84 GiB/1007 GiB used)\n",
      "INFO 11-03 21:23:07 habana_model_runner.py:639] Wrapping in HPU Graph took 0 B of device memory (15.05 GiB/94.62 GiB used) and -252 KiB of host memory (57.84 GiB/1007 GiB used)\n",
      "INFO 11-03 21:23:07 habana_model_runner.py:643] Loading model weights took in total 15.05 GiB of device memory (15.05 GiB/94.62 GiB used) and 3.146 GiB of host memory (57.84 GiB/1007 GiB used)\n",
      "INFO 11-03 21:23:14 habana_worker.py:146] Model profiling run took 5.318 GiB of device memory (20.37 GiB/94.62 GiB used) and 226 MiB of host memory (58.06 GiB/1007 GiB used)\n",
      "INFO 11-03 21:23:14 habana_worker.py:170] Free device memory: 74.25 GiB, 66.83 GiB usable (gpu_memory_utilization=0.9), 26.73 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.4), 40.1 GiB reserved for KV cache\n"
     ]
    }
   ],
   "source": [
    "!grep -B5 \"Free device memory:\" vllm_server_stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These log entries tell us:\n",
    "- State of the memory consumption in the device after loading of model weights and server's profiling run but *before* HPU Graphs Capture.\n",
    "- How much memory is reserved for the **KV Cache and the HPU Graphs** out of the usable memory pool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, the server has begun the warmup and HPU Graphs Capture phase. Let us take a quick look at the [bucketing configuration](https://github.com/HabanaAI/vllm-fork/blob/v0.5.3.post1%2BGaudi-1.18.0/README_GAUDI.md#bucketing-mechanism) used which is crucial in getting efficient server responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_PROMPT_BS_BUCKET_MIN=1 (default:min)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_PROMPT_BS_BUCKET_STEP=32 (default:step)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_PROMPT_BS_BUCKET_MAX=64 (default:max)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_DECODE_BS_BUCKET_MIN=32 (default:min)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_DECODE_BS_BUCKET_STEP=32 (default:step)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_DECODE_BS_BUCKET_MAX=256 (default:max)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_PROMPT_SEQ_BUCKET_MIN=128 (default:min)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_PROMPT_SEQ_BUCKET_STEP=128 (default:step)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_PROMPT_SEQ_BUCKET_MAX=1024 (default:max)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_DECODE_BLOCK_BUCKET_MIN=128 (default:min)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_DECODE_BLOCK_BUCKET_STEP=128 (default:step)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:95] VLLM_DECODE_BLOCK_BUCKET_MAX=4096 (default:max)\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:691] Prompt bucket config (min, step, max_warmup) bs:[1, 32, 64], seq:[128, 128, 1024]\n",
      "INFO 11-03 21:22:30 habana_model_runner.py:696] Decode bucket config (min, step, max_warmup) bs:[32, 32, 256], block:[128, 128, 4096]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n"
     ]
    }
   ],
   "source": [
    "!grep -A14 \"VLLM_PROMPT_BS_BUCKET_MIN=\" vllm_server_stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, run the following cell **after waiting for ten minutes** allowing the server to finish warmup and HPU Graphs capture\n",
    "> **Note**: Warmup and HPUGraphs capture time depends on many factor, e.g. input and output sequence length, batch size, number of buckets (bucketing config) and datatype. It can take anywhere from tens of minutes to even hours based on the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-04 05:27:00 habana_model_runner.py:1635] Warmup finished in 225 secs, allocated 1.429 GiB of device memory\n",
      "INFO 11-04 05:27:00 habana_executor.py:91] init_cache_engine took 41.52 GiB of device memory (61.89 GiB/94.62 GiB used) and 2.675 GiB of host memory (66.15 GiB/1007 GiB used)\n"
     ]
    }
   ],
   "source": [
    "!grep -B1 \"init_cache_engine took\" vllm_server_stdout.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If all looks good with no errors at this point, the server is ready to serve inference requests.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Out of Memory Errors\n",
    "\n",
    "If you see ```Out of Memory``` errors in the logs in any of the above stages, try restarting the server with the following parameters in the command line: \n",
    "- Increased ```--gpu-memory-utilization``` (default: 0.9)- This addresses insufficient available memory per card.\n",
    "- Increased ```--tensor-parallel-size``` (default: 1) - This approach shards model weights across the devices and may help in loading a model which is too big for a single card.\n",
    "\n",
    "Refer [Understanding vLLM logs](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/vLLM_Inference.html#understanding-vllm-logs) article for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Environment Variables\n",
    "Here are some key environment variables that can affect the server's efficiency and can be set before running the above server command.\n",
    "\n",
    "#### VLLM_GRAPH_RESERVED_MEM\n",
    "Defines the ratio of memory reserved for HPU Graphs vs KV Cache. Default value: 0.4\n",
    "\n",
    "Since HPU Graphs and KV Cache occupy the same memory pool (*“usable memory”* determined by ```--gpu-memory-utilization```), a balance is required between the two which can be managed using this variable:\n",
    "- Maximizing KV Cache size helps to accommodate bigger batches resulting in **increased overall throughput**.\n",
    "- On the other hand, maximizing HPU Graphs capture size reduces host overhead times and can be useful for **reducing latency**.\n",
    "\n",
    "#### VLLM_GRAPH_PROMPT_RATIO\n",
    "Determines the ratio of usable graph memory reserved for prefill and decode graphs. Default value: 0.5\n",
    "- Keep a high value if input tokens throughput needs to be prioritized.\n",
    "- Vice versa if output tokens throughput needs priority.\n",
    "\n",
    "#### VLLM_GRAPH_PROMPT_STRATEGY\n",
    "Configure the strategy for determining order of prompt graph capture, `min_tokens` or `max_bs`, Default:`min_tokens`.\n",
    "\n",
    "#### VLLM_GRAPH_DECODE_STRATEGY\n",
    "Configure the strategy for determining order of decode graph capture, `min_tokens` or `max_bs`, Default:`max_bs`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing Benchmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our server is running in the background and ready to serve requests, in this section, we analyze the effects of the various **runtime** performance tuning knobs on the throughput and latency by utilizing the vLLMs in-built `benchmark_serving.py` script for online serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Baseline:\n",
    "Prepare the baseline command to run the with user specified `MODEL` and default key environment variables with other arguments.\n",
    "- It utilizes the `ShareGPT_V3_unfiltered_cleaned_split.json` which is a dataset containing real conversations from ChatGPT.\n",
    "- The results from this command are saved in the `benchmark_serving.log` file and would be compared against results from the other experiments.\n",
    "- The total input tokens and total output tokens are kept fixed across all the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='/root/mnt/weka/data/git_lfs/pytorch/llama3.1/Meta-Llama-3.1-8B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1000, sharegpt_output_len=128, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None)\n",
      "Starting initial single prompt test run...\n",
      "Initial test run completed. Starting main benchmark run...\n",
      "Traffic request rate: inf\n",
      "100%|██████████| 1000/1000 [01:32<00:00, 10.84it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     1000      \n",
      "Benchmark duration (s):                  92.28     \n",
      "Total input tokens:                      224330    \n",
      "Total generated tokens:                  11610     \n",
      "Request throughput (req/s):              10.84     \n",
      "Input token throughput (tok/s):          2431.09   \n",
      "Output token throughput (tok/s):         125.82    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          29911.59  \n",
      "Median TTFT (ms):                        13800.22  \n",
      "P99 TTFT (ms):                           86367.68  \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          802.58    \n",
      "Median TPOT (ms):                        789.73    \n",
      "P99 TPOT (ms):                           1429.97   \n",
      "---------------Inter-token Latency----------------\n",
      "Mean ITL (ms):                           468.55    \n",
      "Median ITL (ms):                         64.59     \n",
      "P99 ITL (ms):                            4964.05   \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python benchmark_serving.py --backend vllm --model $MODEL --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000 --request-rate inf --sharegpt-output-len 128 2>&1 | tee benchmark_serving_baseline.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with a higher VLLM_GRAPH_PROMPT_RATIO:\n",
    "Let us **restart** the server with a higher prefill ratio (VLLM_GRAPH_PROMPT_RATIO=0.8) and run the benchmark again and see the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "Namespace(backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', model='/root/mnt/weka/data/git_lfs/pytorch/llama3.1/Meta-Llama-3.1-8B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1000, sharegpt_output_len=128, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, save_result=False, metadata=None, result_dir=None, result_filename=None)\n",
      "Starting initial single prompt test run...\n",
      "Initial test run completed. Starting main benchmark run...\n",
      "Traffic request rate: inf\n",
      "100%|██████████| 1000/1000 [01:16<00:00, 13.05it/s]\n",
      "============ Serving Benchmark Result ============\n",
      "Successful requests:                     1000      \n",
      "Benchmark duration (s):                  76.60     \n",
      "Total input tokens:                      224330    \n",
      "Total generated tokens:                  11610     \n",
      "Request throughput (req/s):              13.05     \n",
      "Input token throughput (tok/s):          2928.46   \n",
      "Output token throughput (tok/s):         151.56    \n",
      "---------------Time to First Token----------------\n",
      "Mean TTFT (ms):                          25923.98  \n",
      "Median TTFT (ms):                        14903.40  \n",
      "P99 TTFT (ms):                           70713.34  \n",
      "-----Time per Output Token (excl. 1st token)------\n",
      "Mean TPOT (ms):                          731.42    \n",
      "Median TPOT (ms):                        843.48    \n",
      "P99 TPOT (ms):                           1033.74   \n",
      "---------------Inter-token Latency----------------\n",
      "Mean ITL (ms):                           410.81    \n",
      "Median ITL (ms):                         64.16     \n",
      "P99 ITL (ms):                            5844.67   \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "!python benchmark_serving.py --backend vllm --model $MODEL --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000 --request-rate inf --sharegpt-output-len 128 2>&1 | tee benchmark_serving_pr_08.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Experiment (Increased Prefill Graph Memory) when compared to baseline numbers:\n",
    " - 20% Increase in Request throughput (req/s) and similar across other throughtput benchmarks. \n",
    "   (This can be explained by the higher memory reserved for pre-fill stage.)\n",
    " - 13% drop in Mean TTFT\n",
    " - 9% drop in Mean TPOT\n",
    " - 12% drop in Mean ITL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further experimentation:\n",
    "You may retry above experiment with a lower **VLLM_GRAPH_PROMPT_RATIO**, different graph strategies, higher output-lengths etc. and make your own conclusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
