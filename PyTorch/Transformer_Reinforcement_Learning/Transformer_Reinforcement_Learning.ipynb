{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00676b-8d19-4e0d-b61d-68ccc14c294d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714683-1b37-4048-bf91-2b5af1c18d40",
   "metadata": {},
   "source": [
    "## Intel® Gaudi® Accelerator Using Hugging Face Transformer Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db2bc-12ef-47a0-83f8-cfba2a8af03c",
   "metadata": {},
   "source": [
    "\n",
    "This document provides instructions on setting up the Intel Gaudi 2 AI accelerator Instance on the Intel® Developer Cloud or any on-premise Intel Gaudi Node. You will be running models from the Intel Gaudi software Model References and the Hugging Face Optimum Habana library.\n",
    "\n",
    "This assumes that you have setup the latest Intel Gaudi PyTorch Docker image.\n",
    "\n",
    "The first step is to install the Optimum Habana repository from GitHub and run the demo of Transformer Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37216139-611e-4b07-a90f-cb351f18b185",
   "metadata": {},
   "source": [
    "### Fine-tuning with Hugging Face Optimum Habana Library\n",
    "The Optimum Habana library is the interface between the Hugging Face Transformers and Diffusers libraries and the Gaudi 2 card. It provides a set of tools enabling easy model loading, training and inference on single and multi-card settings for different downstream tasks. The following example use the DPO and PPO pipeline to fine-tune a Llama 2 7B model.  For more details, see the [TRL](https://github.com/huggingface/optimum-habana/tree/main/examples/trl) examples at the Optimum-Habana GitHub page. \n",
    "\n",
    "Follow the below steps to install the stable release from the Optimum Habana examples and library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420be01-fb88-466a-9fe7-87006340905f",
   "metadata": {},
   "source": [
    "1. Clone the Optimum-Habana project and check out the lastest stable release.  This repository gives access to the examples that are optimized for Intel Gaudi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07346c1a-1fea-4b62-8a79-760ca7a64073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.15.0 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ded02-2aa1-4725-b1fc-cf917e05d9aa",
   "metadata": {},
   "source": [
    "2. Install Optimum-Habana library. This will install the latest stable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e35cc-0ea1-4205-ae70-323252169c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum-habana==1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627879-a273-4914-8efd-68c9a81ebbb9",
   "metadata": {},
   "source": [
    "3. In order to use the DeepSpeed library on Intel Gaudi 2, install the Intel Gaudi DeepSpeed fork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e99a7-4db0-4519-b406-ccedee40796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f0546-e74c-4363-b443-a0f59504d973",
   "metadata": {},
   "source": [
    "The following example is based on the Optimum-Habana TRL task example. Change to the trl directory and install the additional SW requirements for this specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24537f40-8daa-4ac9-ad19-bf1cfaaf29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/trl/\n",
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2bc05d-ae12-4e35-8e95-124b754d6d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token <your_token_here>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04ce10",
   "metadata": {},
   "source": [
    "### DPO Pipeline\n",
    "\n",
    "#### Training\n",
    "\n",
    "The following example is for the creation of StackLlaMa 2: a Stack exchange llama-v2-7b model. There are two main steps to the DPO training process:\n",
    "\n",
    "1. Supervised fine-tuning of the base llama-v2-7b model to create llama-v2-7b-se:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01698aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/diffusers/models/vq_model.py:20: FutureWarning: `VQEncoderOutput` is deprecated and will be removed in version 0.31. Importing `VQEncoderOutput` from `diffusers.models.vq_model` is deprecated and this will be removed in a future version. Please use `from diffusers.models.autoencoders.vq_model import VQEncoderOutput`, instead.\n",
      "  deprecate(\"VQEncoderOutput\", \"0.31\", deprecation_message)\n",
      "/usr/local/lib/python3.10/dist-packages/diffusers/models/vq_model.py:25: FutureWarning: `VQModel` is deprecated and will be removed in version 0.31. Importing `VQModel` from `diffusers.models.vq_model` is deprecated and this will be removed in a future version. Please use `from diffusers.models.autoencoders.vq_model import VQModel`, instead.\n",
      "  deprecate(\"VQModel\", \"0.31\", deprecation_message)\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:159: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-10-19 01:49:20,597] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.61it/s]\n",
      "Resolving data files: 100%|████████████████████| 20/20 [00:00<00:00, 112.58it/s]\n",
      "100%|████████████████████████████████████████| 400/400 [00:01<00:00, 227.40it/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 0\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056412772 KB\n",
      "------------------------------------------------------------------------------\n",
      "[WARNING|trainer.py:617] 2024-10-19 01:49:33,880 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "/root/mdeopujari/optimum-habana/optimum/habana/trl/trainer/sft_trainer.py:425: UserWarning: You passed `packing=True` to the SFTTrainer/SFTConfig, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n",
      "{'loss': 1.5746, 'grad_norm': 0.056957948952913284, 'learning_rate': 1e-05, 'epoch': 0.02, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.71, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5196, 'grad_norm': 0.05368218570947647, 'learning_rate': 2e-05, 'epoch': 0.04, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.6008, 'grad_norm': 0.0573754757642746, 'learning_rate': 3e-05, 'epoch': 0.06, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.6043, 'grad_norm': 0.06595490127801895, 'learning_rate': 4e-05, 'epoch': 0.08, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5256, 'grad_norm': 0.09553191810846329, 'learning_rate': 5e-05, 'epoch': 0.1, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5816, 'grad_norm': 0.09764542430639267, 'learning_rate': 6e-05, 'epoch': 0.12, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.6123, 'grad_norm': 0.09972520172595978, 'learning_rate': 7e-05, 'epoch': 0.14, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5469, 'grad_norm': 0.12126927822828293, 'learning_rate': 8e-05, 'epoch': 0.16, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4968, 'grad_norm': 0.1586770862340927, 'learning_rate': 9e-05, 'epoch': 0.18, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.535, 'grad_norm': 0.1802036613225937, 'learning_rate': 0.0001, 'epoch': 0.2, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5528, 'grad_norm': 0.14322155714035034, 'learning_rate': 9.98458666866564e-05, 'epoch': 0.22, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5173, 'grad_norm': 0.14882728457450867, 'learning_rate': 9.938441702975689e-05, 'epoch': 0.24, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4589, 'grad_norm': 0.15893760323524475, 'learning_rate': 9.861849601988383e-05, 'epoch': 0.26, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4605, 'grad_norm': 0.18013568222522736, 'learning_rate': 9.755282581475769e-05, 'epoch': 0.28, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4946, 'grad_norm': 0.15864309668540955, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.3, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4814, 'grad_norm': 0.17999328672885895, 'learning_rate': 9.45503262094184e-05, 'epoch': 0.32, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5567, 'grad_norm': 0.16328884661197662, 'learning_rate': 9.263200821770461e-05, 'epoch': 0.34, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4659, 'grad_norm': 0.17083896696567535, 'learning_rate': 9.045084971874738e-05, 'epoch': 0.36, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4654, 'grad_norm': 0.15472805500030518, 'learning_rate': 8.802029828000156e-05, 'epoch': 0.38, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5649, 'grad_norm': 0.16025137901306152, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.4, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4585, 'grad_norm': 0.14991462230682373, 'learning_rate': 8.247240241650918e-05, 'epoch': 0.42, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5128, 'grad_norm': 0.17766867578029633, 'learning_rate': 7.938926261462366e-05, 'epoch': 0.44, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4421, 'grad_norm': 0.1716334968805313, 'learning_rate': 7.612492823579745e-05, 'epoch': 0.46, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4527, 'grad_norm': 0.171142578125, 'learning_rate': 7.269952498697734e-05, 'epoch': 0.48, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5197, 'grad_norm': 0.1809053122997284, 'learning_rate': 6.91341716182545e-05, 'epoch': 0.5, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5652, 'grad_norm': 0.16800864040851593, 'learning_rate': 6.545084971874738e-05, 'epoch': 0.52, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5429, 'grad_norm': 0.17211489379405975, 'learning_rate': 6.167226819279528e-05, 'epoch': 0.54, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5363, 'grad_norm': 0.1702817976474762, 'learning_rate': 5.782172325201155e-05, 'epoch': 0.56, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.72, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4728, 'grad_norm': 0.20073957741260529, 'learning_rate': 5.392295478639225e-05, 'epoch': 0.58, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4166, 'grad_norm': 0.17261633276939392, 'learning_rate': 5e-05, 'epoch': 0.6, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.512, 'grad_norm': 0.22016897797584534, 'learning_rate': 4.607704521360776e-05, 'epoch': 0.62, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.3826, 'grad_norm': 0.1705646961927414, 'learning_rate': 4.2178276747988446e-05, 'epoch': 0.64, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4806, 'grad_norm': 0.16235123574733734, 'learning_rate': 3.832773180720475e-05, 'epoch': 0.66, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4357, 'grad_norm': 0.16754426062107086, 'learning_rate': 3.4549150281252636e-05, 'epoch': 0.68, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5045, 'grad_norm': 0.17245657742023468, 'learning_rate': 3.086582838174551e-05, 'epoch': 0.7, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4022, 'grad_norm': 0.17801426351070404, 'learning_rate': 2.7300475013022663e-05, 'epoch': 0.72, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4951, 'grad_norm': 0.1816994845867157, 'learning_rate': 2.3875071764202563e-05, 'epoch': 0.74, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4872, 'grad_norm': 0.18613915145397186, 'learning_rate': 2.061073738537635e-05, 'epoch': 0.76, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4342, 'grad_norm': 0.18277928233146667, 'learning_rate': 1.7527597583490822e-05, 'epoch': 0.78, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4845, 'grad_norm': 0.1878129541873932, 'learning_rate': 1.4644660940672627e-05, 'epoch': 0.8, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4705, 'grad_norm': 0.18935205042362213, 'learning_rate': 1.1979701719998453e-05, 'epoch': 0.82, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4502, 'grad_norm': 0.19221247732639313, 'learning_rate': 9.549150281252633e-06, 'epoch': 0.84, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.519, 'grad_norm': 0.18282923102378845, 'learning_rate': 7.367991782295391e-06, 'epoch': 0.86, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4416, 'grad_norm': 0.19478771090507507, 'learning_rate': 5.449673790581611e-06, 'epoch': 0.88, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5206, 'grad_norm': 0.16863884031772614, 'learning_rate': 3.8060233744356633e-06, 'epoch': 0.9, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5493, 'grad_norm': 0.17342743277549744, 'learning_rate': 2.4471741852423237e-06, 'epoch': 0.92, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.3963, 'grad_norm': 0.16318970918655396, 'learning_rate': 1.3815039801161721e-06, 'epoch': 0.94, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4197, 'grad_norm': 0.20041875541210175, 'learning_rate': 6.15582970243117e-07, 'epoch': 0.96, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.5102, 'grad_norm': 0.17219312489032745, 'learning_rate': 1.5413331334360182e-07, 'epoch': 0.98, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.4163, 'grad_norm': 0.17957626283168793, 'learning_rate': 0.0, 'epoch': 1.0, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "{'train_runtime': 464.8313, 'train_samples_per_second': 8.605, 'train_steps_per_second': 1.076, 'train_loss': 1.4969593372344971, 'epoch': 1.0, 'memory_allocated (GB)': 48.79, 'max_memory_allocated (GB)': 88.74, 'total_memory_available (GB)': 94.62}\n",
      "100%|█████████████████████████████████████████| 500/500 [07:44<00:00,  1.08it/s]\n",
      "***** train metrics *****\n",
      "  epoch                       =         1.0\n",
      "  max_memory_allocated (GB)   =       88.74\n",
      "  memory_allocated (GB)       =       48.79\n",
      "  total_flos                  = 151326093GF\n",
      "  total_memory_available (GB) =       94.62\n",
      "  train_loss                  =       1.497\n",
      "  train_runtime               =  0:07:44.83\n",
      "  train_samples_per_second    =       8.605\n",
      "  train_steps_per_second      =       1.076\n"
     ]
    }
   ],
   "source": [
    "!python sft.py \\\n",
    "    --model_name_or_path meta-llama/Llama-2-7b-hf \\\n",
    "    --dataset_name \"lvwerra/stack-exchange-paired\" \\\n",
    "    --output_dir=\"./sft\" \\\n",
    "    --max_steps=500 \\\n",
    "    --logging_steps=10 \\\n",
    "    --save_steps=100 \\\n",
    "    --do_train \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --per_device_eval_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=2 \\\n",
    "    --learning_rate=1e-4 \\\n",
    "    --lr_scheduler_type=\"cosine\" \\\n",
    "    --warmup_steps=100 \\\n",
    "    --weight_decay=0.05 \\\n",
    "    --optim=\"paged_adamw_32bit\" \\\n",
    "    --lora_target_modules \"q_proj\" \"v_proj\" \\\n",
    "    --bf16 \\\n",
    "    --remove_unused_columns=False \\\n",
    "    --run_name=\"sft_llama2\" \\\n",
    "    --report_to=none \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce41f05-1eac-4472-ab65-f481c6ee3560",
   "metadata": {},
   "source": [
    "To merge the adaptors to get the final sft merged checkpoint, we can use the merge_peft_adapter.py helper script that comes with TRL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eae77b7-2202-43d4-999a-8c04b3b6dc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.88it/s]\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:159: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-10-19 01:59:45,238] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --base_model_name=\"meta-llama/Llama-2-7b-hf\" --adapter_model_name=\"sft\" --output_name=\"sft/final_merged_checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5158b",
   "metadata": {},
   "source": [
    "2. Run the DPO trainer using the model saved by the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ec49d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                       =     0.0024\n",
      "  max_memory_allocated (GB)   =      86.99\n",
      "  memory_allocated (GB)       =      60.24\n",
      "  total_flos                  =        0GF\n",
      "  total_memory_available (GB) =      94.62\n",
      "  train_loss                  =     1.4503\n",
      "  train_runtime               = 0:34:31.45\n",
      "  train_samples_per_second    =      1.931\n",
      "  train_steps_per_second      =      0.483\n"
     ]
    }
   ],
   "source": [
    "!python dpo.py \\\n",
    "    --model_name_or_path=\"sft/final_merged_checkpoint\" \\\n",
    "    --tokenizer_name_or_path=meta-llama/Llama-2-7b-hf \\\n",
    "    --lora_target_modules \"q_proj\" \"v_proj\" \"k_proj\" \"out_proj\" \"fc_in\" \"fc_out\" \"wte\" \\\n",
    "    --output_dir=\"dpo\" \\\n",
    "    --report_to=none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683be1b",
   "metadata": {},
   "source": [
    "#### Merging the adaptors\n",
    "\n",
    "To merge the adaptors into the base model we can use the merge_peft_adapter.py helper script that comes with TRL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "350dcfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.84it/s]\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:159: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-10-19 03:46:58,894] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --base_model_name=\"meta-llama/Llama-2-7b-hf\" --adapter_model_name=\"dpo\" --output_name=\"stack-llama-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d0ef89",
   "metadata": {},
   "source": [
    "which will also push the model to your HuggingFace hub account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4375ad4",
   "metadata": {},
   "source": [
    "#### Running the model\n",
    "\n",
    "We can load the DPO-trained LoRA adaptors which were saved by the DPO training step and run it through the [text-generation example]([../text-generation/](https://github.com/huggingface/optimum-habana/tree/main/examples/text-generation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8750d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/text-generation\n",
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "10/19/2024 03:47:19 - INFO - __main__ - Single-device run.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00,  6.78it/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056412772 KB\n",
      "------------------------------------------------------------------------------\n",
      "10/19/2024 03:47:25 - INFO - __main__ - Args: Namespace(device='hpu', model_name_or_path='../trl/stack-llama-2/', bf16=True, max_new_tokens=50, max_input_tokens=0, batch_size=1, warmup=3, n_iterations=5, local_rank=0, use_kv_cache=True, use_hpu_graphs=True, dataset_name=None, column_name=None, do_sample=True, num_beams=1, top_k=None, penalty_alpha=None, trim_logits=False, seed=27, profiling_warmup_steps=0, profiling_steps=0, profiling_record_shapes=False, prompt=['When I go to New York I always go see '], bad_words=None, force_words=None, assistant_model=None, peft_model=None, num_return_sequences=1, token=None, model_revision='main', attn_softmax_bf16=False, output_dir=None, bucket_size=-1, bucket_internal=False, dataset_max_samples=-1, limit_hpu_graphs=False, show_graphs_count=False, reuse_cache=False, verbose_workers=False, simulate_dyn_prompt=None, reduce_recompile=False, use_flash_attention=False, flash_attention_recompute=False, flash_attention_causal_mask=False, flash_attention_fast_softmax=False, book_source=False, torch_compile=False, ignore_eos=True, temperature=0.5, top_p=0.5, const_serialization_path=None, trust_remote_code=False, parallel_strategy='none', input_embeds=False, run_partial_dataset=False, load_quantized_model_with_autogptq=False, disk_offload=False, load_quantized_model_with_inc=False, local_quantized_inc_model_path=None, quant_config='', world_size=0, global_rank=0)\n",
      "10/19/2024 03:47:25 - INFO - __main__ - device: hpu, n_hpu: 0, bf16: True\n",
      "10/19/2024 03:47:25 - INFO - __main__ - Model initialization took 7.797s\n",
      "10/19/2024 03:47:25 - INFO - __main__ - Graph compilation...\n",
      "Warming up iteration 1/3\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "10/19/2024 03:47:26 - INFO - __main__ - Time to first token = 356.9162858184427ms\n",
      "Warming up iteration 2/3\n",
      "10/19/2024 03:47:27 - INFO - __main__ - Time to first token = 15.537630999460816ms\n",
      "Warming up iteration 3/3\n",
      "10/19/2024 03:47:27 - INFO - __main__ - Time to first token = 11.140466202050447ms\n",
      "10/19/2024 03:47:27 - INFO - __main__ - Running generate...\n",
      "10/19/2024 03:47:27 - INFO - __main__ - Time to first token = 11.067672166973352ms\n",
      "10/19/2024 03:47:28 - INFO - __main__ - Time to first token = 10.953170945867896ms\n",
      "10/19/2024 03:47:28 - INFO - __main__ - Time to first token = 10.996290948241949ms\n",
      "10/19/2024 03:47:28 - INFO - __main__ - Time to first token = 10.924611939117312ms\n",
      "10/19/2024 03:47:29 - INFO - __main__ - Time to first token = 10.939687956124544ms\n",
      "\n",
      "Input/outputs:\n",
      "input 1: ('When I go to New York I always go see ',)\n",
      "output 1: ('When I go to New York I always go see 1980s.\\nI have a car, a house and a cat. I also have a job, a job, a job and a job.\\nI have a car, a house and a cat. I also have a job',)\n",
      "\n",
      "\n",
      "Stats:\n",
      "----------------------------------------------------------------------------------\n",
      "Input tokens\n",
      "Throughput (including tokenization) = 129.8643127622316 tokens/second\n",
      "Memory allocated                    = 12.66 GB\n",
      "Max memory allocated                = 12.66 GB\n",
      "Total memory available              = 94.62 GB\n",
      "Graph compilation duration          = 1.7309602710884064 seconds\n",
      "----------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%!python ~/optimum-habana/examples/text-generation/run_generation.py \\\n",
    "--model_name_or_path stack-llama-2/ \\\n",
    "--use_hpu_graphs --use_kv_cache --batch_size 1 --bf16 --do_sample --max_new_tokens 50 \\\n",
    "--temperature 0.5 \\\n",
    "--top_p 0.5 \\\n",
    "--prompt \"When I go to New York I always go see \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae666d1-7e11-43fe-a73a-0f1edf047eb5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You now have access to all the Models in Model-References and Optimum-Habana repositories, you can start to look at other models.  Remember that all the models in these repositories are fully documented so they are easy to use.\n",
    "* To explore more models from the Model References, start [here](https://github.com/HabanaAI/Model-References).  \n",
    "* To run more examples using Hugging Face go [here](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models).  \n",
    "* To migrate other models to Gaudi 2, refer to PyTorch Model Porting in the [documentation](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddf63e-263f-4de1-bed1-a959588b7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
