{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00676b-8d19-4e0d-b61d-68ccc14c294d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714683-1b37-4048-bf91-2b5af1c18d40",
   "metadata": {},
   "source": [
    "# Intel® Gaudi® Accelerator Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db2bc-12ef-47a0-83f8-cfba2a8af03c",
   "metadata": {},
   "source": [
    "\n",
    "This document provides instructions on setting up the Intel Gaudi 2 AI accelerator Instance on the Intel® Developer Cloud or any on-premise Intel Gaudi Node. You will be running models from the Intel Gaudi software Model References and the Hugging Face Optimum Habana library.\n",
    "\n",
    "Please follow along with the [video](https://developer.habana.ai/intel-developer-cloud/) on our Developer Page to walk through the steps below.  This assumes that you have setup the latest Intel Gaudi PyTorch Docker image.\n",
    "\n",
    "To set up a multi-node instance with two or more Gaudi nodes, refer to Setting up Multiple Gaudi Nodes in the [Quick Start Guide Documentation](https://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodeshttps://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodes).  \n",
    "\n",
    "The first step is to install the Model-References repository from GitHub and run the \"hello-world\" model from the examples library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ca618d-e3bc-4c36-998d-28ec94b10b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
      "fatal: destination path 'Model-References' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone -b 1.15.0 https://github.com/HabanaAI/Model-References.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de76764c-9a4d-4d44-92f9-d9dfee9906e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/Model-References/PyTorch/examples/computer_vision/hello_world\n"
     ]
    }
   ],
   "source": [
    "%cd Model-References/PyTorch/examples/computer_vision/hello_world/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423f41a",
   "metadata": {},
   "source": [
    "#### Setup the execution environment path and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20088609-bd31-4b3d-b208-05aa6490f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path=['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '~/.local/lib/python3.10/site-packages', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages',\"~/.local/bin\",\"/usr/bin/habanatools\",\"/usr/local/sbin\",\"/usr/local/bin\",\"/usr/sbin\",\"/usr/bin\",\"/sbin\",\"/bin\",\"/usr/games\",\"/usr/local/games\",\"/snap/bin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e5d92",
   "metadata": {},
   "source": [
    "Set the appropriate ENV Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bedb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONPATH']='~/Model-References,/usr/lib/habanalabs/'\n",
    "os.environ['DATA_LOADER_AEON_LIB_PATH'] = '/usr/lib/habanalabs/libaeon.so'\n",
    "os.environ['GC_KERNEL_PATH'] = '/usr/lib/habanalabs/libtpc_kernels.so'\n",
    "os.environ['HABANA_PLUGINS_LIB_PATH'] = '/opt/habanalabs/habana_plugins'\n",
    "os.environ['HABANA_SCAL_BIN_PATH'] = '/opt/habanalabs/engines_fw'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de0ff4",
   "metadata": {},
   "source": [
    "We now run the simple example with the MNIST dataset on one Intel Gaudi card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bc8e49-f883-4162-a74a-bdeb7d309598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 96733142.65it/s]\n",
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "100%|███████████████████████████████| 28881/28881 [00:00<00:00, 64536864.05it/s]\n",
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "100%|███████████████████████████| 1648877/1648877 [00:00<00:00, 49498574.97it/s]\n",
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "100%|█████████████████████████████████| 4542/4542 [00:00<00:00, 18495659.00it/s]\n",
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056399524 KB\n",
      "------------------------------------------------------------------------------\n",
      "Train Epoch: 1 [0/60000.0 (0%)]\tLoss: 2.296875\n",
      "Train Epoch: 1 [640/60000.0 (1%)]\tLoss: 1.492188\n",
      "Train Epoch: 1 [1280/60000.0 (2%)]\tLoss: 0.699219\n",
      "Train Epoch: 1 [1920/60000.0 (3%)]\tLoss: 0.449219\n",
      "Train Epoch: 1 [2560/60000.0 (4%)]\tLoss: 0.208008\n",
      "Train Epoch: 1 [3200/60000.0 (5%)]\tLoss: 0.337891\n",
      "Train Epoch: 1 [3840/60000.0 (6%)]\tLoss: 0.150391\n",
      "Train Epoch: 1 [4480/60000.0 (7%)]\tLoss: 0.229492\n",
      "Train Epoch: 1 [5120/60000.0 (9%)]\tLoss: 0.322266\n",
      "Train Epoch: 1 [5760/60000.0 (10%)]\tLoss: 0.181641\n",
      "Train Epoch: 1 [6400/60000.0 (11%)]\tLoss: 0.144531\n",
      "Train Epoch: 1 [7040/60000.0 (12%)]\tLoss: 0.172852\n",
      "Train Epoch: 1 [7680/60000.0 (13%)]\tLoss: 0.202148\n",
      "Train Epoch: 1 [8320/60000.0 (14%)]\tLoss: 0.078125\n",
      "Train Epoch: 1 [8960/60000.0 (15%)]\tLoss: 0.126953\n",
      "Train Epoch: 1 [9600/60000.0 (16%)]\tLoss: 0.153320\n",
      "Train Epoch: 1 [10240/60000.0 (17%)]\tLoss: 0.148438\n",
      "Train Epoch: 1 [10880/60000.0 (18%)]\tLoss: 0.083008\n",
      "Train Epoch: 1 [11520/60000.0 (19%)]\tLoss: 0.550781\n",
      "Train Epoch: 1 [12160/60000.0 (20%)]\tLoss: 0.152344\n",
      "Train Epoch: 1 [12800/60000.0 (21%)]\tLoss: 0.099121\n",
      "Train Epoch: 1 [13440/60000.0 (22%)]\tLoss: 0.203125\n",
      "Train Epoch: 1 [14080/60000.0 (23%)]\tLoss: 0.073730\n",
      "Train Epoch: 1 [14720/60000.0 (25%)]\tLoss: 0.248047\n",
      "Train Epoch: 1 [15360/60000.0 (26%)]\tLoss: 0.151367\n",
      "Train Epoch: 1 [16000/60000.0 (27%)]\tLoss: 0.175781\n",
      "Train Epoch: 1 [16640/60000.0 (28%)]\tLoss: 0.186523\n",
      "Train Epoch: 1 [17280/60000.0 (29%)]\tLoss: 0.073242\n",
      "Train Epoch: 1 [17920/60000.0 (30%)]\tLoss: 0.125977\n",
      "Train Epoch: 1 [18560/60000.0 (31%)]\tLoss: 0.120605\n",
      "Train Epoch: 1 [19200/60000.0 (32%)]\tLoss: 0.294922\n",
      "Train Epoch: 1 [19840/60000.0 (33%)]\tLoss: 0.188477\n",
      "Train Epoch: 1 [20480/60000.0 (34%)]\tLoss: 0.056396\n",
      "Train Epoch: 1 [21120/60000.0 (35%)]\tLoss: 0.178711\n",
      "Train Epoch: 1 [21760/60000.0 (36%)]\tLoss: 0.024048\n",
      "Train Epoch: 1 [22400/60000.0 (37%)]\tLoss: 0.033691\n",
      "Train Epoch: 1 [23040/60000.0 (38%)]\tLoss: 0.187500\n",
      "Train Epoch: 1 [23680/60000.0 (39%)]\tLoss: 0.233398\n",
      "Train Epoch: 1 [24320/60000.0 (41%)]\tLoss: 0.037354\n",
      "Train Epoch: 1 [24960/60000.0 (42%)]\tLoss: 0.012756\n",
      "Train Epoch: 1 [25600/60000.0 (43%)]\tLoss: 0.025513\n",
      "Train Epoch: 1 [26240/60000.0 (44%)]\tLoss: 0.075684\n",
      "Train Epoch: 1 [26880/60000.0 (45%)]\tLoss: 0.166016\n",
      "Train Epoch: 1 [27520/60000.0 (46%)]\tLoss: 0.136719\n",
      "Train Epoch: 1 [28160/60000.0 (47%)]\tLoss: 0.150391\n",
      "Train Epoch: 1 [28800/60000.0 (48%)]\tLoss: 0.124023\n",
      "Train Epoch: 1 [29440/60000.0 (49%)]\tLoss: 0.051758\n",
      "Train Epoch: 1 [30080/60000.0 (50%)]\tLoss: 0.123047\n",
      "Train Epoch: 1 [30720/60000.0 (51%)]\tLoss: 0.078125\n",
      "Train Epoch: 1 [31360/60000.0 (52%)]\tLoss: 0.086426\n",
      "Train Epoch: 1 [32000/60000.0 (53%)]\tLoss: 0.095215\n",
      "Train Epoch: 1 [32640/60000.0 (54%)]\tLoss: 0.053955\n",
      "Train Epoch: 1 [33280/60000.0 (55%)]\tLoss: 0.211914\n",
      "Train Epoch: 1 [33920/60000.0 (57%)]\tLoss: 0.014404\n",
      "Train Epoch: 1 [34560/60000.0 (58%)]\tLoss: 0.080566\n",
      "Train Epoch: 1 [35200/60000.0 (59%)]\tLoss: 0.243164\n",
      "Train Epoch: 1 [35840/60000.0 (60%)]\tLoss: 0.092773\n",
      "Train Epoch: 1 [36480/60000.0 (61%)]\tLoss: 0.063965\n",
      "Train Epoch: 1 [37120/60000.0 (62%)]\tLoss: 0.074219\n",
      "Train Epoch: 1 [37760/60000.0 (63%)]\tLoss: 0.101074\n",
      "Train Epoch: 1 [38400/60000.0 (64%)]\tLoss: 0.089844\n",
      "Train Epoch: 1 [39040/60000.0 (65%)]\tLoss: 0.008972\n",
      "Train Epoch: 1 [39680/60000.0 (66%)]\tLoss: 0.028320\n",
      "Train Epoch: 1 [40320/60000.0 (67%)]\tLoss: 0.091309\n",
      "Train Epoch: 1 [40960/60000.0 (68%)]\tLoss: 0.084961\n",
      "Train Epoch: 1 [41600/60000.0 (69%)]\tLoss: 0.062012\n",
      "Train Epoch: 1 [42240/60000.0 (70%)]\tLoss: 0.019531\n",
      "Train Epoch: 1 [42880/60000.0 (71%)]\tLoss: 0.156250\n",
      "Train Epoch: 1 [43520/60000.0 (72%)]\tLoss: 0.170898\n",
      "Train Epoch: 1 [44160/60000.0 (74%)]\tLoss: 0.010132\n",
      "Train Epoch: 1 [44800/60000.0 (75%)]\tLoss: 0.121582\n",
      "Train Epoch: 1 [45440/60000.0 (76%)]\tLoss: 0.135742\n",
      "Train Epoch: 1 [46080/60000.0 (77%)]\tLoss: 0.120605\n",
      "Train Epoch: 1 [46720/60000.0 (78%)]\tLoss: 0.110352\n",
      "Train Epoch: 1 [47360/60000.0 (79%)]\tLoss: 0.130859\n",
      "Train Epoch: 1 [48000/60000.0 (80%)]\tLoss: 0.067871\n",
      "Train Epoch: 1 [48640/60000.0 (81%)]\tLoss: 0.075684\n",
      "Train Epoch: 1 [49280/60000.0 (82%)]\tLoss: 0.005615\n",
      "Train Epoch: 1 [49920/60000.0 (83%)]\tLoss: 0.064453\n",
      "Train Epoch: 1 [50560/60000.0 (84%)]\tLoss: 0.080566\n",
      "Train Epoch: 1 [51200/60000.0 (85%)]\tLoss: 0.187500\n",
      "Train Epoch: 1 [51840/60000.0 (86%)]\tLoss: 0.078613\n",
      "Train Epoch: 1 [52480/60000.0 (87%)]\tLoss: 0.008118\n",
      "Train Epoch: 1 [53120/60000.0 (88%)]\tLoss: 0.174805\n",
      "Train Epoch: 1 [53760/60000.0 (90%)]\tLoss: 0.104980\n",
      "Train Epoch: 1 [54400/60000.0 (91%)]\tLoss: 0.017456\n",
      "Train Epoch: 1 [55040/60000.0 (92%)]\tLoss: 0.026001\n",
      "Train Epoch: 1 [55680/60000.0 (93%)]\tLoss: 0.114746\n",
      "Train Epoch: 1 [56320/60000.0 (94%)]\tLoss: 0.045410\n",
      "Train Epoch: 1 [56960/60000.0 (95%)]\tLoss: 0.037354\n",
      "Train Epoch: 1 [57600/60000.0 (96%)]\tLoss: 0.127930\n",
      "Train Epoch: 1 [58240/60000.0 (97%)]\tLoss: 0.008240\n",
      "Train Epoch: 1 [58880/60000.0 (98%)]\tLoss: 0.010071\n",
      "Train Epoch: 1 [59520/60000.0 (99%)]\tLoss: 0.001114\n",
      "\n",
      "Total test set: 10000, number of workers: 1\n",
      "* Average Acc 98.640 Average loss 0.046\n"
     ]
    }
   ],
   "source": [
    "%run mnist.py --batch-size=64 --epochs=1 --lr=1.0 --gamma=0.7 --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37216139-611e-4b07-a90f-cb351f18b185",
   "metadata": {},
   "source": [
    "### Fine-tuning with Hugging Face Optimum Habana Library\n",
    "The Optimum Habana library is the interface between the Hugging Face Transformers and Diffusers libraries and the Gaudi 2 card. It provides a set of tools enabling easy model loading, training and inference on single and multi-card settings for different downstream tasks. The following example uses the text-classification task to fine-tune a BERT-Large model with the MRPC (Microsoft Research Paraphrase Corpus) dataset and also run Inference.\n",
    "\n",
    "Follow the below steps to install the stable release from the Optimum Habana examples and library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420be01-fb88-466a-9fe7-87006340905f",
   "metadata": {},
   "source": [
    "1. Clone the Optimum-Habana project and check out the lastest stable release.  This repository gives access to the examples that are optimized for Intel Gaudi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07346c1a-1fea-4b62-8a79-760ca7a64073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
      "fatal: destination path 'optimum-habana' already exists and is not an empty directory.\n",
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana\n",
      "HEAD is now at 1dfbc02 Release: v1.10.4\n",
      "/root\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials\n",
    "!git clone -b v1.11.0 https://github.com/huggingface/optimum-habana.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ded02-2aa1-4725-b1fc-cf917e05d9aa",
   "metadata": {},
   "source": [
    "2. Install Optimum-Habana library. This will install the latest stable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "983e35cc-0ea1-4205-ae70-323252169c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet optimum-habana==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627879-a273-4914-8efd-68c9a81ebbb9",
   "metadata": {},
   "source": [
    "3. In order to use the DeepSpeed library on Intel Gaudi 2, install the Intel Gaudi DeepSpeed fork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59e99a7-4db0-4519-b406-ccedee40796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet git+https://github.com/HabanaAI/DeepSpeed.git@1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f0546-e74c-4363-b443-a0f59504d973",
   "metadata": {},
   "source": [
    "The following example is based on the Optimum-Habana Text Classification task example. Change to the text-classification directory and install the additional SW requirements for this specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24537f40-8daa-4ac9-ad19-bf1cfaaf29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-classification\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Single_card_tutorials/optimum-habana/examples/text-classification/\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88922f6b-527f-4bc0-bb47-4adea631ef9c",
   "metadata": {},
   "source": [
    "### Execute Single-Card Training\n",
    "This run instruction will fine-tune the BERT-Large Model on one Intel Gaudi card:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8305374a-21fe-4376-ab39-0c6de4222eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/21/2024 15:48:42 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: False, mixed-precision training: True\n",
      "03/21/2024 15:48:42 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/bert-large-uncased-whole-word-masking,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output/mrpc/runs/Mar21_15-48-40_hls2-srv01-demolab,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./output/mrpc/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./output/mrpc/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=True,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/21/2024 15:48:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/21/2024 15:48:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/21/2024 15:48:48 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/21/2024 15:48:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:729] 2024-03-21 15:48:48,783 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-21 15:48:48,788 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:729] 2024-03-21 15:48:48,878 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-21 15:48:48,881 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:48:48,883 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:48:48,883 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:48:48,883 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:48:48,883 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:48:48,883 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:729] 2024-03-21 15:48:48,884 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-21 15:48:48,886 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3478] 2024-03-21 15:48:48,977 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/model.safetensors\n",
      "[INFO|modeling_utils.py:4342] 2024-03-21 15:48:49,360 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4354] 2024-03-21 15:48:49,360 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-97660376e376d148.arrow\n",
      "03/21/2024 15:48:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-97660376e376d148.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-24ea3a949f031485.arrow\n",
      "03/21/2024 15:48:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-24ea3a949f031485.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6a812eb7b2b9cb22.arrow\n",
      "03/21/2024 15:48:49 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6a812eb7b2b9cb22.arrow\n",
      "03/21/2024 15:48:49 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/21/2024 15:48:49 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "03/21/2024 15:48:49 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056399524 KB\n",
      "------------------------------------------------------------------------------\n",
      "[INFO|trainer.py:718] 2024-03-21 15:48:54,071 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:726] 2024-03-21 15:48:54,132 >> ***** Running training *****\n",
      "[INFO|trainer.py:727] 2024-03-21 15:48:54,132 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:728] 2024-03-21 15:48:54,132 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:729] 2024-03-21 15:48:54,132 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:732] 2024-03-21 15:48:54,132 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:733] 2024-03-21 15:48:54,132 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:734] 2024-03-21 15:48:54,133 >>   Total optimization steps = 345\n",
      "[INFO|trainer.py:735] 2024-03-21 15:48:54,138 >>   Number of trainable parameters = 335,143,938\n",
      "100%|█████████████████████████████████████████| 345/345 [01:07<00:00,  6.48it/s][INFO|trainer.py:1011] 2024-03-21 15:50:01,987 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 67.8507, 'train_samples_per_second': 214.097, 'train_steps_per_second': 6.713, 'train_loss': 0.3109767526820086, 'epoch': 3.0, 'memory_allocated (GB)': 7.47, 'max_memory_allocated (GB)': 9.98, 'total_memory_available (GB)': 94.62}\n",
      "100%|█████████████████████████████████████████| 345/345 [01:07<00:00,  5.08it/s]\n",
      "[INFO|trainer.py:1537] 2024-03-21 15:50:01,993 >> Saving model checkpoint to ./output/mrpc/\n",
      "[INFO|configuration_utils.py:473] 2024-03-21 15:50:02,718 >> Configuration saved in ./output/mrpc/config.json\n",
      "[INFO|modeling_utils.py:2495] 2024-03-21 15:50:04,186 >> Model weights saved in ./output/mrpc/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2433] 2024-03-21 15:50:04,186 >> tokenizer config file saved in ./output/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2442] 2024-03-21 15:50:04,186 >> Special tokens file saved in ./output/mrpc/special_tokens_map.json\n",
      "[INFO|configuration_utils.py:113] 2024-03-21 15:50:04,200 >> Configuration saved in ./output/mrpc/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =        3.0\n",
      "  max_memory_allocated (GB)   =       9.98\n",
      "  memory_allocated (GB)       =       7.47\n",
      "  total_memory_available (GB) =      94.62\n",
      "  train_loss                  =      0.311\n",
      "  train_runtime               = 0:01:07.85\n",
      "  train_samples               =       3668\n",
      "  train_samples_per_second    =    214.097\n",
      "  train_steps_per_second      =      6.713\n",
      "03/21/2024 15:50:04 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:718] 2024-03-21 15:50:04,202 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:1619] 2024-03-21 15:50:04,206 >> Using HPU graphs for inference.\n",
      "[INFO|trainer.py:1639] 2024-03-21 15:50:04,207 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1641] 2024-03-21 15:50:04,207 >>   Num examples = 408\n",
      "[INFO|trainer.py:1644] 2024-03-21 15:50:04,207 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 160.87it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                       =        3.0\n",
      "  eval_accuracy               =     0.8725\n",
      "  eval_combined_score         =     0.8918\n",
      "  eval_f1                     =      0.911\n",
      "  eval_loss                   =     0.4017\n",
      "  eval_runtime                = 0:00:01.72\n",
      "  eval_samples                =        408\n",
      "  eval_samples_per_second     =    236.926\n",
      "  eval_steps_per_second       =     29.616\n",
      "  max_memory_allocated (GB)   =       9.98\n",
      "  memory_allocated (GB)       =       7.47\n",
      "  total_memory_available (GB) =      94.62\n"
     ]
    }
   ],
   "source": [
    "%run run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc   \\\n",
    "--do_train   \\\n",
    "--do_eval   \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128   \\\n",
    "--output_dir ./output/mrpc/  \\\n",
    "--use_habana  \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16   \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir \\\n",
    "--throughput_warmup_steps 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23453-cbec-4582-98dd-0dc202499da7",
   "metadata": {},
   "source": [
    "### Inference Example Run\n",
    "Using inference will run the same evaluation metrics (accuracy, F1 score) as shown above. This will display how well the model has performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "182c55ed-33dd-4b0a-9fe5-ecfab8ba51ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/21/2024 15:50:17 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: False, mixed-precision training: True\n",
      "03/21/2024 15:50:17 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/bert-large-uncased-whole-word-masking,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output/mrpc/runs/Mar21_15-50-14_hls2-srv01-demolab,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./output/mrpc/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./output/mrpc/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "throughput_warmup_steps=0,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=True,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "03/21/2024 15:50:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/21/2024 15:50:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "03/21/2024 15:50:23 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "03/21/2024 15:50:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
      "[INFO|configuration_utils.py:729] 2024-03-21 15:50:24,206 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-21 15:50:24,211 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:729] 2024-03-21 15:50:24,303 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-21 15:50:24,306 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:50:24,308 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:50:24,308 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:50:24,308 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:50:24,308 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2027] 2024-03-21 15:50:24,308 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:729] 2024-03-21 15:50:24,309 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-21 15:50:24,312 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.37.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3478] 2024-03-21 15:50:24,403 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/model.safetensors\n",
      "[INFO|modeling_utils.py:4342] 2024-03-21 15:50:24,787 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4354] 2024-03-21 15:50:24,788 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-97660376e376d148.arrow\n",
      "03/21/2024 15:50:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-97660376e376d148.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-24ea3a949f031485.arrow\n",
      "03/21/2024 15:50:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-24ea3a949f031485.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6a812eb7b2b9cb22.arrow\n",
      "03/21/2024 15:50:24 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6a812eb7b2b9cb22.arrow\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056399524 KB\n",
      "------------------------------------------------------------------------------\n",
      "[WARNING|trainer.py:222] 2024-03-21 15:50:29,010 >> The argument `--bf16` was not given but `use_torch_autocast` is True in the Gaudi configuration so mixed-precision training with Torch Autocast is enabled.\n",
      "03/21/2024 15:50:29 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:718] 2024-03-21 15:50:29,012 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, idx, sentence1. If sentence2, idx, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:1619] 2024-03-21 15:50:29,057 >> Using HPU graphs for inference.\n",
      "[INFO|trainer.py:1639] 2024-03-21 15:50:29,059 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1641] 2024-03-21 15:50:29,059 >>   Num examples = 408\n",
      "[INFO|trainer.py:1644] 2024-03-21 15:50:29,059 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 155.82it/s]\n",
      "***** eval metrics *****\n",
      "  eval_accuracy               =      0.598\n",
      "  eval_combined_score         =     0.6289\n",
      "  eval_f1                     =     0.6598\n",
      "  eval_loss                   =     0.6521\n",
      "  eval_runtime                = 0:00:02.37\n",
      "  eval_samples                =        408\n",
      "  eval_samples_per_second     =    171.625\n",
      "  eval_steps_per_second       =     21.453\n",
      "  max_memory_allocated (GB)   =       1.29\n",
      "  memory_allocated (GB)       =       1.29\n",
      "  total_memory_available (GB) =      94.62\n",
      "[INFO|modelcard.py:452] 2024-03-21 15:50:31,608 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'dataset': {'name': 'GLUE MRPC', 'type': 'glue', 'args': 'mrpc'}}\n"
     ]
    }
   ],
   "source": [
    "%run run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_eval \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ./output/mrpc/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae666d1-7e11-43fe-a73a-0f1edf047eb5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You now have access to all the Models in Model-References and Optimum-Habana repositories, you can start to look at other models.  Remember that all the models in these repositories are fully documented so they are easy to use.\n",
    "* To explore more models from the Model References, start [here](https://github.com/HabanaAI/Model-References).  \n",
    "* To run more examples using Hugging Face go [here](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models).  \n",
    "* To migrate other models to Gaudi 2, refer to PyTorch Model Porting in the [documentation](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75a7a4-214e-4b1b-8ec8-5a202758709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
