{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00676b-8d19-4e0d-b61d-68ccc14c294d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714683-1b37-4048-bf91-2b5af1c18d40",
   "metadata": {},
   "source": [
    "# Intel® Gaudi® Accelerator Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db2bc-12ef-47a0-83f8-cfba2a8af03c",
   "metadata": {},
   "source": [
    "\n",
    "This document provides instructions on setting up the Intel Gaudi 2 AI accelerator Instance on the Intel® Developer Cloud or any on-premise Intel Gaudi Node. You will be running models from the Intel Gaudi software Model References and the Hugging Face Optimum Habana library.\n",
    "\n",
    "Please follow along with the [video](https://developer.habana.ai/intel-developer-cloud/) on our Developer Page to walk through the steps below.  This assumes that you have setup the latest Intel Gaudi PyTorch Docker image.\n",
    "\n",
    "To set up a multi-node instance with two or more Gaudi nodes, refer to Setting up Multiple Gaudi Nodes in the [Quick Start Guide Documentation](https://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodeshttps://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodes).  \n",
    "\n",
    "The first step is to install the Model-References repository from GitHub and run the \"hello-world\" model from the examples library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca618d-e3bc-4c36-998d-28ec94b10b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Intel_Gaudi_Quickstart\n",
    "!git clone -b 1.15.0 https://github.com/HabanaAI/Model-References.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76764c-9a4d-4d44-92f9-d9dfee9906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Model-References/PyTorch/examples/computer_vision/hello_world/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423f41a",
   "metadata": {},
   "source": [
    "We set the correct paths for the python execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20088609-bd31-4b3d-b208-05aa6490f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONPATH'] = '$PYTHONPATH:~/Model-References'\n",
    "os.environ['PYTHON'] = '/usr/bin/python3.10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de0ff4",
   "metadata": {},
   "source": [
    "We now run the simple example with the MNIST dataset on one Intel Gaudi card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc8e49-f883-4162-a74a-bdeb7d309598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 mnist.py --batch-size=64 --epochs=1 --lr=1.0 --gamma=0.7 --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0421856",
   "metadata": {},
   "source": [
    "We can now run the same model on eight Intel Gaudi cards using mpirun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c6a3e-d613-447d-8476-c7b13f6d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 8 --bind-to core --map-by slot:PE=6 \\\n",
    "      --rank-by core --report-bindings \\\n",
    "      --allow-run-as-root \\\n",
    "      python3 mnist.py \\\n",
    "      --batch-size=64 --epochs=1 \\\n",
    "      --lr=1.0 --gamma=0.7 \\\n",
    "      --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37216139-611e-4b07-a90f-cb351f18b185",
   "metadata": {},
   "source": [
    "### Fine-tuning with Hugging Face Optimum Habana Library\n",
    "The Optimum Habana library is the interface between the Hugging Face Transformers and Diffusers libraries and the Gaudi 2 card. It provides a set of tools enabling easy model loading, training and inference on single and multi-card settings for different downstream tasks. The following example uses the text-classification task to fine-tune a BERT-Large model with the MRPC (Microsoft Research Paraphrase Corpus) dataset and also run Inference.\n",
    "\n",
    "Follow the below steps to install the stable release from the Optimum Habana examples and library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420be01-fb88-466a-9fe7-87006340905f",
   "metadata": {},
   "source": [
    "1. Clone the Optimum-Habana project and check out the lastest stable release.  This repository gives access to the examples that are optimized for Intel Gaudi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07346c1a-1fea-4b62-8a79-760ca7a64073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.11.0 https://github.com/huggingface/optimum-habana.git\n",
    "%cd ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ded02-2aa1-4725-b1fc-cf917e05d9aa",
   "metadata": {},
   "source": [
    "2. Install Optimum-Habana library. This will install the latest stable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e35cc-0ea1-4205-ae70-323252169c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet optimum-habana==1.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627879-a273-4914-8efd-68c9a81ebbb9",
   "metadata": {},
   "source": [
    "3. In order to use the DeepSpeed library on Intel Gaudi 2, install the Intel Gaudi DeepSpeed fork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e99a7-4db0-4519-b406-ccedee40796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f0546-e74c-4363-b443-a0f59504d973",
   "metadata": {},
   "source": [
    "The following example is based on the Optimum-Habana Text Classification task example. Change to the text-classification directory and install the additional SW requirements for this specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24537f40-8daa-4ac9-ad19-bf1cfaaf29f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/text-classification/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88922f6b-527f-4bc0-bb47-4adea631ef9c",
   "metadata": {},
   "source": [
    "### Execute Single-Card Training\n",
    "This run instruction will fine-tune the BERT-Large Model on one Intel Gaudi card:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305374a-21fe-4376-ab39-0c6de4222eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc   \\\n",
    "--do_train   \\\n",
    "--do_eval   \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128   \\\n",
    "--output_dir ./output/mrpc/  \\\n",
    "--use_habana  \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16   \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba73e4-2918-4aac-b234-a20f55a0111c",
   "metadata": {},
   "source": [
    "### Execute Multi-Card Training\n",
    "In this example, you will be doing the same fine-tuning task with eight Gaudi 2 cards.   In this case the Optimum-Habana models repository has a helper script called `gaudi_spawn.py` that manages multi card execution.  \n",
    "Notice the execution time for the fine-tuning compared to the single-card run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9e408-1d2d-467f-b6e1-311905dc4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../gaudi_spawn.py  --world_size 8 --use_mpi run_glue.py  \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking  \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc  \\\n",
    "--do_train  \\\n",
    "--do_eval  \\\n",
    "--per_device_train_batch_size 32  \\\n",
    "--per_device_eval_batch_size 8  \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128  \\\n",
    "--output_dir /tmp/mrpc_output/  \\\n",
    "--use_habana   \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16    \\\n",
    "--use_hpu_graphs_for_inference  \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628edd21-ebcc-43ac-a34a-1aa31133a268",
   "metadata": {},
   "source": [
    "### Training with DeepSpeed\n",
    "With the DeepSpeed package already installed, run multi-card training with DeepSpeed. The command below will create and point to a ds_config.json file to set up the parameters of the DeepSpeed run. Once the ds_config.json file is created, you can run the DeepSpeed training command below. \n",
    "\n",
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed and will still execute on eight Intel Gaudi 2 Accelerators\n",
    "\n",
    "In this case, we will run the ZeRO2 optimizer and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bda36-56e7-4712-806c-fd94626d4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c79b54-0af7-4b41-9bdf-d3d36271074c",
   "metadata": {},
   "source": [
    "This is the DeepSpeed run command for the bert-fine tuning.   At the completion of the run, compare the runtime and Max Memory usage with the non-DeepSpeed run above, you will see even faster execution and reduced memory consumption.  With larger models these advantages of using DeepSpeed are very important for running Large Language and Generative AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569eb0e8-e998-4c69-a07c-058583e67ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 3 \\\n",
    "--max_seq_length 128 \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir /tmp/mrpc_output/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir  \\\n",
    "--deepspeed ds_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23453-cbec-4582-98dd-0dc202499da7",
   "metadata": {},
   "source": [
    "### Inference Example Run\n",
    "Using inference will run the same evaluation metrics (accuracy, F1 score) as shown above. This will display how well the model has performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c55ed-33dd-4b0a-9fe5-ecfab8ba51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_eval \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ./output/mrpc/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae666d1-7e11-43fe-a73a-0f1edf047eb5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You now have access to all the Models in Model-References and Optimum-Habana repositories, you can start to look at other models.  Remember that all the models in these repositories are fully documented so they are easy to use.\n",
    "* To explore more models from the Model References, start [here](https://github.com/HabanaAI/Model-References).  \n",
    "* To run more examples using Hugging Face go [here](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models).  \n",
    "* To migrate other models to Gaudi 2, refer to PyTorch Model Porting in the [documentation](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c577b02-bb5a-4cf7-a3cc-f58df42264f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
