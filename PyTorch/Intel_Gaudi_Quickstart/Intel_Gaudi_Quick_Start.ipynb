{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc00676b-8d19-4e0d-b61d-68ccc14c294d",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "\n",
    "##### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b714683-1b37-4048-bf91-2b5af1c18d40",
   "metadata": {},
   "source": [
    "# Intel® Gaudi® Accelerator Quick Start Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6db2bc-12ef-47a0-83f8-cfba2a8af03c",
   "metadata": {},
   "source": [
    "\n",
    "This document provides instructions on setting up the Intel Gaudi 2 AI accelerator Instance on the Intel® Developer Cloud or any on-premise Intel Gaudi Node. You will be running models from the Intel Gaudi software Model References and the Hugging Face Optimum Habana library.\n",
    "\n",
    "Please follow along with the [video](https://developer.habana.ai/intel-developer-cloud/) on our Developer Page to walk through the steps below.  This assumes that you have setup the latest Intel Gaudi PyTorch Docker image.\n",
    "\n",
    "To set up a multi-node instance with two or more Gaudi nodes, refer to Setting up Multiple Gaudi Nodes in the [Quick Start Guide Documentation](https://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodeshttps://docs.habana.ai/en/latest/Intel_DevCloud_Quick_Start/Intel_DevCloud_Quick_Start.html#setting-up-multiple-gaudi-nodes).  \n",
    "\n",
    "The first step is to install the Model-References repository from GitHub and run the \"hello-world\" model from the examples library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca618d-e3bc-4c36-998d-28ec94b10b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Intel_Gaudi_Quickstart\n",
    "!git clone -b 1.17.1 https://github.com/HabanaAI/Model-References.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76764c-9a4d-4d44-92f9-d9dfee9906e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Model-References/PyTorch/examples/computer_vision/hello_world/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423f41a",
   "metadata": {},
   "source": [
    "We set the correct paths for the python execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20088609-bd31-4b3d-b208-05aa6490f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONPATH'] = '$PYTHONPATH:~/Model-References'\n",
    "os.environ['PYTHON'] = '/usr/bin/python3.10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de0ff4",
   "metadata": {},
   "source": [
    "We now run the simple example with the MNIST dataset on one Intel Gaudi card:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc8e49-f883-4162-a74a-bdeb7d309598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 mnist.py --batch-size=64 --epochs=1 --lr=1.0 --gamma=0.7 --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0421856",
   "metadata": {},
   "source": [
    "We can now run the same model on eight Intel Gaudi cards using mpirun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c6a3e-d613-447d-8476-c7b13f6d59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -n 8 --bind-to core --map-by slot:PE=6 \\\n",
    "      --rank-by core --report-bindings \\\n",
    "      --allow-run-as-root \\\n",
    "      python3 mnist.py \\\n",
    "      --batch-size=64 --epochs=1 \\\n",
    "      --lr=1.0 --gamma=0.7 \\\n",
    "      --hpu --autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37216139-611e-4b07-a90f-cb351f18b185",
   "metadata": {},
   "source": [
    "### Fine-tuning with Hugging Face Optimum Habana Library\n",
    "The Optimum Habana library is the interface between the Hugging Face Transformers and Diffusers libraries and the Gaudi 2 card. It provides a set of tools enabling easy model loading, training and inference on single and multi-card settings for different downstream tasks. The following example uses the text-classification task to fine-tune a BERT-Large model with the MRPC (Microsoft Research Paraphrase Corpus) dataset and also run Inference.\n",
    "\n",
    "Follow the below steps to install the stable release from the Optimum Habana examples and library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420be01-fb88-466a-9fe7-87006340905f",
   "metadata": {},
   "source": [
    "1. Clone the Optimum-Habana project and check out the latest stable release.  This repository gives access to the examples that are optimized for Intel Gaudi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07346c1a-1fea-4b62-8a79-760ca7a64073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n",
      "Cloning into 'optimum-habana'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 18369, done.\u001b[K\n",
      "remote: Counting objects: 100% (1954/1954), done.\u001b[K\n",
      "remote: Compressing objects: 100% (916/916), done.\u001b[K\n",
      "remote: Total 18369 (delta 1310), reused 1431 (delta 909), pack-reused 16415 (from 1)\u001b[K\n",
      "Receiving objects: 100% (18369/18369), 11.88 MiB | 9.52 MiB/s, done.\n",
      "Resolving deltas: 100% (12642/12642), done.\n",
      "Note: switching to '1266993d741ba97e929965c01307f3c6cce8c107'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.13.2 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ded02-2aa1-4725-b1fc-cf917e05d9aa",
   "metadata": {},
   "source": [
    "2. Install Optimum-Habana library. This will install the latest stable library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "983e35cc-0ea1-4205-ae70-323252169c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet optimum-habana==1.13.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627879-a273-4914-8efd-68c9a81ebbb9",
   "metadata": {},
   "source": [
    "3. In order to use the DeepSpeed library on Intel Gaudi 2, install the Intel Gaudi DeepSpeed fork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d59e99a7-4db0-4519-b406-ccedee40796d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.17.1\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.17.1) to /tmp/pip-req-build-_ehmoh8h\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-_ehmoh8h\n",
      "  Running command git checkout -b 1.17.1 --track origin/1.17.1\n",
      "  Switched to a new branch '1.17.1'\n",
      "  Branch '1.17.1' set up to track remote branch '1.17.1' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit e3078cb74d027995725e39806e358a2fceee16be\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson (from deepspeed==0.14.0+hpu.synapse.v1.17.1)\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (1.10.13)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (8.0.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (2.3.1a0+git4989238)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.17.1) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0+hpu.synapse.v1.17.1) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (3.15.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.14.0+hpu.synapse.v1.17.1) (1.3.0)\n",
      "Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0+hpu.synapse.v1.17.1-py3-none-any.whl size=1445878 sha256=223f43dc4598639fc4bbc6a7fbc81e4adbdfd53c6535a23b3d5891b2a3b29994\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bpwpbo8t/wheels/72/74/40/31a4f403d3ece1f3d39f4ae5d87a3cc9752dc00209dc0a9faf\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: hjson, deepspeed\n",
      "Successfully installed deepspeed-0.14.0+hpu.synapse.v1.17.1 hjson-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.17.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f0546-e74c-4363-b443-a0f59504d973",
   "metadata": {},
   "source": [
    "The following example is based on the Optimum-Habana Text Classification task example. Change to the text-classification directory and install the additional SW requirements for this specific example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24537f40-8daa-4ac9-ad19-bf1cfaaf29f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/text-classification\n",
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.20.3)\n",
      "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.3.1a0+git4989238)\n",
      "Collecting evaluate (from -r requirements.txt (line 7))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.4.0->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.24.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 6)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.4.0->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.4.0->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.4.0->-r requirements.txt (line 1)) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.4.0->-r requirements.txt (line 1)) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 6)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~/optimum-habana/examples/text-classification/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88922f6b-527f-4bc0-bb47-4adea631ef9c",
   "metadata": {},
   "source": [
    "### Execute Single-Card Training\n",
    "This run instruction will fine-tune the BERT-Large Model on one Intel Gaudi card:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8305374a-21fe-4376-ab39-0c6de4222eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|utils.py:225] 2024-10-01 02:21:41,553 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "gaudi_config.json: 100%|██████████████████████| 90.0/90.0 [00:00<00:00, 380kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "10/01/2024 02:21:44 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: False, mixed-precision training: True\n",
      "10/01/2024 02:21:44 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "eval_use_gather_object=False,\n",
      "evaluation_strategy=None,\n",
      "fp8=False,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/bert-large-uncased-whole-word-masking,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output/mrpc/runs/Oct01_02-21-41_sc09wynn08-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./output/mrpc/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./output/mrpc/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=True,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "README.md: 100%|███████████████████████████| 35.3k/35.3k [00:00<00:00, 39.0MB/s]\n",
      "Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "10/01/2024 02:21:46 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
      "Downloading and preparing dataset glue/mrpc to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
      "10/01/2024 02:21:46 - INFO - datasets.builder - Downloading and preparing dataset glue/mrpc to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
      "train-00000-of-00001.parquet: 100%|██████████| 649k/649k [00:00<00:00, 2.35MB/s]\n",
      "validation-00000-of-00001.parquet: 100%|███| 75.7k/75.7k [00:00<00:00, 25.6MB/s]\n",
      "test-00000-of-00001.parquet: 100%|███████████| 308k/308k [00:00<00:00, 15.3MB/s]\n",
      "Downloading took 0.0 min\n",
      "10/01/2024 02:21:48 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "10/01/2024 02:21:48 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "10/01/2024 02:21:48 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 100%|████| 3668/3668 [00:00<00:00, 181470.51 examples/s]\n",
      "Generating validation split\n",
      "10/01/2024 02:21:48 - INFO - datasets.builder - Generating validation split\n",
      "Generating validation split: 100%|██| 408/408 [00:00<00:00, 79542.44 examples/s]\n",
      "Generating test split\n",
      "10/01/2024 02:21:48 - INFO - datasets.builder - Generating test split\n",
      "Generating test split: 100%|█████| 1725/1725 [00:00<00:00, 202529.80 examples/s]\n",
      "All the splits matched successfully.\n",
      "10/01/2024 02:21:48 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
      "10/01/2024 02:21:48 - INFO - datasets.builder - Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
      "config.json: 100%|█████████████████████████████| 434/434 [00:00<00:00, 2.05MB/s]\n",
      "[INFO|configuration_utils.py:733] 2024-10-01 02:21:48,420 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-01 02:21:48,423 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|██████████████████| 48.0/48.0 [00:00<00:00, 233kB/s]\n",
      "[INFO|configuration_utils.py:733] 2024-10-01 02:21:48,650 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-01 02:21:48,652 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 3.59MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 7.18MB/s]\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-10-01 02:21:49,360 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-10-01 02:21:49,360 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-10-01 02:21:49,361 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-10-01 02:21:49,361 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2289] 2024-10-01 02:21:49,361 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:733] 2024-10-01 02:21:49,361 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-01 02:21:49,364 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.43.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "model.safetensors: 100%|███████████████████| 1.34G/1.34G [00:46<00:00, 28.6MB/s]\n",
      "[INFO|modeling_utils.py:3634] 2024-10-01 02:22:36,772 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-large-uncased-whole-word-masking/snapshots/bf1420893378c390773c9452c3602fcee89f9241/model.safetensors\n",
      "[INFO|modeling_utils.py:4453] 2024-10-01 02:22:37,001 >> Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:4465] 2024-10-01 02:22:37,001 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on dataset:   0%|             | 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6e678ce21ee6f621.arrow\n",
      "10/01/2024 02:22:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6e678ce21ee6f621.arrow\n",
      "Running tokenizer on dataset: 100%|█| 3668/3668 [00:00<00:00, 6479.41 examples/s\n",
      "Running tokenizer on dataset:   0%|              | 0/408 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-37d1e2726d4fc874.arrow\n",
      "10/01/2024 02:22:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-37d1e2726d4fc874.arrow\n",
      "Running tokenizer on dataset: 100%|██| 408/408 [00:00<00:00, 7867.83 examples/s]\n",
      "Running tokenizer on dataset:   0%|             | 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4d2f6427bcc7e8cb.arrow\n",
      "10/01/2024 02:22:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4d2f6427bcc7e8cb.arrow\n",
      "Running tokenizer on dataset: 100%|█| 1725/1725 [00:00<00:00, 10920.52 examples/\n",
      "10/01/2024 02:22:37 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "10/01/2024 02:22:37 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "10/01/2024 02:22:37 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "Downloading builder script: 100%|██████████| 5.75k/5.75k [00:00<00:00, 9.61MB/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056392344 KB\n",
      "------------------------------------------------------------------------------\n",
      "[INFO|trainer.py:811] 2024-10-01 02:22:42,068 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:160: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-10-01 02:22:42,109] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[INFO|trainer.py:785] 2024-10-01 02:22:42,466 >> ***** Running training *****\n",
      "[INFO|trainer.py:786] 2024-10-01 02:22:42,466 >>   Num examples = 3,668\n",
      "[INFO|trainer.py:787] 2024-10-01 02:22:42,466 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:788] 2024-10-01 02:22:42,466 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:791] 2024-10-01 02:22:42,466 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:792] 2024-10-01 02:22:42,466 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:793] 2024-10-01 02:22:42,466 >>   Total optimization steps = 345\n",
      "[INFO|trainer.py:794] 2024-10-01 02:22:42,471 >>   Number of trainable parameters = 335,143,938\n",
      "100%|█████████████████████████████████████████| 345/345 [01:07<00:00,  6.84it/s][INFO|trainer.py:1658] 2024-10-01 02:23:49,594 >> Saving model checkpoint to ./output/mrpc/checkpoint-345\n",
      "[INFO|configuration_utils.py:472] 2024-10-01 02:23:50,327 >> Configuration saved in ./output/mrpc/checkpoint-345/config.json\n",
      "[INFO|modeling_utils.py:2755] 2024-10-01 02:23:52,029 >> Model weights saved in ./output/mrpc/checkpoint-345/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-10-01 02:23:52,030 >> tokenizer config file saved in ./output/mrpc/checkpoint-345/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-10-01 02:23:52,030 >> Special tokens file saved in ./output/mrpc/checkpoint-345/special_tokens_map.json\n",
      "[INFO|configuration_utils.py:125] 2024-10-01 02:23:52,042 >> Configuration saved in ./output/mrpc/checkpoint-345/gaudi_config.json\n",
      "[INFO|trainer.py:1079] 2024-10-01 02:23:55,492 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 73.0217, 'train_samples_per_second': 190.44, 'train_steps_per_second': 5.971, 'train_loss': 0.2980295595915421, 'epoch': 3.0, 'memory_allocated (GB)': 7.45, 'max_memory_allocated (GB)': 9.96, 'total_memory_available (GB)': 94.62}\n",
      "100%|█████████████████████████████████████████| 345/345 [01:13<00:00,  4.72it/s]\n",
      "[INFO|trainer.py:1658] 2024-10-01 02:23:55,494 >> Saving model checkpoint to ./output/mrpc/\n",
      "[INFO|configuration_utils.py:472] 2024-10-01 02:23:55,683 >> Configuration saved in ./output/mrpc/config.json\n",
      "[INFO|modeling_utils.py:2755] 2024-10-01 02:23:56,520 >> Model weights saved in ./output/mrpc/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2702] 2024-10-01 02:23:56,521 >> tokenizer config file saved in ./output/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2711] 2024-10-01 02:23:56,521 >> Special tokens file saved in ./output/mrpc/special_tokens_map.json\n",
      "[INFO|configuration_utils.py:125] 2024-10-01 02:23:56,530 >> Configuration saved in ./output/mrpc/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =        3.0\n",
      "  max_memory_allocated (GB)   =       9.96\n",
      "  memory_allocated (GB)       =       7.45\n",
      "  total_flos                  =  2395483GF\n",
      "  total_memory_available (GB) =      94.62\n",
      "  train_loss                  =      0.298\n",
      "  train_runtime               = 0:01:13.02\n",
      "  train_samples               =       3668\n",
      "  train_samples_per_second    =     190.44\n",
      "  train_steps_per_second      =      5.971\n",
      "10/01/2024 02:23:56 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:811] 2024-10-01 02:23:56,531 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:1812] 2024-10-01 02:23:56,535 >> Using HPU graphs for inference.\n",
      "[INFO|trainer.py:1830] 2024-10-01 02:23:56,535 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:1832] 2024-10-01 02:23:56,535 >>   Num examples = 408\n",
      "[INFO|trainer.py:1835] 2024-10-01 02:23:56,535 >>   Batch size = 8\n",
      "100%|██████████████████████████████████████████| 51/51 [00:00<00:00, 138.33it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                       =        3.0\n",
      "  eval_accuracy               =     0.8824\n",
      "  eval_combined_score         =     0.9001\n",
      "  eval_f1                     =     0.9178\n",
      "  eval_loss                   =     0.4238\n",
      "  eval_runtime                = 0:00:01.94\n",
      "  eval_samples                =        408\n",
      "  eval_samples_per_second     =   1118.104\n",
      "  eval_steps_per_second       =    139.763\n",
      "  max_memory_allocated (GB)   =       9.96\n",
      "  memory_allocated (GB)       =       7.45\n",
      "  total_memory_available (GB) =      94.62\n"
     ]
    }
   ],
   "source": [
    "!python run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc   \\\n",
    "--do_train   \\\n",
    "--do_eval   \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128   \\\n",
    "--output_dir ./output/mrpc/  \\\n",
    "--use_habana  \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16   \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba73e4-2918-4aac-b234-a20f55a0111c",
   "metadata": {},
   "source": [
    "### Execute Multi-Card Training\n",
    "In this example, you will be doing the same fine-tuning task with eight Gaudi 2 cards.   In this case the Optimum-Habana models repository has a helper script called `gaudi_spawn.py` that manages multi card execution.  \n",
    "Notice the execution time for the fine-tuning compared to the single-card run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9e408-1d2d-467f-b6e1-311905dc4eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING|utils.py:225] 2024-10-01 02:26:06,593 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "Running with the following model specific env vars: \n",
      "MASTER_ADDR=localhost\n",
      "MASTER_PORT=29500\n",
      "DistributedRunner run(): command = mpirun -n 8 --bind-to core --map-by socket:PE=10 --rank-by core --report-bindings --allow-run-as-root /usr/bin/python run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking --gaudi_config_name Habana/bert-large-uncased-whole-word-masking --task_name mrpc --do_train --do_eval --per_device_train_batch_size 32 --per_device_eval_batch_size 8 --learning_rate 3e-5 --num_train_epochs 3 --max_seq_length 128 --output_dir /tmp/mrpc_output/ --use_habana --use_lazy_mode --bf16 --use_hpu_graphs_for_inference --throughput_warmup_steps 3 --report_to none --overwrite_output_dir\n",
      "Authorization required, but no authorization protocol specified\n",
      "[sc09wynn08-hls2:01902] MCW rank 5 bound to socket 1[core 50[hwt 0-1]], socket 1[core 51[hwt 0-1]], socket 1[core 52[hwt 0-1]], socket 1[core 53[hwt 0-1]], socket 1[core 54[hwt 0-1]], socket 1[core 55[hwt 0-1]], socket 1[core 56[hwt 0-1]], socket 1[core 57[hwt 0-1]], socket 1[core 58[hwt 0-1]], socket 1[core 59[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09wynn08-hls2:01902] MCW rank 6 bound to socket 1[core 60[hwt 0-1]], socket 1[core 61[hwt 0-1]], socket 1[core 62[hwt 0-1]], socket 1[core 63[hwt 0-1]], socket 1[core 64[hwt 0-1]], socket 1[core 65[hwt 0-1]], socket 1[core 66[hwt 0-1]], socket 1[core 67[hwt 0-1]], socket 1[core 68[hwt 0-1]], socket 1[core 69[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../..]\n",
      "[sc09wynn08-hls2:01902] MCW rank 7 bound to socket 1[core 70[hwt 0-1]], socket 1[core 71[hwt 0-1]], socket 1[core 72[hwt 0-1]], socket 1[core 73[hwt 0-1]], socket 1[core 74[hwt 0-1]], socket 1[core 75[hwt 0-1]], socket 1[core 76[hwt 0-1]], socket 1[core 77[hwt 0-1]], socket 1[core 78[hwt 0-1]], socket 1[core 79[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB]\n",
      "[sc09wynn08-hls2:01902] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]], socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09wynn08-hls2:01902] MCW rank 1 bound to socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]], socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]], socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]]: [../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09wynn08-hls2:01902] MCW rank 2 bound to socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]], socket 0[core 24[hwt 0-1]], socket 0[core 25[hwt 0-1]], socket 0[core 26[hwt 0-1]], socket 0[core 27[hwt 0-1]], socket 0[core 28[hwt 0-1]], socket 0[core 29[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09wynn08-hls2:01902] MCW rank 3 bound to socket 0[core 30[hwt 0-1]], socket 0[core 31[hwt 0-1]], socket 0[core 32[hwt 0-1]], socket 0[core 33[hwt 0-1]], socket 0[core 34[hwt 0-1]], socket 0[core 35[hwt 0-1]], socket 0[core 36[hwt 0-1]], socket 0[core 37[hwt 0-1]], socket 0[core 38[hwt 0-1]], socket 0[core 39[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[sc09wynn08-hls2:01902] MCW rank 4 bound to socket 1[core 40[hwt 0-1]], socket 1[core 41[hwt 0-1]], socket 1[core 42[hwt 0-1]], socket 1[core 43[hwt 0-1]], socket 1[core 44[hwt 0-1]], socket 1[core 45[hwt 0-1]], socket 1[core 46[hwt 0-1]], socket 1[core 47[hwt 0-1]], socket 1[core 48[hwt 0-1]], socket 1[core 49[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../../../../../../../../../../../../../..]\n",
      "[WARNING|utils.py:225] 2024-10-01 02:26:13,053 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:225] 2024-10-01 02:26:14,394 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:225] 2024-10-01 02:26:14,751 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:225] 2024-10-01 02:26:14,973 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:225] 2024-10-01 02:26:15,400 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "[WARNING|utils.py:225] 2024-10-01 02:26:16,244 >> optimum-habana v1.13.2 has been validated for SynapseAI v1.17.0 but the driver version is v1.18.0, this could lead to undefined behavior!\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python ../gaudi_spawn.py  --world_size 8 --use_mpi run_glue.py  \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking  \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking  \\\n",
    "--task_name mrpc  \\\n",
    "--do_train  \\\n",
    "--do_eval  \\\n",
    "--per_device_train_batch_size 32  \\\n",
    "--per_device_eval_batch_size 8  \\\n",
    "--learning_rate 3e-5  \\\n",
    "--num_train_epochs 3   \\\n",
    "--max_seq_length 128  \\\n",
    "--output_dir /tmp/mrpc_output/  \\\n",
    "--use_habana   \\\n",
    "--use_lazy_mode   \\\n",
    "--bf16    \\\n",
    "--use_hpu_graphs_for_inference  \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628edd21-ebcc-43ac-a34a-1aa31133a268",
   "metadata": {},
   "source": [
    "### Training with DeepSpeed\n",
    "With the DeepSpeed package already installed, run multi-card training with DeepSpeed. The command below will create and point to a ds_config.json file to set up the parameters of the DeepSpeed run. Once the ds_config.json file is created, you can run the DeepSpeed training command below. \n",
    "\n",
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed and will still execute on eight Intel Gaudi 2 Accelerators\n",
    "\n",
    "In this case, we will run the ZeRO2 optimizer and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bda36-56e7-4712-806c-fd94626d4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c79b54-0af7-4b41-9bdf-d3d36271074c",
   "metadata": {},
   "source": [
    "This is the DeepSpeed run command for the bert-fine tuning.   At the completion of the run, compare the runtime and Max Memory usage with the non-DeepSpeed run above, you will see even faster execution and reduced memory consumption.  With larger models these advantages of using DeepSpeed are very important for running Large Language and Generative AI models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569eb0e8-e998-4c69-a07c-058583e67ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_glue.py \\\n",
    "--model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--per_device_train_batch_size 32 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 3 \\\n",
    "--max_seq_length 128 \\\n",
    "--overwrite_output_dir \\\n",
    "--output_dir /tmp/mrpc_output/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir  \\\n",
    "--deepspeed ds_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23453-cbec-4582-98dd-0dc202499da7",
   "metadata": {},
   "source": [
    "### Inference Example Run\n",
    "This is a separate example using inference only. This will run the same evaluation metrics (accuracy, F1 score) as shown above. This will display how well the model has performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c55ed-33dd-4b0a-9fe5-ecfab8ba51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking \\\n",
    "--gaudi_config_name Habana/bert-large-uncased-whole-word-masking \\\n",
    "--task_name mrpc \\\n",
    "--do_eval \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ./output/mrpc/ \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_inference \\\n",
    "--report_to none \\\n",
    "--overwrite_output_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae666d1-7e11-43fe-a73a-0f1edf047eb5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "You now have access to all the Models in Model-References and Optimum-Habana repositories, you can start to look at other models.  Remember that all the models in these repositories are fully documented so they are easy to use.\n",
    "* To explore more models from the Model References, start [here](https://github.com/HabanaAI/Model-References).  \n",
    "* To run more examples using Hugging Face go [here](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models).  \n",
    "* To migrate other models to Gaudi 2, refer to PyTorch Model Porting in the [documentation](https://docs.habana.ai/en/latest/PyTorch/PyTorch_Model_Porting/GPU_Migration_Toolkit/GPU_Migration_Toolkit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c577b02-bb5a-4cf7-a3cc-f58df42264f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please be sure to run this exit command to ensure that the resources running on Intel Gaudi are released \n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
