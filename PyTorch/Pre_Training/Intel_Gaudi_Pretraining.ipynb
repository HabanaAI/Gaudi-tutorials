{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9599e2f3-6b9c-4578-9501-7d5c65df408a",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d33aa-8f5b-4de3-bdaa-b2bfac88e1e8",
   "metadata": {},
   "source": [
    "# Pretraining of Llama2-7B using FP8 on the Intel&reg; Gaudi&reg; 2 AI Accelerator\n",
    "This example will show will show how to run pretraining of Meta Llama2 7B, using the Megatron-DeepSpeed  library, on the Intel Gaudi Accelerator. The Megatron-DeepSpeed library is used to improve memory consumption on Intel Gaudi while running large language models.\n",
    "\n",
    "You will learn how to setup the environment, select parameters, execute the workload and then see a price-performance comparison.  Intel Gaudi supports PyTorch as the main framework for Training.\n",
    "\n",
    "This tutorial will be based on the Habana implementation of [DeepSpeed repository](https://github.com/HabanaAI/Megatron-DeepSpeed), where examples can be found for training large transformer language models such as LLaMA at scale.\n",
    "\n",
    "The following steps will let you run pretraining on the Llama 7B. In the next sections each step will be described step-by-step:\n",
    "\n",
    "•\tGet Access to an Intel Gaudi node, using the Intel® Tiber™ Developer Cloud is recommended.  \n",
    "•\tRun the Intel Gaudi PyTorch Docker image; this ensures that all the SW is installed and configured properly.  \n",
    "•\tInstall pre-requisites.  \n",
    "•\tDownload and pre-process dataset.  \n",
    "•\tSelect parameters and run pretraining on the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58e160",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Accessing The Intel Gaudi Node\n",
    "To access an Intel Gaudi node in the Intel Tiber Developer cloud, you will go to [Intel Developer Cloud Console](https://console.cloud.intel.com/hardware) and access the hardware instances to select the Intel® Gaudi® 2 platform for deep learning and follow the steps to start and connect to the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d79263",
   "metadata": {},
   "source": [
    "\n",
    "### Docker Setup\n",
    "Now that you have access to the node, you will use the latest Intel Gaudi docker image by first calling the docker run command which will automatically download and run the docker:\n",
    "\n",
    "```\n",
    "docker run -itd --name Gaudi_Docker --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest\n",
    "```\n",
    "\n",
    "We then start the docker and enter the docker environment by issuing the following command: \n",
    "```\n",
    "docker exec -it Gaudi_Docker bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2237ab3",
   "metadata": {},
   "source": [
    "### Model Setup \n",
    "Now that we’re running in a docker environment, we can now install the remaining libraries and model repositories:\n",
    "Start in the root directory and install the DeepSpeed Library; DeepSpeed is used to improve memory consumption on Intel Gaudi while running large language models. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0750c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /root\n",
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b669e1c",
   "metadata": {},
   "source": [
    "Now install the Hugging Face Optimum Habana library and clone the Megatron-DeepSpeed repository, notice that we’re selecting the latest validated release of Optimum Habana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba1fa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum-habana==1.14.0\n",
    "!git clone -b 1.18.0 https://github.com/HabanaAI/Megatron-DeepSpeed.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50cdb2",
   "metadata": {},
   "source": [
    "Next, we transition to the Megatron-DeepSpeed directory and install the set of requirements to perform training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61daaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/Megatron-DeepSpeed\n",
    "!pip install -r megatron/core/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf145b8-641e-4bc4-b804-8b0f3d01a789",
   "metadata": {},
   "source": [
    "Setup the correct path for Megatron-DeepSpeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a9f7b1-acdb-4d0d-bf13-d0ed23931997",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MEGATRON_DEEPSPEED_ROOT=/root/Megatron-DeepSpeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62547952-b388-4ec2-9efd-b2812567c42c",
   "metadata": {},
   "source": [
    "Finally, Set Python 3.10 as the default Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e13f9-d4a9-4f0e-b25d-c994f8ffcdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTHON=/usr/bin/python3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e9cc46",
   "metadata": {},
   "source": [
    "### How to download the dataset\n",
    "To download datasets used for training Llama2, you can follow directions in the Megatron-Deepspeed Github page, which show steps to download and preprocess the Oscar-en dataset. This dataset is big, and it will take considerable time to download and preprocess. \n",
    "For this tutorial, we will use a smaller dataset, the customized RedPajama dataset, which will download and prepare much faster, with the purpose to illustrate the pre-training flow.\n",
    "\n",
    "First, download the redpajama dataset list file, then pick only the first jsonl file, which is arxiv:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7150b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /root\n",
    "!mkdir -p redpajama\n",
    "%cd redpajama\n",
    "!wget 'https://data.together.xyz/redpajama-data-1T/v1.0.0/urls.txt'\n",
    "!head -n 1 urls.txt > first_jsonl.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41945f84-cb16-40ce-94b0-1d158e68493f",
   "metadata": {},
   "source": [
    "Next, download the arxiv subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ab699-2ac1-47f5-bac1-7fe3167a486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir arxiv\n",
    "!wget -P arxiv/ https://data.together.xyz/redpajama-data-1T/v1.0.0/arxiv/arxiv_023827cd-7ee8-42e6-aa7b-661731f4c70f.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cc4a6-5ea2-4583-971d-e21aba0c877b",
   "metadata": {},
   "source": [
    "We also need to download the tokenizer file correspondent to the Llama7B model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac8461-da87-4c74-a377-3906391f50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O tokenizer.model \"https://huggingface.co/huggyllama/llama-7b/resolve/main/tokenizer.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80939316-d6f7-439f-943f-e32c044fcd0c",
   "metadata": {},
   "source": [
    "The last step is to install the modules needed for data preparation and complete the pre-processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c0d6c7-cab2-4069-91dc-bee2fcc51683",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /root/redpajama/\n",
    "!pip install nltk sentencepiece\n",
    "!mkdir -p arxiv_tokenized\n",
    "!wget -P arxiv_tokenized https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
    "!wget -P arxiv_tokenized https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
    "!python $MEGATRON_DEEPSPEED_ROOT/tools/preprocess_data.py --input arxiv/*.jsonl \\\n",
    "      --output-prefix arxiv_tokenized/meg-gpt2 --tokenizer-model ./tokenizer.model \\\n",
    "      --append-eod --tokenizer-type GPT2BPETokenizer --workers 64 --vocab-file arxiv_tokenized/gpt2-vocab.json --merge-file arxiv_tokenized/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cbd059",
   "metadata": {},
   "source": [
    "### Running Llama2 7B Pretraing Using the FP8 Datatype\n",
    "We are now ready to start running pretraining on this model.  \n",
    "We will use the DeepSpeed library with a set of settings to more efficiently run pretraining using 8 Intel Gaudi cards: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc93a4-ef7d-40ee-9d8c-ee3f95ca0aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export MEGATRON_DEEPSPEED_ROOT='/root/Megatron-DeepSpeed'\n",
    "echo $MEGATRON_DEEPSPEED_ROOT\n",
    "export LOG_LEVEL_ALL=4\n",
    "export ENABLE_CONSOLE=true\n",
    "export HABANA_LOGS=./habana_log\n",
    "export MEGATRON_DEEPSPEED_ROOT=/root/Megatron-DeepSpeed/\n",
    "export MODEL_REFERENCES_ROOT=/root/Megatron-DeepSpeed/\n",
    "export HL_DATA_DIR_ROOT=/root/redpajama/arxiv_tokenized/\n",
    "export HL_DATA_FILE_PREFIX=meg-gpt2_text_document\n",
    "export OUT_DIR=Llama2-7B-training\n",
    "export HL_HOSTSFILE=/launch/hostsfile\n",
    "mkdir -p ${OUT_DIR}\n",
    "HL_SAVE=0 \\\n",
    "HL_EXIT_INTERVAL=80 \\\n",
    "HL_RESULTS_DIR=${OUT_DIR} \\\n",
    "HL_LOG_INTERVAL=10 \\\n",
    "HL_TOKENIZER_TYPE=GPT2BPETokenizer \\\n",
    "HL_DATA_DIR_ROOT=${HL_DATA_DIR_ROOT} \\\n",
    "HL_DATA_FILE_PREFIX=$HL_DATA_FILE_PREFIX \\\n",
    "HL_GBS=1024 \\\n",
    "HL_LLAMA_VER=2 \\\n",
    "HL_LLAMA_MODEL_SIZE=7 \\\n",
    "HL_NUM_NODES=1 \\\n",
    "HL_PP=1 HL_TP=1 HL_DP=8 \\\n",
    "HL_CKP_ACT=2 \\\n",
    "HL_SEQ_LEN=4096 \\\n",
    "HL_ZERO_STAGE=1 \\\n",
    "HL_USE_FAST_SOFTMAX=1 \\\n",
    "HL_GRAD_ACCUM_DTYPE=bf16  \\\n",
    "HL_USE_TRANSFORMER_ENGINE=1 \\\n",
    "HL_USE_CACHE_FP8_WEIGHT_FWD=1 \\\n",
    "HL_USE_CACHE_FP8_WEIGHT=1 \\\n",
    "${MODEL_REFERENCES_ROOT}/scripts/run_llama.sh 2>&1 | tee ${OUT_DIR}/llama_8x.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92516049",
   "metadata": {},
   "source": [
    "As the performance results can vary depending on hardware used, the results shown in this section are to be considered as examples and not as benchmark results.\n",
    "\n",
    "Once the pretraining ends, the following information is reported in the output log at the end of the execution (remember that the sample run ended after 80 iterations, as specified by the env variable: HL_EXIT_INTERVAL=80):\n",
    "\n",
    "```\n",
    "iteration       80/  500000 | consumed samples:        81920 | consumed tokens:    335544320 | elapsed time per iteration (ms): 62373.1 | learning rate: 1.200E-05 | global batch size:  1024 | lm loss: 3.354671E+00 | loss scale: 1.0 | grad norm: 4.962 | num zeros: 0.0 | actual seqlen:  4096 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.417 | tokens per gpu per second (tgs): 8405.678 | TFLOPs: 409.21 |\n",
    "```\n",
    "\n",
    "The total number of tokens per second is:  \n",
    "\n",
    "tokens per gpu per second (tgs) * 8 HPUs ~= 8400 * 8 ~= 67,200 tokens/sec\n",
    "\n",
    "You can now see the final values that align with the published numbers from the [developer website](https://www.intel.com/content/www/us/en/developer/platform/gaudi/model-performance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98217bb7",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "Now that you have run a pretraining case, you can go back to the Hugging Face Optimum Habana [validated models](https://github.com/huggingface/optimum-habana?tab=readme-ov-file#validated-models) to see more options for running training or inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f697a5f-1ea3-45b5-8c8e-bb2bbace026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
