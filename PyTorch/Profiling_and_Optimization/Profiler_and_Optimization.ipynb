{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208ee6a9-c3d5-4d5b-9102-33b6cab06aa6",
   "metadata": {},
   "source": [
    "Copyright (c) 2024 Habana Labs, Ltd. an Intel Company.\n",
	"SPDX-License-Identifier: Apache-2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14d34d-3cbe-4b29-bf7a-a5f7d6eb22b5",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This tutorial will show the user how to run the Intel Gaudi Profiling tools: the habana_perf_tool and the Tensorboard plug-in on the Intel Gaudi 2 AI Accelerator, and the profiling trace viewer.  These tools will provide the user valueable optimization tips and information to modify any model for better performance.   Following these steps and using these tools can help you better understand some of the bottlenecks of your model.  For more information, please refer to the [Profiling](https://docs.habana.ai/en/latest/Profiling/index.html) section of the documentation for info on how to setup the profiler and the [Optimization Guide](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/index.html) for additional background on other optimization techniques.\n",
    "\n",
    "| Task                                 | Description                                             | Details                                         |\n",
    "|--------------------------------------|---------------------------------------------------------|-------------------------------------------------|\n",
    "| PyTorch Profiling with TensorBoard   | Obtains Gaudi-specific recommendations for performance using TensorBoard. | [Profiling with PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html#profiling-with-pytorch)        |\n",
    "| Review the PT_HPU_METRICS_FILE      | Looks for excessive re-compilations during runtime.     | [Runtime Environment Variables](https://docs.habana.ai/en/latest/PyTorch/Reference/Runtime_Flags.html#pytorch-runtime-flags)                   |                         \n",
    "| Profiling Trace Viewer               | Uses Perfetto to view traces.           |  [Getting Started with Intel Gaudi Profiler](https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Getting_Started_with_Profiler.html#getting-started-with-profiler)                      |                         \n",
    "| Model Logging                        | Sets ENABLE_CONSOLE to set Logging for debug and analysis. | [Runtime Environment Variables](https://docs.habana.ai/en/latest/PyTorch/Reference/Runtime_Flags.html#pytorch-runtime-flags)                |                         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50df7f-7e0f-44cd-a4e4-32d5d1219dd0",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "To run the this jupyter notebook and the Tensorboard viewer, set the appropriate ports for access when you ssh into the Intel Gaudi 2 node. you need to ensure that the following ports are open:\n",
    "* 8888 (for running this jupyter notebook)\n",
    "* 6006 (for running Tensorboard)    \n",
    "\n",
    "Do to this, you need to add the following in your overall ssh commmand when connecting to the Intel Gaudi Node:\n",
    "\n",
    "`ssh -L 8888:localhost:8888 -L 6006:localhost:6006 .... `\n",
    "\n",
    "We start with an Intel Gaudi PyTorch Docker image and run this notebook.   For this example, we'll be using the [Swin Transformer](https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k) model from the Hugging Face Repository running on Hugging Face's Optimum-Habana library.  So the first step is to load the Optimum-Habana library and model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bdc37b-0715-4caf-be4c-644ea0aa521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "!git clone -b v1.15.0 https://github.com/huggingface/optimum-habana.git\n",
    "!pip install optimum-habana==1.15.0\n",
    "!pip install pickleshare ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec63423",
   "metadata": {},
   "source": [
    "We now will go into the image-classification task and load the specfic requirements for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301b6c1-f975-45f6-893c-13885ef9d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/optimum-habana/examples/image-classification\n",
    "!pip install --quiet -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faeecd-5853-47d7-b148-ab327048447d",
   "metadata": {},
   "source": [
    "### Running the Model\n",
    "Now that the model is loaded, we'll run the model and look for the trace files for analysis. \n",
    "\n",
    "For this model script we can see the profiling set in the utils.py. \n",
    "For other models not in optimum-habana, users can refer to [Profiling_with_PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html) to setup profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48df403-cee4-4898-a4a6-5432d9cc58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   301\t            schedule = torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1)\n",
      "   302\t            activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.HPU]\n",
      "   303\t\n",
      "   304\t            profiler = torch.profiler.profile(\n",
      "   305\t                schedule=schedule,\n",
      "   306\t                activities=activities,\n",
      "   307\t                on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),\n",
      "   308\t                record_shapes=record_shapes,\n",
      "   309\t                with_stack=False,\n",
      "   310\t            )\n",
      "   311\t            self.start = profiler.start\n",
      "   312\t            self.stop = profiler.stop\n",
      "   313\t            self.step = profiler.step\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat -n ../../optimum/habana/utils.py | head -n 313 | tail -n 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a07d2b-f089-435a-b346-50b2eb82b8ca",
   "metadata": {},
   "source": [
    "Run Model to collect trace file (unoptimized)\n",
    "Swin Transformer is a model that capably serves as a general-purpose backbone for computer vision. run_image_classification.py is a script that showcases how to fine-tune Swin Transformer on HPUs.\n",
    "\n",
    "Notice the torch profiler specific commands:\n",
    "\n",
    "- `--profiling_warmup_steps 10` - profiler will wait for warmup steps\n",
    "- `--profiling_steps 3` - records for the next active steps  \n",
    "                             \n",
    "The collected trace files will be saved to ./hpu_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a9142-a80e-4c4f-82b7-4f08509996c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.habana.utils import HabanaProfile\n",
    "HabanaProfile.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031de875-5f20-4893-af32-5ce9451afe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056412772 KB\n",
      "------------------------------------------------------------------------------\n",
      "[INFO|trainer.py:790] 2024-10-18 20:45:00,170 >> ***** Running training *****\n",
      "[INFO|trainer.py:791] 2024-10-18 20:45:00,170 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:792] 2024-10-18 20:45:00,170 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:793] 2024-10-18 20:45:00,170 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:796] 2024-10-18 20:45:00,170 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:797] 2024-10-18 20:45:00,170 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:798] 2024-10-18 20:45:00,170 >>   Total optimization steps = 1,330\n",
      "[INFO|trainer.py:799] 2024-10-18 20:45:00,174 >>   Number of trainable parameters = 86,753,474\n",
      "{'loss': 0.4453, 'grad_norm': 38.917335510253906, 'learning_rate': 1.8721804511278196e-05, 'epoch': 0.75, 'memory_allocated (GB)': 11.13, 'max_memory_allocated (GB)': 11.5, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 0.2587, 'grad_norm': 15.137947082519531, 'learning_rate': 7.443609022556391e-06, 'epoch': 1.5, 'memory_allocated (GB)': 11.12, 'max_memory_allocated (GB)': 18.37, 'total_memory_available (GB)': 94.62}\n",
      "100%|██████████████████████████████████████▉| 1329/1330 [03:52<00:00,  6.80it/s][INFO|trainer.py:1086] 2024-10-18 20:48:52,993 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 232.8326, 'train_samples_per_second': 393.695, 'train_steps_per_second': 6.16, 'train_loss': 0.3214681639707178, 'epoch': 2.0, 'memory_allocated (GB)': 11.45, 'max_memory_allocated (GB)': 18.37, 'total_memory_available (GB)': 94.62}\n",
      "100%|███████████████████████████████████████| 1330/1330 [03:52<00:00,  5.71it/s]\n",
      "[INFO|trainer.py:1676] 2024-10-18 20:48:53,009 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:410] 2024-10-18 20:48:53,109 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:2836] 2024-10-18 20:48:53,723 >> Model weights saved in /tmp/outputs/model.safetensors\n",
      "[INFO|image_processing_base.py:258] 2024-10-18 20:48:53,724 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:125] 2024-10-18 20:48:53,724 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =          2.0\n",
      "  max_memory_allocated (GB)   =        18.37\n",
      "  memory_allocated (GB)       =        11.45\n",
      "  total_flos                  = 6211366589GF\n",
      "  total_memory_available (GB) =        94.62\n",
      "  train_loss                  =       0.3215\n",
      "  train_runtime               =   0:03:52.83\n",
      "  train_samples_per_second    =      393.695\n",
      "  train_steps_per_second      =         6.16\n",
      "[INFO|modelcard.py:449] 2024-10-18 20:48:53,832 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}}\n"
     ]
    }
   ],
   "source": [
    "%run run_image_classification.py \\\n",
    "    --model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir /tmp/outputs/ \\\n",
    "    --remove_unused_columns False \\\n",
    "    --image_column_name img \\\n",
    "    --do_train \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy no \\\n",
    "    --load_best_model_at_end False \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337 \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/swin \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \\\n",
    "    --report_to none \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ignore_mismatched_sizes \\\n",
    "    --profiling_warmup_steps 10 \\\n",
    "    --profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f029263-187b-46be-b017-09c5be1ef8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ./hpu_profile/*.pt.trace.json ./hpu_profile/UNOPT.pt.trace.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23106fe-e911-4e67-9c8f-4ad8d587408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/image-classification/hpu_profile\n",
      "total 187084\n",
      "drwxr-xr-x 1 root root         0 Oct 18 20:58 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 1 root root         0 Oct 18 20:43 \u001b[01;34m..\u001b[0m/\n",
      "drwxr-xr-x 1 root root         0 Oct 18 20:58 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
      "-rw-r--r-- 1 root root 191570168 Oct 18 20:45 sc09wynn05-hls2_14734.1729284340533778439.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "%ls -al hpu_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21347a9a",
   "metadata": {},
   "source": [
    "### Reviewing the Details in Tensorboard and perf_tool\n",
    "Now that the training is completed, you can see the trace files (...pt.trace.json) have been generated and now can be viewed.  Two types of information are produced by TensorBoard:\n",
    "\n",
    "Model Performance Tracking - While your workload is being processed in batches, you can track the progress of the training process on the dashboard in real-time by monitoring the model’s cost (loss) and accuracy.\n",
    "\n",
    "Profiling Analysis - Right after the last requested step was completed, the collected profiling data is analyzed by TensorBoard and then immediately submitted to your browser, without any need to wait till the training process is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8d01f1-1c80-4ba7-8384-2626e93f6ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34ec15902bbafacb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34ec15902bbafacb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./hpu_profile --port 6006    # Your port selection may vary, default is 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6df7a-d03d-4994-b8c5-ac9099b68567",
   "metadata": {},
   "source": [
    "If you do not want to run the TensorBoard UI, you can take the same .json log files and use the habana_perf_tool that will parse the existing .json file and provide the same recommendations for performance enhancements, but in a text form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f599b9c-98c6-437e-ad9c-b12c7b09ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-18 20:56:53,275 - pytorch_profiler - DEBUG - Loading /root/optimum-habana/examples/image-classification/hpu_profile/sc09wynn05-hls2_14734.1729284340533778439.pt.trace.json\n",
      "Import Data (KB): 100%|█████████████| 187080/187080 [00:01<00:00, 112314.37it/s]\n",
      "2024-10-18 20:56:56,310 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2024-10-18 20:57:02,010 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='51,47,7', pt_bridge_compute='16', pt_mem_copy='7', pt_mem_log='', pt_build_graph='46,49,50,6')\n",
      "2024-10-18 20:57:02,010 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='5', backward='45', synapse_launch='0,1,48', synapse_wait='2,10', device_mme='41,42,43,44', device_tpc='17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40', device_dma='8,11,12,13,14,15')\n",
      "2024-10-18 20:57:02,897 - pytorch_profiler - DEBUG - Device ratio: 37.66 % (193.313984 ms, 513.299755 ms)\n",
      "2024-10-18 20:57:02,897 - pytorch_profiler - DEBUG - Device/Host ratio: 37.66% / 62.34%\n",
      "2024-10-18 20:57:03,277 - pytorch_profiler - DEBUG - Host Summary Graph Build: 13.80 % (70.039949 ms, 507.41137 ms)\n",
      "2024-10-18 20:57:03,343 - pytorch_profiler - DEBUG - Host Summary DataLoader: 55.18 % (280.003883 ms, 507.41137 ms)\n",
      "2024-10-18 20:57:03,488 - pytorch_profiler - DEBUG - Host Summary Input Time: 3.63 % (18.406094 ms, 507.41137 ms)\n",
      "2024-10-18 20:57:03,574 - pytorch_profiler - DEBUG - Host Summary Compile Time: 0.00 % (0.0 ms, 507.41137 ms)\n",
      "2024-10-18 20:57:03,956 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 81.54%\n",
      "2024-10-18 20:57:03,956 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 96.95 %\n",
      "2024-10-18 20:57:03,956 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2024-10-18 20:57:03,956 - pytorch_profiler - DEBUG - \tThis run has high time cost on input data loading. 55.18% of the step time is in DataLoader. You could use Habana DataLoader. Or you could try to tune num_workers on DataLoader's construction.\n",
      "2024-10-18 20:57:04,182 - pytorch_profiler - DEBUG - [Device Summary] MME total time 74.87 ms\n",
      "2024-10-18 20:57:09,154 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 46.48 ms\n",
      "2024-10-18 20:57:09,156 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 93.33 ms\n",
      "2024-10-18 20:57:11,110 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 22.79 ms\n",
      "2024-10-18 20:57:11,111 - pytorch_profiler - DEBUG - [Device Summary] Idle total time: 2.33 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./hpu_profile/UNOPT.pt.trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5821e9-fe68-4c76-93ef-b3b6c8492a02",
   "metadata": {},
   "source": [
    "### Using the Perfetto Trace Viewer\n",
    "Finally, to view the details of the Intel Gaudi Device itself, you can view the traces in the perfetto trace viewer.  \n",
    "\n",
    "This step requires you to set the `hl-prof-config` settings and the Environment variable `HABANA_PROFILE=1` as shown below, this will generate the .hltv file that can be viewed using https://perfetto.habana.ai.  Since this is using the Gaudi profiler, the runtime profiling commands need to be removed.  At the end of this run, you will see a `my_profiling_session_12345.hltv` file that can be loaded into the Perfetto browser.\n",
    "\n",
    "For More Information to enable your model to use the Habana Perfetto Trace viewer, you can refer to the documentation https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Getting_Started_with_Profiler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5291e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/image-classification\n",
      "Resetting config file /root/.habana/prof_config.json\n",
      "Edited file /root/.habana/prof_config.json\n"
     ]
    }
   ],
   "source": [
    "!hl-prof-config -e off -phase=multi-enq -g 1-20 -s my_profiling_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9fe7f-5817-4803-90b8-244da14213c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HABANA_PROFILE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26eb57-a519-4935-b7a5-92a56cdc1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run run_image_classification.py \\\n",
    "    --model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir /tmp/outputs/ \\\n",
    "    --remove_unused_columns False \\\n",
    "    --image_column_name img \\\n",
    "    --do_train \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy no \\\n",
    "    --load_best_model_at_end False \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337 \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --report_to none \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/swin \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ignore_mismatched_sizes \n",
    "    #--profiling_warmup_steps 10 \\\n",
    "    #--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1931bdba-bfdf-436e-981c-754ba2f50c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 58536303 Oct 10 22:04 my_profiling_session_521.hltv\n"
     ]
    }
   ],
   "source": [
    "!ls -l *.hltv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9a9ad-dc28-44f6-9af0-995932c3cfb0",
   "metadata": {},
   "source": [
    "Consult the [Analysis guide](https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Analysis.html) for performing a thorough analysis of the above .hltv profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
