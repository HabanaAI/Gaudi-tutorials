{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208ee6a9-c3d5-4d5b-9102-33b6cab06aa6",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "# Intel&reg; Gaudi&reg; 2 Model Profiling and Optimization using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14d34d-3cbe-4b29-bf7a-a5f7d6eb22b5",
   "metadata": {},
   "source": [
    "## Objective\n",
    "This tutorial will show the user how to run the Intel Gaudi Profiling tools: the habana_perf_tool and the Tensorboard plug-in on the Intel Gaudi 2 AI Accelerator, and the profiling trace viewer.  These tools will provide the user valueable optimization tips and information to modify any model for better performance.   Following these steps and using these tools can help you better understand some of the bottlenecks of your model.  For more information, please refer to the [Profiling](https://docs.habana.ai/en/latest/Profiling/index.html) section of the documentation for info on how to setup the profiler and the [Optimization Guide](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/index.html) for additional background on other optimization techniques.\n",
    "\n",
    "| Task                                 | Description                                             | Details                                         |\n",
    "|--------------------------------------|---------------------------------------------------------|-------------------------------------------------|\n",
    "| PyTorch Profiling with TensorBoard   | Obtains Gaudi-specific recommendations for performance using TensorBoard. | [Profiling with PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html#profiling-with-pytorch)        |\n",
    "| Review the PT_HPU_METRICS_FILE      | Looks for excessive re-compilations during runtime.     | [Runtime Environment Variables](https://docs.habana.ai/en/latest/PyTorch/Reference/Runtime_Flags.html#pytorch-runtime-flags)                   |                         \n",
    "| Profiling Trace Viewer               | Uses Perfetto to view traces.           |  [Getting Started with Intel Gaudi Profiler](https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Getting_Started_with_Profiler.html#getting-started-with-profiler)                      |                         \n",
    "| Model Logging                        | Sets ENABLE_CONSOLE to set Logging for debug and analysis. | [Runtime Environment Variables](https://docs.habana.ai/en/latest/PyTorch/Reference/Runtime_Flags.html#pytorch-runtime-flags)                |                         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c50df7f-7e0f-44cd-a4e4-32d5d1219dd0",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "To run the this jupyter notebook and the Tensorboard viewer, set the appropriate ports for access when you ssh into the Intel Gaudi 2 node. you need to ensure that the following ports are open:\n",
    "* 8888 (for running this jupyter notebook)\n",
    "* 6006 (for running Tensorboard)    \n",
    "\n",
    "Do to this, you need to add the following in your overall ssh commmand when connecting to the Intel Gaudi Node:\n",
    "\n",
    "`ssh -L 8888:localhost:8888 -L 6006:localhost:6006 .... `\n",
    "\n",
    "We start with an Intel Gaudi PyTorch Docker image and run this notebook.   For this example, we'll be using the [Swin Transformer](https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k) model from the Hugging Face Repository running on Hugging Face's Optimum-Habana library.  So the first step is to load the Optimum-Habana library and model repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2d3680-76ea-4945-853e-dea9011033f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (8.22.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.14.2)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting optimum-habana==1.10.4\n",
      "  Downloading optimum_habana-1.10.4-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<4.38.0,>=4.37.0 (from optimum-habana==1.10.4)\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (from optimum-habana==1.10.4) (1.17.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from optimum-habana==1.10.4) (2.1.1a0+gitb51c9f6)\n",
      "Collecting accelerate<0.28.0 (from optimum-habana==1.10.4)\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting diffusers<0.27.0,>=0.26.0 (from optimum-habana==1.10.4)\n",
      "  Downloading diffusers-0.26.3-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==1.10.4) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==1.10.4) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==1.10.4) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==1.10.4) (6.0)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==1.10.4) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==1.10.4) (0.4.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (7.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (3.13.1)\n",
      "Collecting huggingface-hub (from accelerate<0.28.0->optimum-habana==1.10.4)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (2023.5.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (2.31.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (10.2.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==1.10.4) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==1.10.4) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==1.10.4) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==1.10.4) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==1.10.4) (2023.10.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.38.0,>=4.37.0->optimum-habana==1.10.4) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<4.38.0,>=4.37.0->optimum-habana==1.10.4) (4.66.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum->optimum-habana==1.10.4) (15.0.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from optimum->optimum-habana==1.10.4) (2.14.7)\n",
      "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers<4.38.0,>=4.37.0->optimum-habana==1.10.4)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum->optimum-habana==1.10.4) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum->optimum-habana==1.10.4) (3.20.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum->optimum-habana==1.10.4) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->optimum->optimum-habana==1.10.4) (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (2023.11.17)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers<0.27.0,>=0.26.0->optimum-habana==1.10.4) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->optimum-habana==1.10.4) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->optimum-habana==1.10.4) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum->optimum-habana==1.10.4) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum->optimum-habana==1.10.4) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum->optimum-habana==1.10.4) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum->optimum-habana==1.10.4) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum->optimum-habana==1.10.4) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->optimum->optimum-habana==1.10.4) (4.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum->optimum-habana==1.10.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum->optimum-habana==1.10.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->optimum->optimum-habana==1.10.4) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum->optimum-habana==1.10.4) (1.16.0)\n",
      "Downloading optimum_habana-1.10.4-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diffusers-0.26.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, diffusers, accelerate, transformers, optimum-habana\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.14.1\n",
      "    Uninstalling tokenizers-0.14.1:\n",
      "      Successfully uninstalled tokenizers-0.14.1\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.23.1\n",
      "    Uninstalling diffusers-0.23.1:\n",
      "      Successfully uninstalled diffusers-0.23.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.28.0\n",
      "    Uninstalling accelerate-0.28.0:\n",
      "      Successfully uninstalled accelerate-0.28.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.34.1\n",
      "    Uninstalling transformers-4.34.1:\n",
      "      Successfully uninstalled transformers-4.34.1\n",
      "  Attempting uninstall: optimum-habana\n",
      "    Found existing installation: optimum-habana 1.9.0\n",
      "    Uninstalling optimum-habana-1.9.0:\n",
      "      Successfully uninstalled optimum-habana-1.9.0\n",
      "Successfully installed accelerate-0.27.2 diffusers-0.26.3 huggingface-hub-0.21.4 optimum-habana-1.10.4 tokenizers-0.15.2 transformers-4.37.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd ~/Gaudi-tutorials/PyTorch/Profiling_and_Optimization\n",
    "!pip install pickleshare ipython\n",
    "!pip install optimum-habana==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e75d5fc-5196-4b80-8fe8-d58b22949fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 10772, done.\u001b[K\n",
      "remote: Counting objects: 100% (3023/3023), done.\u001b[K\n",
      "remote: Compressing objects: 100% (700/700), done.\u001b[K\n",
      "remote: Total 10772 (delta 2665), reused 2444 (delta 2285), pack-reused 7749\u001b[K\n",
      "Receiving objects: 100% (10772/10772), 4.69 MiB | 3.63 MiB/s, done.\n",
      "Resolving deltas: 100% (7348/7348), done.\n",
      "/root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana\n",
      "Note: switching to 'v1.10.4'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n",
      "HEAD is now at 1dfbc02 Release: v1.10.4\n",
      "/root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/optimum-habana\n",
    "%cd optimum-habana\n",
    "!git checkout v1.12.0\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec63423",
   "metadata": {},
   "source": [
    "We now will go into the image-classification task and load the specfic requirements for the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d301b6c1-f975-45f6-893c-13885ef9d18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification\n",
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.1a0+gitb51c9f6)\n",
      "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.16.0+fbb4cc5)\n",
      "Requirement already satisfied: datasets>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.14.7)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.4.1.post1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (0.21.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.0->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.0->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 1)) (2.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/image-classification\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57faeecd-5853-47d7-b148-ab327048447d",
   "metadata": {},
   "source": [
    "### Running the Model\n",
    "Now that the model is loaded, we'll run the model and look for the trace files for analysis. \n",
    "\n",
    "For this model script we can see the profiling set in the utils.py. \n",
    "For other models not in optimum-habana, users can refer to [Profiling_with_PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html) to setup profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48df403-cee4-4898-a4a6-5432d9cc58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   262\t            profiler = torch.profiler.profile(\n",
      "   263\t                schedule=schedule,\n",
      "   264\t                activities=activities,\n",
      "   265\t                on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),\n",
      "   266\t                record_shapes=record_shapes,\n",
      "   267\t                with_stack=True,\n",
      "   268\t            )\n",
      "   269\t            self.start = profiler.start\n",
      "   270\t            self.stop = profiler.stop\n",
      "   271\t            self.step = profiler.step\n",
      "   272\t            HabanaProfile.enable.invalid = True\n",
      "   273\t            HabanaProfile.disable.invalid = True\n",
      "   274\t\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat -n ../../optimum/habana/utils.py | head -n 313 | tail -n 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a07d2b-f089-435a-b346-50b2eb82b8ca",
   "metadata": {},
   "source": [
    "Run Model to collect trace file (unoptimized)\n",
    "Swin Transformer is a model that capably serves as a general-purpose backbone for computer vision. run_image_classification.py is a script that showcases how to fine-tune Swin Transformer on HPUs.\n",
    "\n",
    "Notice the torch profiler specific commands:\n",
    "\n",
    "- `--profiling_warmup_steps 10` - profiler will wait for warmup steps\n",
    "- `--profiling_steps 3` - records for the next active steps  \n",
    "                             \n",
    "The collected trace files will be saved to ./hpu_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031de875-5f20-4893-af32-5ce9451afe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/18/2024 22:45:42 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: False, mixed-precision training: True\n",
      "03/18/2024 22:45:42 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Mar18_22-45-40_hls2-srv01-demolab,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|configuration_utils.py:729] 2024-03-18 22:45:47,025 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-03-18 22:45:47,032 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.37.2\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3476] 2024-03-18 22:45:47,038 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/model.safetensors\n",
      "[INFO|modeling_utils.py:4350] 2024-03-18 22:45:47,992 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:4371] 2024-03-18 22:45:47,993 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:375] 2024-03-18 22:45:48,100 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/68dc76680a5bf3bdf670669f3025dc9be2e30781/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:366] 2024-03-18 22:45:48,100 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "[INFO|image_processing_utils.py:738] 2024-03-18 22:45:48,104 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:425] 2024-03-18 22:45:48,105 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056399524 KB\n",
      "------------------------------------------------------------------------------\n",
      "[INFO|trainer.py:726] 2024-03-18 22:45:51,464 >> ***** Running training *****\n",
      "[INFO|trainer.py:727] 2024-03-18 22:45:51,464 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:728] 2024-03-18 22:45:51,464 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:729] 2024-03-18 22:45:51,464 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:732] 2024-03-18 22:45:51,464 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:733] 2024-03-18 22:45:51,464 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:734] 2024-03-18 22:45:51,464 >>   Total optimization steps = 1,330\n",
      "[INFO|trainer.py:735] 2024-03-18 22:45:51,471 >>   Number of trainable parameters = 86,753,474\n",
      "  1%|▎                                        | 10/1330 [00:27<12:20,  1.78it/s]STAGE:2024-03-18 22:46:19 1924:1924 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "  1%|▍                                        | 13/1330 [00:28<07:49,  2.81it/s]STAGE:2024-03-18 22:46:20 1924:1924 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-03-18 22:46:20 1924:1924 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "{'loss': 0.4459, 'learning_rate': 1.8721804511278196e-05, 'epoch': 0.75, 'memory_allocated (GB)': 11.16, 'max_memory_allocated (GB)': 11.54, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 0.2589, 'learning_rate': 7.443609022556391e-06, 'epoch': 1.5, 'memory_allocated (GB)': 11.16, 'max_memory_allocated (GB)': 18.4, 'total_memory_available (GB)': 94.62}\n",
      "100%|██████████████████████████████████████▉| 1329/1330 [04:01<00:00,  7.27it/s][INFO|trainer.py:1011] 2024-03-18 22:49:53,176 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 241.7104, 'train_samples_per_second': 390.487, 'train_steps_per_second': 6.11, 'train_loss': 0.3217942202001586, 'epoch': 2.0, 'memory_allocated (GB)': 11.48, 'max_memory_allocated (GB)': 18.4, 'total_memory_available (GB)': 94.62}\n",
      "100%|███████████████████████████████████████| 1330/1330 [04:01<00:00,  5.50it/s]\n",
      "[INFO|trainer.py:1537] 2024-03-18 22:49:53,185 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:473] 2024-03-18 22:49:53,314 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:2493] 2024-03-18 22:49:53,584 >> Model weights saved in /tmp/outputs/model.safetensors\n",
      "[INFO|image_processing_utils.py:258] 2024-03-18 22:49:53,585 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2024-03-18 22:49:53,585 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =        2.0\n",
      "  max_memory_allocated (GB)   =       18.4\n",
      "  memory_allocated (GB)       =      11.48\n",
      "  total_memory_available (GB) =      94.62\n",
      "  train_loss                  =     0.3218\n",
      "  train_runtime               = 0:04:01.71\n",
      "  train_samples_per_second    =    390.487\n",
      "  train_steps_per_second      =       6.11\n",
      "[INFO|modelcard.py:452] 2024-03-18 22:49:54,129 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}}\n"
     ]
    }
   ],
   "source": [
    "!python run_image_classification.py \\\n",
    "    --model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir /tmp/outputs/ \\\n",
    "    --remove_unused_columns False \\\n",
    "    --image_column_name img \\\n",
    "    --do_train \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy no \\\n",
    "    --load_best_model_at_end False \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337 \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/swin \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \\\n",
    "    --report_to none \\\n",
    "    --throughput_warmup_steps 2 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ignore_mismatched_sizes \\\n",
    "    --profiling_warmup_steps 10 \\\n",
    "    --profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23106fe-e911-4e67-9c8f-4ad8d587408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'hpu_profile'\n",
      "/root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile\n",
      "total 258112\n",
      "drwxr-xr-x 2 root root      4096 Mar 18 22:46 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 4 root root      4096 Mar 18 22:46 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root 264297613 Mar 18 22:46 hls2-srv01-demolab_1924.1710802005850814532.pt.trace.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    }
   ],
   "source": [
    "%cd hpu_profile\n",
    "%ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21347a9a",
   "metadata": {},
   "source": [
    "### Reviewing the Details in Tensorboard and perf_tool\n",
    "Now that the training is completed, you can see the trace files (...pt.trace.json) have been generated and now can be viewed.  Two types of information are produced by TensorBoard:\n",
    "\n",
    "Model Performance Tracking - While your workload is being processed in batches, you can track the progress of the training process on the dashboard in real-time by monitoring the model’s cost (loss) and accuracy.\n",
    "\n",
    "Profiling Analysis - Right after the last requested step was completed, the collected profiling data is analyzed by TensorBoard and then immediately submitted to your browser, without any need to wait till the training process is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb8d01f1-1c80-4ba7-8384-2626e93f6ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-34ec15902bbafacb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-34ec15902bbafacb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=~/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile --port 6006    # Your port selection may vary, default is 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6df7a-d03d-4994-b8c5-ac9099b68567",
   "metadata": {},
   "source": [
    "If you do not want to run the TensorBoard UI, you can take the same .json log files and use the habana_perf_tool that will parse the existing .json file and provide the same recommendations for performance enhancements, but in a text form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9dee212-948f-473c-a4d8-b530525071cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-18 22:52:36,310 - pytorch_profiler - DEBUG - Loading /root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile/hls2-srv01-demolab_1924.1710802005850814532.pt.trace.json\n",
      "Import Data (KB): 100%|██████████████| 258103/258103 [00:02<00:00, 97726.52it/s]\n",
      "2024-03-18 22:52:39,972 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2024-03-18 22:52:48,030 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='32,49,35', pt_bridge_compute='33', pt_mem_copy='35', pt_mem_log='', pt_build_graph='48,34,51,52')\n",
      "2024-03-18 22:52:48,030 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='31', backward='47', synapse_launch='0,2,50', synapse_wait='1,37', device_mme='43,44,45,46', device_tpc='7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30', device_dma='6,38,39,40,41,42')\n",
      "2024-03-18 22:52:49,689 - pytorch_profiler - DEBUG - Device ratio: 28.32 % (202.93 ms, 716.607 ms)\n",
      "2024-03-18 22:52:49,689 - pytorch_profiler - DEBUG - Device/Host ratio: 28.32% / 71.68%\n",
      "2024-03-18 22:52:50,441 - pytorch_profiler - DEBUG - Host Summary Graph Build: 17.28 % (123.280976 ms, 713.275 ms)\n",
      "2024-03-18 22:52:50,607 - pytorch_profiler - DEBUG - Host Summary DataLoader: 52.07 % (371.417 ms, 713.275 ms)\n",
      "2024-03-18 22:52:50,878 - pytorch_profiler - DEBUG - Host Summary Input Time: 2.46 % (17.518 ms, 713.275 ms)\n",
      "2024-03-18 22:52:51,017 - pytorch_profiler - DEBUG - Host Summary Compile Time: 0.00 % (0.0 ms, 713.275 ms)\n",
      "2024-03-18 22:52:51,453 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 81.41%\n",
      "2024-03-18 22:52:51,453 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 98.36 %\n",
      "2024-03-18 22:52:51,453 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2024-03-18 22:52:51,453 - pytorch_profiler - DEBUG - \tThis run has high time cost on input data loading. 52.07% of the step time is in DataLoader. You could use Habana DataLoader. Or you could try to tune num_workers on DataLoader's construction.\n",
      "2024-03-18 22:52:51,822 - pytorch_profiler - DEBUG - [Device Summary] MME total time 78.11 ms\n",
      "2024-03-18 22:52:57,571 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 47.34 ms\n",
      "2024-03-18 22:52:57,572 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 98.75 ms\n",
      "2024-03-18 22:52:59,523 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 26.52 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace /root/Gaudi-tutorials/PyTorch/Profiling_and_Optimization/optimum-habana/examples/image-classification/hpu_profile/hls2-srv01-demolab_1924.1710802005850814532.pt.trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5821e9-fe68-4c76-93ef-b3b6c8492a02",
   "metadata": {},
   "source": [
    "### Using the Perfetto Trace Viewer\n",
    "Finally, to view the details of the Intel Gaudi Device itself, you can view the traces in the perfetto trace viewer.  \n",
    "\n",
    "This step requires you to set the `hl-prof-config` settings and the Environment variable `HABANA_PROFILE=1` as shown below, this will generate the .hltv file that can be viewed using https://perfetto.habana.ai.  Since this is using the Gaudi profiler, the runtime profiling commands need to be removed.  At the end of this run, you will see a `my_profiling_session_12345.hltv` file that can be loaded into the Perfetto browser.\n",
    "\n",
    "For More Information to enable your model to use the Habana Perfetto Trace viewer, you can refer to the documentation https://docs.habana.ai/en/latest/Profiling/Intel_Gaudi_Profiling/Getting_Started_with_Profiler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "!hl-prof-config -e off -phase=multi-enq -g 1-20 -s my_profiling_session\n",
    "!export HABANA_PROFILE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26eb57-a519-4935-b7a5-92a56cdc1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "!HABANA_PROFILE=1 python run_image_classification.py \\\n",
    "    --model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "    --dataset_name cifar10 \\\n",
    "    --output_dir /tmp/outputs/ \\\n",
    "    --remove_unused_columns False \\\n",
    "    --image_column_name img \\\n",
    "    --do_train \\\n",
    "    --learning_rate 3e-5 \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --per_device_train_batch_size 64 \\\n",
    "    --evaluation_strategy no \\\n",
    "    --save_strategy no \\\n",
    "    --load_best_model_at_end False \\\n",
    "    --save_total_limit 3 \\\n",
    "    --seed 1337 \\\n",
    "    --use_habana \\\n",
    "    --use_lazy_mode \\\n",
    "    --report_to none \\\n",
    "    --use_hpu_graphs_for_training \\\n",
    "    --gaudi_config_name Habana/swin \\\n",
    "    --throughput_warmup_steps 3 \\\n",
    "    --bf16 \\\n",
    "    --throughput_warmup_steps 2 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ignore_mismatched_sizes \n",
    "    #--profiling_warmup_steps 10 \\\n",
    "    #--profiling_steps 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
