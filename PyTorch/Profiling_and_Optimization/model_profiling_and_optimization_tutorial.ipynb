{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "#### Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
    "you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
    "# Model Profiling and optimization using Swin Transformer from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objecive\n",
    "This tutorial will show the user how to run the Habnaba Profliing tools; the habana_perf_tool and the Tensorboard plug-in.  These tools will provide the user valueable optimization tips and information to modify any model for better performance.   For more information, please refer to the [Profiling](https://docs.habana.ai/en/latest/Profiling/index.html) section of the documentation for info on how to setup the profiler and the [Optimization Guide](https://docs.habana.ai/en/latest/PyTorch/Model_Optimization_PyTorch/index.html) for additional background on other optimization techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "We start with a Habana PyTorch Docker image and run this notebook.   For this example, we'll be using the [Swin Transformer](https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k) model from the Hugging Face Repository running on Hugging Face's Optimum-Habana library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum[habana]\n",
      "  Downloading optimum-1.8.8.tar.gz (244 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.1/244.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.15.1a0+42759b1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Collecting huggingface-hub>=0.8.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.1)\n",
      "Collecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m277.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.0.1a0+git37b7ddc)\n",
      "Collecting optimum-habana\n",
      "  Downloading optimum_habana-1.6.0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m384.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<4.29.0\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (2020.10.28)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (3.19.5)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m343.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.8.4)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m395.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (1.4.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m388.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m434.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m428.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers>=0.12.0\n",
      "  Downloading diffusers-0.17.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.12.0->optimum-habana->optimum[habana]) (6.6.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->optimum-habana->optimum[habana]) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.12.0->optimum-habana->optimum[habana]) (3.15.0)\n",
      "Building wheels for collected packages: optimum\n",
      "  Building wheel for optimum (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum: filename=optimum-1.8.8-py3-none-any.whl size=318455 sha256=2ea2857c786c4385a32099fe4791be36627b7f3eb874093017908ec09333b46c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wjyzd51r/wheels/a0/93/00/e6f8f49a40e3af11f7d883b36482af20073db0520b340e0f24\n",
      "Successfully built optimum\n",
      "Installing collected packages: tokenizers, sentencepiece, xxhash, pyarrow, pillow, humanfriendly, dill, multiprocess, huggingface-hub, coloredlogs, transformers, diffusers, accelerate, datasets, optimum, optimum-habana\n",
      "Successfully installed accelerate-0.20.3 coloredlogs-15.0.1 datasets-2.13.1 diffusers-0.17.1 dill-0.3.6 huggingface-hub-0.15.1 humanfriendly-10.0 multiprocess-0.70.14 optimum-1.8.8 optimum-habana-1.6.0 pillow-9.5.0 pyarrow-12.0.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install optimum[habana]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the HuggingFace Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 4208, done.\u001b[K\n",
      "remote: Counting objects: 100% (1319/1319), done.\u001b[K\n",
      "remote: Compressing objects: 100% (517/517), done.\u001b[K\n",
      "remote: Total 4208 (delta 895), reused 1061 (delta 760), pack-reused 2889\u001b[K\n",
      "Receiving objects: 100% (4208/4208), 2.24 MiB | 5.86 MiB/s, done.\n",
      "Resolving deltas: 100% (2677/2677), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go the image-classification example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/workshop/Gaudi2-Workshop/Model-Optimization/optimum-habana/examples/image-classification\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.15.1a0+42759b1)\n",
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (2.13.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (4.6.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.12.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (2023.5.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (12.0.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (3.8.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (0.3.6)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.4.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, responses, evaluate\n",
      "Successfully installed evaluate-0.4.0 joblib-1.2.0 responses-0.18.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look in utils.py to see torch profiler code\n",
    "For other models not in optimum-habana, users can refer to [Profiling_with_PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   245\t            schedule = torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1)\n",
      "   246\t            activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.HPU]\n",
      "   247\t\n",
      "   248\t            profiler = torch.profiler.profile(\n",
      "   249\t                schedule=schedule,\n",
      "   250\t                activities=activities,\n",
      "   251\t                on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),\n",
      "   252\t                record_shapes=True,\n",
      "   253\t                with_stack=True,\n",
      "   254\t            )\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat -n ../../optimum/habana/utils.py | head -n 254 | tail -n 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model to collect trace file (unoptimized)\n",
    "Swin Transformer is a model that capably serves as a general-purpose backbone for computer vision. `run_image_classification.py` is a script that showcases how to fine-tune Swin Transformer on HPUs.\n",
    "\n",
    "Notice the Habana specific commands:\n",
    "\n",
    "`--use_habana` - allows training to run on Habana Gaudi  \n",
    "`--use_hpu_graphs` - reduces recompilation by replaying the graph  \n",
    "`--gaudi_config_name Habana/swin` - mapping to HuggingFace Swin Model config \n",
    "\n",
    "Notice the torch profiler specific commands:\n",
    "\n",
    "`--profiling_warmup_steps 10` - profiler will wait for warmup steps  \n",
    "`--profiling_steps 3` - records for the next active steps \n",
    "\n",
    "The collected trace files will be saved to `./hpu_profile`; but copies will be moved to the `./swin_profile` folder for reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/training_args.py:252: FutureWarning: `--use_hpu_graphs` is deprecated and will be removed in a future version of ü§ó Optimum Habana. Use `--use_hpu_graphs_for_inference` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:10:28 - WARNING - __main__ - Process rank: -1, device: hpu, distributed training: False, mixed-precision training: True\n",
      "06/27/2023 03:10:28 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Jun27_03-10-27_sc09wynn01-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=True,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (‚Ä¶)in/gaudi_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 450/450 [00:00<00:00, 114kB/s]\n",
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.61k/3.61k [00:00<00:00, 37.1MB/s]\n",
      "Downloading metadata: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.66k/1.66k [00:00<00:00, 21.4MB/s]\n",
      "Downloading readme: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.00k/5.00k [00:00<00:00, 47.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cifar10/plain_text to /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170M/170M [00:05<00:00, 31.0MB/s] \n",
      "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]/usr/local/lib/python3.8/dist-packages/datasets/features/image.py:325: UserWarning: Downcasting array dtype uint8 to uint8 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar10 downloaded and prepared to /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating train split:   0%|          | 1/50000 [00:00<3:18:50,  4.19 examples/s]\r\n",
      "Generating train split:   1%|          | 446/50000 [00:00<00:29, 1667.00 examples/s]\r\n",
      "Generating train split:   2%|‚ñè         | 895/50000 [00:00<00:18, 2647.98 examples/s]\r\n",
      "Generating train split:   3%|‚ñé         | 1309/50000 [00:00<00:15, 3140.83 examples/s]\r\n",
      "Generating train split:   4%|‚ñé         | 1767/50000 [00:00<00:13, 3601.21 examples/s]\r\n",
      "Generating train split:   4%|‚ñç         | 2196/50000 [00:00<00:12, 3817.27 examples/s]\r\n",
      "Generating train split:   5%|‚ñå         | 2656/50000 [00:00<00:11, 4058.34 examples/s]\r\n",
      "Generating train split:   6%|‚ñå         | 3100/50000 [00:00<00:11, 4173.32 examples/s]\r\n",
      "Generating train split:   7%|‚ñã         | 3559/50000 [00:01<00:10, 4294.56 examples/s]\r\n",
      "Generating train split:   8%|‚ñä         | 4002/50000 [00:01<00:10, 4333.26 examples/s]\r\n",
      "Generating train split:   9%|‚ñâ         | 4462/50000 [00:01<00:10, 4411.66 examples/s]\r\n",
      "Generating train split:  10%|‚ñâ         | 4922/50000 [00:01<00:10, 4466.12 examples/s]\r\n",
      "Generating train split:  11%|‚ñà         | 5584/50000 [00:01<00:09, 4441.67 examples/s]\r\n",
      "Generating train split:  13%|‚ñà‚ñé        | 6257/50000 [00:01<00:09, 4453.38 examples/s]\r\n",
      "Generating train split:  13%|‚ñà‚ñé        | 6718/50000 [00:01<00:09, 4489.06 examples/s]\r\n",
      "Generating train split:  15%|‚ñà‚ñç        | 7388/50000 [00:01<00:09, 4479.05 examples/s]\r\n",
      "Generating train split:  16%|‚ñà‚ñå        | 7849/50000 [00:01<00:09, 4510.04 examples/s]\r\n",
      "Generating train split:  17%|‚ñà‚ñã        | 8520/50000 [00:02<00:09, 4494.26 examples/s]\r\n",
      "Generating train split:  18%|‚ñà‚ñä        | 8980/50000 [00:02<00:09, 4517.07 examples/s]\r\n",
      "Generating train split:  19%|‚ñà‚ñâ        | 9644/50000 [00:02<00:09, 4482.87 examples/s]\r\n",
      "Generating train split:  20%|‚ñà‚ñà        | 10231/50000 [00:02<00:15, 2537.54 examples/s]\r\n",
      "Generating train split:  21%|‚ñà‚ñà‚ñè       | 10686/50000 [00:02<00:13, 2853.54 examples/s]\r\n",
      "Generating train split:  22%|‚ñà‚ñà‚ñè       | 11114/50000 [00:03<00:12, 3113.86 examples/s]\r\n",
      "Generating train split:  23%|‚ñà‚ñà‚ñé       | 11570/50000 [00:03<00:11, 3412.04 examples/s]\r\n",
      "Generating train split:  24%|‚ñà‚ñà‚ñç       | 12000/50000 [00:03<00:10, 3612.29 examples/s]\r\n",
      "Generating train split:  25%|‚ñà‚ñà‚ñç       | 12458/50000 [00:03<00:09, 3848.23 examples/s]\r\n",
      "Generating train split:  26%|‚ñà‚ñà‚ñå       | 12915/50000 [00:03<00:09, 4033.91 examples/s]\r\n",
      "Generating train split:  27%|‚ñà‚ñà‚ñã       | 13356/50000 [00:03<00:08, 4133.82 examples/s]\r\n",
      "Generating train split:  28%|‚ñà‚ñà‚ñä       | 13814/50000 [00:03<00:08, 4255.37 examples/s]\r\n",
      "Generating train split:  29%|‚ñà‚ñà‚ñâ       | 14483/50000 [00:03<00:08, 4328.55 examples/s]\r\n",
      "Generating train split:  30%|‚ñà‚ñà‚ñâ       | 14940/50000 [00:03<00:07, 4391.22 examples/s]\r\n",
      "Generating train split:  31%|‚ñà‚ñà‚ñà       | 15609/50000 [00:04<00:07, 4412.52 examples/s]\r\n",
      "Generating train split:  33%|‚ñà‚ñà‚ñà‚ñé      | 16261/50000 [00:04<00:07, 4386.70 examples/s]\r\n",
      "Generating train split:  33%|‚ñà‚ñà‚ñà‚ñé      | 16719/50000 [00:04<00:07, 4430.88 examples/s]\r\n",
      "Generating train split:  35%|‚ñà‚ñà‚ñà‚ñç      | 17390/50000 [00:04<00:07, 4443.79 examples/s]\r\n",
      "Generating train split:  36%|‚ñà‚ñà‚ñà‚ñå      | 17849/50000 [00:04<00:07, 4476.70 examples/s]\r\n",
      "Generating train split:  37%|‚ñà‚ñà‚ñà‚ñã      | 18517/50000 [00:04<00:07, 4464.24 examples/s]\r\n",
      "Generating train split:  38%|‚ñà‚ñà‚ñà‚ñä      | 18974/50000 [00:04<00:06, 4489.43 examples/s]\r\n",
      "Generating train split:  39%|‚ñà‚ñà‚ñà‚ñâ      | 19645/50000 [00:04<00:06, 4479.58 examples/s]\r\n",
      "Generating train split:  40%|‚ñà‚ñà‚ñà‚ñà      | 20228/50000 [00:05<00:09, 3260.48 examples/s]\r\n",
      "Generating train split:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 20668/50000 [00:05<00:08, 3478.70 examples/s]\r\n",
      "Generating train split:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21093/50000 [00:05<00:07, 3644.33 examples/s]\r\n",
      "Generating train split:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 21546/50000 [00:05<00:07, 3851.73 examples/s]\r\n",
      "Generating train split:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22000/50000 [00:05<00:07, 3988.17 examples/s]\r\n",
      "Generating train split:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22450/50000 [00:05<00:06, 4121.60 examples/s]\r\n",
      "Generating train split:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 22907/50000 [00:05<00:06, 4242.28 examples/s]\r\n",
      "Generating train split:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 23571/50000 [00:06<00:06, 4306.11 examples/s]\r\n",
      "Generating train split:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24226/50000 [00:06<00:05, 4316.88 examples/s]\r\n",
      "Generating train split:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 24680/50000 [00:06<00:05, 4367.71 examples/s]\r\n",
      "Generating train split:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25339/50000 [00:06<00:05, 4372.94 examples/s]\r\n",
      "Generating train split:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 25793/50000 [00:06<00:05, 4413.26 examples/s]\r\n",
      "Generating train split:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 26453/50000 [00:06<00:05, 4388.52 examples/s]\r\n",
      "Generating train split:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 26907/50000 [00:06<00:05, 4424.59 examples/s]\r\n",
      "Generating train split:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 27569/50000 [00:06<00:05, 4416.86 examples/s]\r\n",
      "Generating train split:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28227/50000 [00:07<00:04, 4405.80 examples/s]\r\n",
      "Generating train split:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 28681/50000 [00:07<00:04, 4436.05 examples/s]\r\n",
      "Generating train split:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29339/50000 [00:07<00:04, 4415.43 examples/s]\r\n",
      "Generating train split:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 29794/50000 [00:07<00:04, 4446.21 examples/s]\r\n",
      "Generating train split:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30444/50000 [00:07<00:06, 3113.71 examples/s]\r\n",
      "Generating train split:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 30887/50000 [00:07<00:05, 3359.45 examples/s]\r\n",
      "Generating train split:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31307/50000 [00:07<00:05, 3535.59 examples/s]\r\n",
      "Generating train split:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 31756/50000 [00:08<00:04, 3755.01 examples/s]\r\n",
      "Generating train split:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32184/50000 [00:08<00:04, 3884.34 examples/s]\r\n",
      "Generating train split:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 32631/50000 [00:08<00:04, 4034.11 examples/s]\r\n",
      "Generating train split:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33059/50000 [00:08<00:04, 4098.49 examples/s]\r\n",
      "Generating train split:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 33512/50000 [00:08<00:03, 4216.92 examples/s]\r\n",
      "Generating train split:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 33962/50000 [00:08<00:03, 4294.67 examples/s]\r\n",
      "Generating train split:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 34617/50000 [00:08<00:03, 4318.84 examples/s]\r\n",
      "Generating train split:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35269/50000 [00:08<00:03, 4327.50 examples/s]\r\n",
      "Generating train split:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 35720/50000 [00:08<00:03, 4369.86 examples/s]\r\n",
      "Generating train split:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36375/50000 [00:09<00:03, 4365.05 examples/s]\r\n",
      "Generating train split:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 36821/50000 [00:09<00:03, 4385.91 examples/s]\r\n",
      "Generating train split:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37475/50000 [00:09<00:02, 4374.67 examples/s]\r\n",
      "Generating train split:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 37923/50000 [00:09<00:02, 4398.12 examples/s]\r\n",
      "Generating train split:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 38567/50000 [00:09<00:02, 4358.16 examples/s]\r\n",
      "Generating train split:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39221/50000 [00:09<00:02, 4354.79 examples/s]\r\n",
      "Generating train split:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 39673/50000 [00:09<00:02, 4391.26 examples/s]\r\n",
      "Generating train split:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40229/50000 [00:10<00:03, 3046.83 examples/s]\r\n",
      "Generating train split:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 40683/50000 [00:10<00:02, 3332.11 examples/s]\r\n",
      "Generating train split:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41116/50000 [00:10<00:02, 3543.73 examples/s]\r\n",
      "Generating train split:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 41573/50000 [00:10<00:02, 3782.64 examples/s]\r\n",
      "Generating train split:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42014/50000 [00:10<00:02, 3938.44 examples/s]\r\n",
      "Generating train split:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42471/50000 [00:10<00:01, 4103.17 examples/s]\r\n",
      "Generating train split:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 42924/50000 [00:10<00:01, 4218.87 examples/s]\r\n",
      "Generating train split:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 43574/50000 [00:10<00:01, 4259.22 examples/s]\r\n",
      "Generating train split:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44230/50000 [00:11<00:01, 4293.47 examples/s]\r\n",
      "Generating train split:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 44683/50000 [00:11<00:01, 4349.24 examples/s]\r\n",
      "Generating train split:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45347/50000 [00:11<00:01, 4373.50 examples/s]\r\n",
      "Generating train split:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 45800/50000 [00:11<00:00, 4411.95 examples/s]\r\n",
      "Generating train split:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 46465/50000 [00:11<00:00, 4415.43 examples/s]\r\n",
      "Generating train split:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 46922/50000 [00:11<00:00, 4450.48 examples/s]\r\n",
      "Generating train split:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 47580/50000 [00:11<00:00, 4424.76 examples/s]\r\n",
      "Generating train split:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 48240/50000 [00:11<00:00, 4414.53 examples/s]\r\n",
      "Generating train split:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 48691/50000 [00:12<00:00, 4433.93 examples/s]\r\n",
      "Generating train split:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49352/50000 [00:12<00:00, 4419.93 examples/s]\r\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 49801/50000 [00:12<00:00, 4435.08 examples/s]\r\n",
      "                                                                                      \r\n",
      "\r\n",
      "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]\r\n",
      "Generating test split:   0%|          | 1/10000 [00:00<54:46,  3.04 examples/s]\r\n",
      "Generating test split:   5%|‚ñç         | 455/10000 [00:00<00:06, 1377.38 examples/s]\r\n",
      "Generating test split:   9%|‚ñâ         | 912/10000 [00:00<00:03, 2340.97 examples/s]\r\n",
      "Generating test split:  13%|‚ñà‚ñé        | 1338/10000 [00:00<00:02, 2916.12 examples/s]\r\n",
      "Generating test split:  18%|‚ñà‚ñä        | 1794/10000 [00:00<00:02, 3408.00 examples/s]\r\n",
      "Generating test split:  22%|‚ñà‚ñà‚ñè       | 2231/10000 [00:00<00:02, 3693.87 examples/s]\r\n",
      "Generating test split:  27%|‚ñà‚ñà‚ñã       | 2687/10000 [00:00<00:01, 3953.02 examples/s]\r\n",
      "Generating test split:  33%|‚ñà‚ñà‚ñà‚ñé      | 3345/10000 [00:01<00:01, 4117.87 examples/s]\r\n",
      "Generating test split:  38%|‚ñà‚ñà‚ñà‚ñä      | 3802/10000 [00:01<00:01, 4237.10 examples/s]\r\n",
      "Generating test split:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4466/10000 [00:01<00:01, 4302.08 examples/s]\r\n",
      "Generating test split:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4926/10000 [00:01<00:01, 4375.13 examples/s]\r\n",
      "Generating test split:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5599/10000 [00:01<00:00, 4411.23 examples/s]\r\n",
      "Generating test split:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 6263/10000 [00:01<00:00, 4412.52 examples/s]\r\n",
      "Generating test split:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6719/10000 [00:01<00:00, 4444.97 examples/s]\r\n",
      "Generating test split:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 7391/10000 [00:01<00:00, 4453.67 examples/s]\r\n",
      "Generating test split:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7851/10000 [00:02<00:00, 4485.23 examples/s]\r\n",
      "Generating test split:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 8509/10000 [00:02<00:00, 4448.36 examples/s]\r\n",
      "Generating test split:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8967/10000 [00:02<00:00, 4478.74 examples/s]\r\n",
      "Generating test split:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 9631/10000 [00:02<00:00, 4455.57 examples/s]\r\n",
      "                                                                                    \r\n",
      "\r\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\r\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 672.70it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.20k/4.20k [00:00<00:00, 28.3MB/s]     \n",
      "Downloading (‚Ä¶)lve/main/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.67M/1.67M [00:00<00:00, 32.3MB/s]\n",
      "[INFO|configuration_utils.py:668] 2023-06-27 03:10:53,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-06-27 03:10:53,256 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 437M/437M [00:03<00:00, 139MB/s]  \n",
      "[INFO|modeling_utils.py:2534] 2023-06-27 03:10:56,631 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3190] 2023-06-27 03:10:57,597 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:3211] 2023-06-27 03:10:57,597 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (‚Ä¶)rocessor_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 255/255 [00:00<00:00, 280kB/s]\n",
      "[INFO|image_processing_utils.py:308] 2023-06-27 03:10:57,848 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:327] 2023-06-27 03:10:57,848 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "[INFO|image_processing_utils.py:532] 2023-06-27 03:10:57,850 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:353] 2023-06-27 03:10:57,850 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056426612 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:770] 2023-06-27 03:10:59,201 >> ***** Running training *****\n",
      "[INFO|trainer.py:771] 2023-06-27 03:10:59,201 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:772] 2023-06-27 03:10:59,201 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:773] 2023-06-27 03:10:59,201 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:774] 2023-06-27 03:10:59,201 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:775] 2023-06-27 03:10:59,201 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:776] 2023-06-27 03:10:59,201 >>   Total optimization steps = 665\n",
      "[INFO|trainer.py:777] 2023-06-27 03:10:59,203 >>   Number of trainable parameters = 86,753,474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 10/665 [00:35<08:12,  1.33it/s]STAGE:2023-06-27 03:11:34 241:241 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "  2%|‚ñè         | 13/665 [00:48<25:14,  2.32s/it]STAGE:2023-06-27 03:11:48 241:241 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-27 03:11:48 241:241 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [03:27<00:00,  5.12it/s][INFO|trainer.py:1041] 2023-06-27 03:14:26,800 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [03:27<00:00,  3.20it/s]\n",
      "[INFO|trainer.py:1766] 2023-06-27 03:14:26,871 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:457] 2023-06-27 03:14:27,022 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:1847] 2023-06-27 03:14:27,309 >> Model weights saved in /tmp/outputs/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:203] 2023-06-27 03:14:27,309 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2023-06-27 03:14:27,309 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "[INFO|modelcard.py:451] 2023-06-27 03:14:27,467 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'cifar10', 'type': 'cifar10', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27, 'learning_rate': 7.443609022556391e-06, 'epoch': 0.75, 'memory_allocated (GB)': 90.52, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "{'train_runtime': 207.6646, 'train_samples_per_second': 240.412, 'train_steps_per_second': 3.762, 'train_loss': 0.2721804511278195, 'epoch': 1.0, 'memory_allocated (GB)': 90.84, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "***** train metrics *****\n",
      "  epoch                       =        1.0\n",
      "  max_memory_allocated (GB)   =      92.25\n",
      "  memory_allocated (GB)       =      90.84\n",
      "  total_memory_available (GB) =      93.74\n",
      "  train_loss                  =     0.2722\n",
      "  train_runtime               = 0:03:27.66\n",
      "  train_samples_per_second    =    240.412\n",
      "  train_steps_per_second      =      3.762\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python run_image_classification.py \\\n",
    "--model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "--dataset_name cifar10 \\\n",
    "--output_dir /tmp/outputs/ \\\n",
    "--remove_unused_columns False \\\n",
    "--do_train \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--evaluation_strategy no \\\n",
    "--save_strategy no \\\n",
    "--load_best_model_at_end False \\\n",
    "--save_total_limit 3 \\\n",
    "--seed 1337 \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/swin \\\n",
    "--throughput_warmup_steps 2 \\\n",
    "--overwrite_output_dir \\\n",
    "--ignore_mismatched_sizes \\\n",
    "--profiling_warmup_steps 10 \\\n",
    "--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 746936\r\n",
      "drwxr-xr-x 2 root root      4096 Jun 27 03:37 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 5 root root      4096 Jun 27 05:49 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 root root 247277431 Jun 27 03:27 sc09wynn01-hls2_1725.1687836473515.pt.trace.json\r\n",
      "-rw-r--r-- 1 root root 270297943 Jun 27 03:12 sc09wynn01-hls2_241.1687835527077.pt.trace.json\r\n",
      "-rw-r--r-- 1 root root 247261794 Jun 27 03:37 sc09wynn01-hls2_3618.1687837025280.pt.trace.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./hpu_profile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two ways to use HPU Performance Analysis tool\n",
    "\n",
    "We can launch Tensorboard to see the performance analysis results:\n",
    "```sh\n",
    "tensorboard --logdir xxx\n",
    "```\n",
    "\n",
    "Or simply use `habana_perf_tool` to see the console output analysis:\n",
    "```sh\n",
    "habana_perf_tool --trace xxx.trace.json\n",
    "```\n",
    "\n",
    "Notice the contents of `habana_perf_tool` console output:\n",
    "\n",
    "`Device/Host ratio` - To show the overall performance, device utilization  \n",
    "`Host Summary` - Host side performance, to show dataloader, graph build, data copy and compile  \n",
    "`Device Summary` - Device side performance, to show MME, TPC and DMA  \n",
    "`Host/Device Recommendations` - Performance Recommendations for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 200084\n",
      "drwxr-xr-x 3 root root      4096 Jul 19 22:06 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 5 root root      4096 Jul 19 21:57 \u001b[01;34m..\u001b[0m/\n",
      "drwxr-xr-x 2 root root      4096 Jul 19 22:06 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
      "-rw-r--r-- 1 root root 204870550 Jul 19 22:00 UNOPT.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./swin_profile/unoptimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 22:07:04,476 - pytorch_profiler - DEBUG - Loading ./swin_profile/unoptimized/UNOPT.pt.trace.json\n",
      "Import Data (KB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200068/200068 [00:01<00:00, 101312.72it/s]\n",
      "2023-07-19 22:07:07,468 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2023-07-19 22:07:15,881 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='46,51,6', pt_bridge_compute='15', pt_mem_copy='6', pt_mem_log='', pt_build_graph='48,49,45,5')\n",
      "2023-07-19 22:07:15,881 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='4', backward='44', synapse_launch='0,47,50', synapse_wait='1,9', device_mme='40,41,42,43', device_tpc='16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39', device_dma='7,10,11,12,13,14')\n",
      "2023-07-19 22:07:18,228 - pytorch_profiler - DEBUG - Device ratio: 61.66 % (288.393 ms, 467.734 ms)\n",
      "2023-07-19 22:07:18,228 - pytorch_profiler - DEBUG - Device/Host ratio: 61.66% / 38.34%\n",
      "2023-07-19 22:07:19,098 - pytorch_profiler - DEBUG - Host Summary Graph Build: 14.50 % (60.240976 ms, 415.491 ms)\n",
      "2023-07-19 22:07:19,288 - pytorch_profiler - DEBUG - Host Summary DataLoader: 55.98 % (232.607 ms, 415.491 ms)\n",
      "2023-07-19 22:07:19,565 - pytorch_profiler - DEBUG - Host Summary Input Time: 4.62 % (19.187 ms, 415.491 ms)\n",
      "2023-07-19 22:07:19,772 - pytorch_profiler - DEBUG - Host Summary Compile Time: 1.52 % (6.31 ms, 415.491 ms)\n",
      "2023-07-19 22:07:20,245 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 77.08%\n",
      "2023-07-19 22:07:20,245 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 81.88 %\n",
      "2023-07-19 22:07:20,245 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2023-07-19 22:07:20,245 - pytorch_profiler - DEBUG - \tThis run has high time cost on input data loading. 55.98% of the step time is in DataLoader. You could use Habana DataLoader. Or you could try to tune num_workers on DataLoader's construction.\n",
      "2023-07-19 22:07:20,245 - pytorch_profiler - DEBUG - \tCompile times per step : [2]. Compile ratio: 1.52% (total time: 6.31 ms)\n",
      "2023-07-19 22:07:20,561 - pytorch_profiler - DEBUG - [Device Summary] MME total time 88.28 ms\n",
      "2023-07-19 22:07:27,530 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 57.94 ms\n",
      "2023-07-19 22:07:27,531 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 165.36 ms\n",
      "2023-07-19 22:07:29,530 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 29.43 ms\n",
      "2023-07-19 22:07:29,530 - pytorch_profiler - DEBUG - [Device Summary] Idle total time: 5.32 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./swin_profile/unoptimized/UNOPT.pt.trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case you will see that the Data Loader is taking too much time in the HOST, and the tool is recommending that we try the Habana Dataloader or increase the number of workers used by the Data Loader, so let's try that and see the result.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply optimization 1 (set dataloader num_workers)\n",
    "Notice the command for optimization:\n",
    "\n",
    "`--dataloader_num_workers 4` - perform multi-process data loading by simply setting the `num_workers` to a positive integer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:26:42 - WARNING - __main__ - Process rank: -1, device: hpu, distributed training: False, mixed-precision training: True\n",
      "06/27/2023 03:26:42 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Jun27_03-26-42_sc09wynn01-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=True,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/27/2023 03:26:43 - WARNING - datasets.builder - Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/training_args.py:252: FutureWarning: `--use_hpu_graphs` is deprecated and will be removed in a future version of ü§ó Optimum Habana. Use `--use_hpu_graphs_for_inference` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:26:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-0e6610774520e174.arrow\n",
      "06/27/2023 03:26:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ab7f4e0bbd39b5eb.arrow\n",
      "06/27/2023 03:26:43 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ec174b38d51cda7f.arrow and /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-bc2d211ee9dfd2ce.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 629.87it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-06-27 03:26:44,334 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-06-27 03:26:44,338 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2534] 2023-06-27 03:26:44,341 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3190] 2023-06-27 03:26:45,251 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:3211] 2023-06-27 03:26:45,251 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:308] 2023-06-27 03:26:45,361 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:327] 2023-06-27 03:26:45,361 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "[INFO|image_processing_utils.py:532] 2023-06-27 03:26:45,363 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:353] 2023-06-27 03:26:45,363 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056426612 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:770] 2023-06-27 03:26:46,729 >> ***** Running training *****\n",
      "[INFO|trainer.py:771] 2023-06-27 03:26:46,729 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:772] 2023-06-27 03:26:46,729 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:773] 2023-06-27 03:26:46,729 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:774] 2023-06-27 03:26:46,729 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:775] 2023-06-27 03:26:46,729 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:776] 2023-06-27 03:26:46,729 >>   Total optimization steps = 665\n",
      "[INFO|trainer.py:777] 2023-06-27 03:26:46,730 >>   Number of trainable parameters = 86,753,474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 10/665 [00:34<07:19,  1.49it/s]STAGE:2023-06-27 03:27:21 1725:1725 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "  2%|‚ñè         | 13/665 [00:48<24:06,  2.22s/it]STAGE:2023-06-27 03:27:35 1725:1725 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-27 03:27:35 1725:1725 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [02:43<00:00,  8.75it/s][INFO|trainer.py:1041] 2023-06-27 03:29:30,205 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [02:43<00:00,  4.07it/s]\n",
      "[INFO|trainer.py:1766] 2023-06-27 03:29:30,243 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:457] 2023-06-27 03:29:30,355 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:1847] 2023-06-27 03:29:31,002 >> Model weights saved in /tmp/outputs/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:203] 2023-06-27 03:29:31,003 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2023-06-27 03:29:31,003 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "[INFO|modelcard.py:451] 2023-06-27 03:29:31,180 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'cifar10', 'type': 'cifar10', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.288, 'learning_rate': 7.443609022556391e-06, 'epoch': 0.75, 'memory_allocated (GB)': 90.52, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "{'train_runtime': 163.5047, 'train_samples_per_second': 322.011, 'train_steps_per_second': 5.039, 'train_loss': 0.28533834586466167, 'epoch': 1.0, 'memory_allocated (GB)': 90.84, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "***** train metrics *****\n",
      "  epoch                       =        1.0\n",
      "  max_memory_allocated (GB)   =      92.25\n",
      "  memory_allocated (GB)       =      90.84\n",
      "  total_memory_available (GB) =      93.74\n",
      "  train_loss                  =     0.2853\n",
      "  train_runtime               = 0:02:43.50\n",
      "  train_samples_per_second    =    322.011\n",
      "  train_steps_per_second      =      5.039\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python run_image_classification.py \\\n",
    "--model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "--dataset_name cifar10 \\\n",
    "--output_dir /tmp/outputs/ \\\n",
    "--remove_unused_columns False \\\n",
    "--do_train \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--evaluation_strategy no \\\n",
    "--save_strategy no \\\n",
    "--load_best_model_at_end False \\\n",
    "--save_total_limit 3 \\\n",
    "--seed 1337 \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/swin \\\n",
    "--throughput_warmup_steps 2 \\\n",
    "--overwrite_output_dir \\\n",
    "--ignore_mismatched_sizes \\\n",
    "--dataloader_num_workers 4 \\\n",
    "--profiling_warmup_steps 10 \\\n",
    "--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 177488\n",
      "drwxr-xr-x 3 root root      4096 Jul 19 22:05 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 5 root root      4096 Jul 19 21:57 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root 181733526 Jul 19 22:00 1stOPT.pt.trace.json\n",
      "drwxr-xr-x 2 root root      4096 Jul 19 22:05 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./swin_profile/1st_optim_num_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 22:05:39,782 - pytorch_profiler - DEBUG - Loading ./swin_profile/1st_optim_num_worker/1stOPT.pt.trace.json\n",
      "Import Data (KB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 177474/177474 [00:01<00:00, 102009.17it/s]\n",
      "2023-07-19 22:05:42,539 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2023-07-19 22:05:49,949 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='9,54,49', pt_bridge_compute='18', pt_mem_copy='9', pt_mem_log='', pt_build_graph='8,48,51,52')\n",
      "2023-07-19 22:05:49,950 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='7', backward='47', synapse_launch='0,50,53', synapse_wait='1,12', device_mme='43,45,46,44', device_tpc='36,30,26,31,23,25,35,19,29,38,24,22,33,37,27,20,41,32,28,34,40,42,39,21', device_dma='10,17,15,13,14,16')\n",
      "2023-07-19 22:05:52,033 - pytorch_profiler - DEBUG - Device ratio: 90.84 % (283.428 ms, 312.02 ms)\n",
      "2023-07-19 22:05:52,033 - pytorch_profiler - DEBUG - Device/Host ratio: 90.84% / 9.16%\n",
      "2023-07-19 22:05:52,798 - pytorch_profiler - DEBUG - Host Summary Graph Build: 28.77 % (59.886976 ms, 208.177 ms)\n",
      "2023-07-19 22:05:52,939 - pytorch_profiler - DEBUG - Host Summary DataLoader: 1.56 % (3.249 ms, 208.177 ms)\n",
      "2023-07-19 22:05:53,161 - pytorch_profiler - DEBUG - Host Summary Input Time: 11.58 % (24.109 ms, 208.177 ms)\n",
      "2023-07-19 22:05:53,343 - pytorch_profiler - DEBUG - Host Summary Compile Time: 2.28 % (4.746 ms, 208.177 ms)\n",
      "2023-07-19 22:05:53,810 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 77.08%\n",
      "2023-07-19 22:05:53,811 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 86.27 %\n",
      "2023-07-19 22:05:53,811 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2023-07-19 22:05:53,811 - pytorch_profiler - DEBUG - \t11.58% H2D of the step time is in Input Data Time. Step call times: [28, 28, 28]. You could try to set non-blocking in torch.Tensor.to and pin_memory in DataLoader's construction to asynchronously convert CPU tensor with pinned memory to a HPU tensor.\n",
      "2023-07-19 22:05:53,811 - pytorch_profiler - DEBUG - \tCompile times per step : [2]. Compile ratio: 2.28% (total time: 4.75 ms)\n",
      "2023-07-19 22:05:54,126 - pytorch_profiler - DEBUG - [Device Summary] MME total time 88.26 ms\n",
      "2023-07-19 22:06:01,047 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 57.95 ms\n",
      "2023-07-19 22:06:01,049 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 165.50 ms\n",
      "2023-07-19 22:06:03,065 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 26.40 ms\n",
      "2023-07-19 22:06:03,065 - pytorch_profiler - DEBUG - [Device Summary] Idle total time: 3.26 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./swin_profile/1st_optim_num_worker/1stOPT.pt.trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a much better result, where we see the HOST ratio drop to 9% and the throughput improve by 30%.  However, the tool is recommending to try using non-blocking data copy to streamline the code execution.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply optimization 2 (using asynchronous copy)\n",
    "Notice the command for optimization:\n",
    "\n",
    "`--non_blocking_data_copy True` - specifying the argument `non_blocking=True` during the copy operation, the Python thread can continue to execute other tasks while the copy occurs in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:35:50 - WARNING - __main__ - Process rank: -1, device: hpu, distributed training: False, mixed-precision training: True\n",
      "06/27/2023 03:35:50 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Jun27_03-35-49_sc09wynn01-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=True,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=True,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/27/2023 03:35:51 - WARNING - datasets.builder - Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/training_args.py:252: FutureWarning: `--use_hpu_graphs` is deprecated and will be removed in a future version of ü§ó Optimum Habana. Use `--use_hpu_graphs_for_inference` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:35:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-0e6610774520e174.arrow\n",
      "06/27/2023 03:35:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ab7f4e0bbd39b5eb.arrow\n",
      "06/27/2023 03:35:51 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ec174b38d51cda7f.arrow and /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-bc2d211ee9dfd2ce.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 591.62it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-06-27 03:35:51,720 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-06-27 03:35:51,724 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2534] 2023-06-27 03:35:51,727 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3190] 2023-06-27 03:35:52,612 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:3211] 2023-06-27 03:35:52,613 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:308] 2023-06-27 03:35:52,712 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:327] 2023-06-27 03:35:52,712 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "[INFO|image_processing_utils.py:532] 2023-06-27 03:35:52,713 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:353] 2023-06-27 03:35:52,714 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056426612 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:770] 2023-06-27 03:35:54,115 >> ***** Running training *****\n",
      "[INFO|trainer.py:771] 2023-06-27 03:35:54,116 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:772] 2023-06-27 03:35:54,116 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:773] 2023-06-27 03:35:54,116 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:774] 2023-06-27 03:35:54,116 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:775] 2023-06-27 03:35:54,116 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:776] 2023-06-27 03:35:54,116 >>   Total optimization steps = 665\n",
      "[INFO|trainer.py:777] 2023-06-27 03:35:54,117 >>   Number of trainable parameters = 86,753,474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 10/665 [00:38<07:58,  1.37it/s]STAGE:2023-06-27 03:36:32 3618:3618 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "  2%|‚ñè         | 13/665 [00:50<23:58,  2.21s/it]STAGE:2023-06-27 03:36:45 3618:3618 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-27 03:36:45 3618:3618 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [02:43<00:00,  8.45it/s][INFO|trainer.py:1041] 2023-06-27 03:38:37,480 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 665/665 [02:43<00:00,  4.07it/s]\n",
      "[INFO|trainer.py:1766] 2023-06-27 03:38:37,508 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:457] 2023-06-27 03:38:37,591 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:1847] 2023-06-27 03:38:38,242 >> Model weights saved in /tmp/outputs/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:203] 2023-06-27 03:38:38,242 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2023-06-27 03:38:38,243 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "[INFO|modelcard.py:451] 2023-06-27 03:38:38,437 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'cifar10', 'type': 'cifar10', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.288, 'learning_rate': 7.443609022556391e-06, 'epoch': 0.75, 'memory_allocated (GB)': 90.52, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "{'train_runtime': 163.3821, 'train_samples_per_second': 330.061, 'train_steps_per_second': 5.165, 'train_loss': 0.28533834586466167, 'epoch': 1.0, 'memory_allocated (GB)': 90.84, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "***** train metrics *****\n",
      "  epoch                       =        1.0\n",
      "  max_memory_allocated (GB)   =      92.25\n",
      "  memory_allocated (GB)       =      90.84\n",
      "  total_memory_available (GB) =      93.74\n",
      "  train_loss                  =     0.2853\n",
      "  train_runtime               = 0:02:43.38\n",
      "  train_samples_per_second    =    330.061\n",
      "  train_steps_per_second      =      5.165\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python run_image_classification.py \\\n",
    "--model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "--dataset_name cifar10 \\\n",
    "--output_dir /tmp/outputs/ \\\n",
    "--remove_unused_columns False \\\n",
    "--do_train \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--evaluation_strategy no \\\n",
    "--save_strategy no \\\n",
    "--load_best_model_at_end False \\\n",
    "--save_total_limit 3 \\\n",
    "--seed 1337 \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/swin \\\n",
    "--throughput_warmup_steps 2 \\\n",
    "--overwrite_output_dir \\\n",
    "--ignore_mismatched_sizes \\\n",
    "--dataloader_num_workers 4 \\\n",
    "--non_blocking_data_copy True \\\n",
    "--profiling_warmup_steps 10 \\\n",
    "--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 177508\n",
      "drwxr-xr-x 3 root root      4096 Jul 19 22:05 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 5 root root      4096 Jul 19 21:57 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root 181755865 Jul 19 22:04 2ndOPT.pt.trace.json\n",
      "drwxr-xr-x 2 root root      4096 Jul 19 22:03 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./swin_profile/2nd_optim_non_blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 22:04:37,679 - pytorch_profiler - DEBUG - Loading ./swin_profile/2nd_optim_non_blocking/2ndOPT.pt.trace.json\n",
      "Import Data (KB): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 177495/177495 [00:01<00:00, 102617.38it/s]\n",
      "2023-07-19 22:04:40,426 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2023-07-19 22:04:47,805 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='56,9,51', pt_bridge_compute='15', pt_mem_copy='9,58,13,57', pt_mem_log='', pt_build_graph='8,50,53,54')\n",
      "2023-07-19 22:04:47,806 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='7', backward='49', synapse_launch='0,52,55', synapse_wait='1,12', device_mme='45,47,48,46', device_tpc='29,31,26,32,41,25,27,21,36,28,30,24,35,43,39,22,44,38,34,42,33,37,40,23', device_dma='10,19,17,20,16,18')\n",
      "2023-07-19 22:04:49,814 - pytorch_profiler - DEBUG - Device ratio: 91.74 % (280.442 ms, 305.698 ms)\n",
      "2023-07-19 22:04:49,814 - pytorch_profiler - DEBUG - Device/Host ratio: 91.74% / 8.26%\n",
      "2023-07-19 22:04:50,555 - pytorch_profiler - DEBUG - Host Summary Graph Build: 33.66 % (67.771976 ms, 201.314 ms)\n",
      "2023-07-19 22:04:50,699 - pytorch_profiler - DEBUG - Host Summary DataLoader: 1.69 % (3.412 ms, 201.314 ms)\n",
      "2023-07-19 22:04:50,915 - pytorch_profiler - DEBUG - Host Summary Input Time: 1.33 % (2.687 ms, 201.314 ms)\n",
      "2023-07-19 22:04:51,093 - pytorch_profiler - DEBUG - Host Summary Compile Time: 2.31 % (4.652 ms, 201.314 ms)\n",
      "2023-07-19 22:04:51,552 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 77.08%\n",
      "2023-07-19 22:04:51,552 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 87.45 %\n",
      "2023-07-19 22:04:51,552 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2023-07-19 22:04:51,552 - pytorch_profiler - DEBUG - \tCompile times per step : [2]. Compile ratio: 2.31% (total time: 4.65 ms)\n",
      "2023-07-19 22:04:51,867 - pytorch_profiler - DEBUG - [Device Summary] MME total time 88.22 ms\n",
      "2023-07-19 22:04:58,786 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 57.90 ms\n",
      "2023-07-19 22:04:58,788 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 165.44 ms\n",
      "2023-07-19 22:05:00,819 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 33.71 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./swin_profile/2nd_optim_non_blocking/2ndOPT.pt.trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of optimizations \r\n",
    "##### First run‚Äã  \r\n",
    "Device utilization 61.6%, host is heavy with data loader costs 55.9%; Recommendations: tune num_workers or use Habana dataloader‚Äã  \r\n",
    "##### Second run (tune num_workers)‚Äã  \r\n",
    "Device utilization up to 90.8%, but data copy costs 11.5% of host step time; Recommendations: try to set non-blocking in torch.Tensor.to and pin_memory in DataLoader‚Äã  \r\n",
    "##### Third run (set non_blocking)‚Äã  \r\n",
    "Device utilization up to 91.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Viewer\n",
    "Finally, we'll launch the Tensorboard Viewer for the last training run.  The profiler can show three main sections:  \n",
    "\n",
    "#### HPU Overview  \n",
    "When using the TensorBoard profiler, the initial view will include a comprehensive summary of the Gaudi HPU, showing both the Gaudi Device execution information as well as the Host CPU information. You will be able to see the utilization of both Host and Device and see debug guidance at the bottom of the section that can provide some guidance for performance optimization  \n",
    "  \n",
    "#### HPU Kernel View  \n",
    "The HPU Kernel view provides specific details into the Gaudi HPU kernel, showing the utilization in the Tensor Processing Cores (TPC) and the matrix multiplication engine (MME)  \n",
    "\n",
    "#### Memory Profiling    \n",
    "To monitor HPU memory during training, set the profile_memory argument to True in the torch.profiler.profile function  \n",
    "\n",
    "See the [Profiling](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html) section in the documenation for more information on instrutmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6852a74fc390f968\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6852a74fc390f968\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6077;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./swin_profile/2nd_optim_non_blocking/ --port 6006    # Your port selectoin may vary, default is 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
